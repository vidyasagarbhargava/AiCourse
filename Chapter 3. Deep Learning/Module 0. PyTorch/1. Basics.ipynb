{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to PyTorch\n",
    "\n",
    "\n",
    "## General\n",
    "\n",
    "> An open source machine learning framework that accelerates the path from research prototyping to production deployment.\n",
    "\n",
    "- Mostly used to create neural networks\n",
    "- Specialized to use hardware acceleration (GPU, TPU, Tensor Cores etc.)\n",
    "- __API based on `numpy`__ (`torch.Tensor` instead of `np.array`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.float32 torch.Size([300])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "# Alias below is pretty common\n",
    "# one can also use torch directly\n",
    "import torch.nn.functional as F\n",
    "\n",
    "X = torch.rand(300, 10)  # Random uniform\n",
    "print(type(X))\n",
    "\n",
    "W = torch.randn(10)  # Random normal\n",
    "b = torch.tensor([1])  # create again another random tensor\n",
    "\n",
    "y = X @ W + b\n",
    "\n",
    "print(y.dtype, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data types\n",
    "\n",
    "PyTorch, similarly to `numpy` provides multiple data types, for example: \n",
    "\n",
    "- `torch.float` (32-bit precision)\n",
    "- `torch.double` (64-bit precision)\n",
    "- `torch.half` (16-bit precision)\n",
    "\n",
    "and many others (see [here](https://pytorch.org/docs/stable/tensor_attributes.html)).\n",
    "\n",
    "> Usually we will use floating point values (either `float` or `half`), depending on context\n",
    "\n",
    "### Why not double?\n",
    "\n",
    "> Default `dtype` in PyTorch is `float` because __it doesn't take up so much memory and is accurate enough__\n",
    "\n",
    "Also, GPU memory is costly (and there isn't enough of it usually), hence lower precision (up to a certain point) might be a good solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casting\n",
    "\n",
    "One can easily cast PyTorch tensors to desired data types, see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64 torch.float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "array = np.random.randn(10, 5)\n",
    "tensor = torch.from_numpy(array)\n",
    "print(array.dtype, tensor.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cast to half type\n",
    "new_tensor = tensor.half()\n",
    "\n",
    "new_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.096  ,  0.3977 ,  1.913  , -0.2268 ,  0.5503 ],\n",
       "       [-0.838  , -0.693  , -1.229  , -0.961  ,  1.445  ],\n",
       "       [ 0.5923 , -2.684  ,  0.3794 ,  0.7896 ,  0.0817 ],\n",
       "       [-0.2156 ,  1.046  , -0.1942 ,  0.1126 , -0.0657 ],\n",
       "       [-0.899  ,  1.178  ,  0.7314 , -0.4578 ,  1.394  ],\n",
       "       [ 0.5015 ,  0.747  ,  0.452  , -0.555  ,  1.456  ],\n",
       "       [-0.2515 ,  1.233  ,  0.753  , -0.01872, -1.093  ],\n",
       "       [ 0.2747 , -0.95   , -1.289  , -0.1742 , -1.067  ],\n",
       "       [ 0.1503 ,  1.081  ,  0.4014 , -2.285  ,  0.788  ],\n",
       "       [ 0.285  ,  1.14   , -0.652  ,  0.97   ,  0.9097 ]], dtype=float16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy interoperability\n",
    "new_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upcasting\n",
    "(new_tensor + tensor).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device type\n",
    "\n",
    "PyTorch can utilize multiple device types. In general:\n",
    "- we use CPU for data loading\n",
    "- we use specialized devices (usually GPU, sometimes TPU) for running the data through neural network\n",
    "\n",
    "> TPU support is currently experimental, see challenges for more info\n",
    "\n",
    "Let's start by checking if GPU is available on our devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this information we can create a special device type that we can later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In later sections (basics of training) you will see how we can use this device variable\n",
    "for device agnostic code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Automatic differentiation\n",
    "\n",
    "In order for neural networks to learn we need to calculate gradients of `loss` w.r.t. parameters (like we did with linear regression previously).\n",
    "\n",
    "> This time differentiation graph (__sometimes also called a tape__) is provided by PyTorch\n",
    "\n",
    "![](./images/grad.jpg)\n",
    "\n",
    "To use PyTorch's [autograd](https://pytorch.org/docs/stable/autograd.html) we need a few changes in the above code.\n",
    "\n",
    "First, we have to mark tensors which require gradient using `requires_grad=True` argument during creation:\n",
    "\n",
    "> Most of PyTorch functions creating tensors like `rand`, `randn` etc. have `requires_grad` as an optional parameter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn(10, requires_grad=True)\n",
    "b = torch.tensor([1.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Only tensor of floating data type can have gradient! __No integers or a-like__\n",
    "\n",
    "After that we can use them normally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X @ W + b\n",
    "\n",
    "loss = y.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running backpropagation\n",
    "\n",
    "Like we did during \"Gradient Methods\" we can run backpropagation algorithm explicitly.\n",
    "\n",
    "> In PyTorch we run backpropagation __on tensor__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n",
      "tensor([147.2227, 142.4924, 138.7641, 148.3273, 153.3686, 148.1476, 156.7694,\n",
      "        146.8128, 155.7344, 154.1845]) tensor([300.])\n"
     ]
    }
   ],
   "source": [
    "print(W.grad, b.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# Use .grad attribute \n",
    "print(W.grad, b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Implicitly tensor with `1` is fed into `backward` __if tensor is a scalar!__\n",
    "\n",
    "> If tensor is not a scalar, you have to provide a tensor with initial gradient of specified shape, see [here](https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grad_fn, how PyTorch keeps track of operations\n",
    "\n",
    "- __PyTorch keeps functions which created the tensor (if any) inside `grad_fn`__ attribute\n",
    "- if `grad_fn` is `None` it is a tensor which:\n",
    "    - was created by user explicitly (either with `requires_grad` set to `True` or `False`)\n",
    "    \n",
    "See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7f545fcafdf0> False True\n",
      "None True True\n",
      "None True False\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn, y.is_leaf, y.requires_grad)\n",
    "\n",
    "print(W.grad_fn, W.is_leaf, W.requires_grad)\n",
    "\n",
    "print(X.grad_fn, X.is_leaf, X.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn.Module\n",
    "\n",
    "> [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) is a base class for every deep learning model in PyTorch (usually neural networks)\n",
    "\n",
    "Given that, we will inherit it from it each time we create a more complicated module.\n",
    "\n",
    "Let's see how we can code up linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn  # also common alias\n",
    "\n",
    "\n",
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self, n_features: int):\n",
    "        # This line is always required at the beginning\n",
    "        # Registers parameters of our model in graph\n",
    "        super().__init__()\n",
    "\n",
    "        self.W = torch.nn.Parameter(torch.randn(n_features))\n",
    "        self.b = torch.nn.Parameter(torch.ones(1))\n",
    "        self.other_tensor = torch.randn(5)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X @ self.W + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.Parameter\n",
    "\n",
    "> If we want a tensor to be a part of `nn.Module` we have to wrap it inside `nn.Parameter`\n",
    "\n",
    "Let's see what parameters our model currently has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W torch.Size([15])\n",
      "b torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(15)\n",
    "\n",
    "# named_parameters is a generator, you can also use parameters method\n",
    "for name, parameter in model.named_parameters():\n",
    "    print(name, parameter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see `self.other_tensor` __is not registered as a parameter__.\n",
    "\n",
    "This means, we won't be able to easily optimize it and it is \"merely an attribute\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward method\n",
    "\n",
    "Users should implement logic of the model (how data goes through neural network) inside this method.\n",
    "\n",
    "> When running data through our model we will use `__call__` method. __This ensures any hooks registered for module will run correctly__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(torch.randn(64, 15))\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input shape\n",
    "\n",
    "> PyTorch requires `(batch_size, n_features1, ..., n_features2)` tensors as input\n",
    "\n",
    "In the case above, batch size was `64` with `15` input features to linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "- Recreate linear regression using [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) (single line)\n",
    "- Create __LogisticRegression__ using `torch.nn.Linear` inside `nn.Module` (simply assign to `self`. It should take `in_features` and `out_features` and return output from the `nn.Linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression = ...\n",
    "\n",
    "class LogisticRegression(...):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic training loop\n",
    "\n",
    "Let's assume we have our data already in place (we will use `torch.random.randn` as input, __we will talk about data and datasets in the next chapter__).\n",
    "\n",
    "Below is a \"standard\" (skeletonized) training loop for regression tasks.\n",
    "\n",
    "> __Loop like this is prevalent when using \"pure\" PyTorch__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Dummy data\n",
    "X, y = torch.randn(64, 15), torch.randn(64)\n",
    "X, y = X.to(device), y.to(device)\n",
    "\n",
    "for epoch in range(20):\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    # Perform backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # Perform optimization step & zero-out gradient\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print(f\"EPOCH: {epoch} | LOSS: {loss.detach()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casting to device\n",
    "\n",
    "- __Both data and module have to be casted to device__\n",
    "- Loss is calculated using specified \"criterion\", in our case Mean Squared Error\n",
    "- `backward()` is run on resulting tensor loss\n",
    "- Optimizer takes a step and it's gradient has to be zero-ed out before next optimization step (otherwise it would be accumulated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- PyTorch can be considered as `numpy` on GPU for neural networks\n",
    "- PyTorch provides different data types:\n",
    "    - `float` is a good default value (good balance between necessary precision, performance and memory usage)\n",
    "- PyTorch can run on different devices:\n",
    "    - GPU is used for running neural networks\n",
    "    - CPU is used for data loading and other intensive tasks\n",
    "- We should write device agnostic code (basics shown here)\n",
    "- We should inherit from `torch.nn.Module` when creating neural networks\n",
    "    - Override `__init__` and `forward`\n",
    "    - Use this model via functor `__call__`\n",
    "- Basic PyTorch training loop consists of:\n",
    "    - Setting up criterion\n",
    "    - Casting model and data to device\n",
    "    - Backpropagating loss\n",
    "    - Taking optimizer step\n",
    "    - Zeroing out gradient in model using `optimizer.zero_grad()`\n",
    "\n",
    "\n",
    "## Challenges\n",
    "\n",
    "- Use your created logistic regression in this loop. Change `y` to be\n",
    "- What is \"automatic mixed precision\"? Check out [this part of documentation](https://pytorch.org/docs/stable/notes/amp_examples.html). When one should use it?\n",
    "- Read more about [TPUs](https://medium.com/pytorch/get-started-with-pytorch-cloud-tpus-and-colab-a24757b8f7fc) provided by Google. Which neural networks are likely to get performance improvements with this device?\n",
    "- Check out [CUDA semantics](https://pytorch.org/docs/stable/notes/cuda.html) in PyTorch. How to choose specific GPU device if there are multiple of them?\n",
    "- What are the aforementioned hooks? Check out [this article](https://medium.com/the-dl/how-to-use-pytorch-hooks-5041d777f904) for more information\n",
    "- Check available function and attributes of [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
