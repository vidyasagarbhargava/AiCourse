{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "## Plan\n",
    "\n",
    "- Other resources for models:\n",
    "    - Research papers and specific repos\n",
    "    - Facebook AI Research / DeepMind and a-like repositories\n",
    "- Model conversion\n",
    "- Ways to finetune model\n",
    "\n",
    "> Transfer Learning is an idea to reuse knowledge learned by models in other tasks\n",
    "\n",
    "This approach allows us to spend less time on:\n",
    "- coding and coming up with neural network architectures \n",
    "- collecting data (as large amounts of data were already used to train these networks)\n",
    "\n",
    "Furthermore, it helps with:\n",
    "- generalization (knowledge from similar domain can be easily transferable)\n",
    "- training time (weights are initialized better)\n",
    "\n",
    "And, maybe even more important, __allows us to use knowledge from datasets which are too large to train on one's machine__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchvision\n",
    "\n",
    "> `torchvision` ([documentation](https://pytorch.org/docs/stable/torchvision/models.html)) provides SOTA (or close to State Of The Art) neural network models for computer vision tasks\n",
    "\n",
    "Those models were (usually) trained on a well-known `ImageNet` dataset\n",
    "\n",
    "## ImageNet\n",
    "\n",
    "[ImageNet](http://image-net.org/) is not only a dataset, but also __a yearly held classification competition__.\n",
    "\n",
    "Overview of the dataset:\n",
    "- Over 1 million images\n",
    "- Images are of different sizes (but usually those are cropped to `224x224` - `384x384`)\n",
    "- `1000` classes (a lot, this task is hard!)\n",
    "\n",
    "> __One should keep current best models on ImageNet in mind as those are often used as standalone/part of other models!__\n",
    "\n",
    "- At this moment EfficientNet based architectures are current SOTA (original research paper [here](https://arxiv.org/abs/1905.11946))\n",
    "- __Around 90% Top-1 accuracy achieved__ (and 98% Top-5) which means __we are getting closer to solving this dataset as we have \"solved\" MNIST or CIFAR__\n",
    "\n",
    "## Using models\n",
    "\n",
    "> Loading `torchvision` models is simple, use [source code of model](https://pytorch.org/vision/0.8/models.html#torchvision.models.resnet18) to see all available arguments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "model = torchvision.models.vgg11(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision models classes\n",
    "\n",
    "Models provided by `torchvision` (and not only) can be divided into a few categories (`torchvision` addition if provided by the package):\n",
    "\n",
    "## Classification\n",
    "\n",
    "> Basic task, most of the models were trained on ImageNet (or sometimes pretrained with even larger datasets beforehand). \n",
    "\n",
    "Accuracy classification looks more or less like below (non comprehensive list and grouped by theme, full list [here](https://paperswithcode.com/sota/image-classification-on-imagenet)), sorted from best to last:\n",
    "\n",
    "- EfficientNet family - [research](https://arxiv.org/abs/1905.11946) | `EfficientNet-BN`, `EfficientNet-LN` and their variations\n",
    "- ResNet family - we saw basic idea standing behind it during convolution classes | `torchvision` | `ResNext`, `ResNet`, `Wide ResNe(X)t`\n",
    "- Inception family | `torchvision` | `InceptionV3`, `Xception`\n",
    "- MobileNets | `torchvision` | `MobileNetV{1, 2, 3}`, used as building block of EfficientNet\n",
    "- Older models of historical importance:\n",
    "    - VGG family | `torchvision` | VGG11, VGG19, large and inefficient in comparison\n",
    "    - AlexNet | `torchvision` | First neural network winning ImageNet competition\n",
    "    \n",
    "> __There are a lot of other interesting ideas presented in ImageNet related papers, read them if you are curious!__\n",
    "\n",
    "### Which model should I choose?\n",
    "\n",
    "As always, that depends on your use case, but rough guidelines could be:\n",
    "\n",
    "- __ResNets__:\n",
    "    - battle tested\n",
    "    - work really well in many tasks\n",
    "    - fast and well optimized in many frameworks (perfect for GPUs)\n",
    "    - __may not be the most efficient parameter-wise__\n",
    "    - __go to for initial runs__\n",
    "- __EfficientNets__:\n",
    "    - current SOTA\n",
    "    - may not be as general as ResNet (though research is ever growing)\n",
    "    - may not be as as optimized (ever changing, __potentially faster than ResNets, sometimes much faster__)\n",
    "    - __more efficient parameter-wise__ (smaller model than ResNet, on the order of `10`)\n",
    "    - __test when you want to push your accuracy__\n",
    "    - __test when you want to deploy to mobile and other constrained devices__ (and you need better results)\n",
    "- __MobileNets__:\n",
    "    - really fast (especially on CPU)\n",
    "    - __battle tested for edge deployment & constrained environments__ (AWS Lambda, Mobile)\n",
    "    - can be really really small (below `1KK` parameters) yet good enough\n",
    "    - __use for mobile, may handle a lot of tasks good enough!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number in ResNet tells us how many layers it has\n",
    "resnet = torchvision.models.resnet32(pretrained=True) # Loading weights trained on ImageNet\n",
    "\n",
    "# Interesting model between MobileNets and good accuracy\n",
    "mnasnet = torchvision.models.mnasnet1_0(num_classes=100) # Choosing classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other tasks\n",
    "\n",
    "- [Semantic Segmentation](https://pytorch.org/vision/stable/models.html#semantic-segmentation)\n",
    "- [Object Detection & Image Segmentation](https://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection)\n",
    "\n",
    "![](images/segmentation_vs_detection.png)\n",
    "\n",
    "[Image Source](https://towardsdatascience.com/a-hitchhikers-guide-to-object-detection-and-instance-segmentation-ac0146fe8e11)\n",
    "\n",
    "__We will not go into details about those models during this lesson__, but important things to keep in mind:\n",
    "- Those models use `classification` models seen above as __backbone__ (feature creator for specific task), __recurring theme in vision!__\n",
    "- Usually trained on large [`COCO` dataset](https://cocodataset.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try to beat old model with pretrained weights fine-tuned (we will later learn more about that) on CIFAR10 for `1` epoch!\n",
    "\n",
    "- __Run first and second cell ONLY ONCE__\n",
    "- __In the third cell:__\n",
    "    - You can use any `torchvision` model\n",
    "    - WITHOUT `pretrained` weights\n",
    "    - Train for at most `5` epochs (you can check validation accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "import torchvision\n",
    "from pl_bolts.datamodules import CIFAR10DataModule\n",
    "\n",
    "with tempfile.TemporaryDirectory() as data_dir:\n",
    "    dm = CIFAR10DataModule(\n",
    "        data_dir=data_dir, shuffle=True, num_workers=1, normalize=True, batch_size=64\n",
    "    )\n",
    "    train_dataloader = dm.train_dataloader()\n",
    "    test_dataloader = dm.test_dataloader()\n",
    "    validation_dataloader = dm.validation_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, labels):\n",
    "    return torch.sum(torch.argmax(logits, dim=-1) == labels)\n",
    "\n",
    "# Your baseline to beat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Hub\n",
    "\n",
    "> PyTorch provides hub from which one can simply download models ([page](https://pytorch.org/hub/) | [module](https://pytorch.org/docs/stable/hub.html))\n",
    "\n",
    "It works in a similar fashion to `torchvision` and is currently being developed as __official source of PyTorch models__.\n",
    "\n",
    "- Anyone can make their models work with PyTorch Hub\n",
    "- `torchvision` models are available through it\n",
    "- Other, non vision models are also provided (including NLP, Audio, Generative)\n",
    "\n",
    "One can easily see available models in repository usuing `torch.hub.list`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/intel-isl/MiDaS/archive/master.zip\" to /home/vyz/.cache/torch/hub/master.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['MiDaS', 'MiDaS_small', 'MidasNet', 'MidasNet_small', 'transforms']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.hub.list(github=\"intel-isl/MiDaS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to find repositories?\n",
    "\n",
    "- Official repositories are linked on [PyTorch Hub](https://pytorch.org/hub/) webpage\n",
    "- Non-official and hosted by users can be found in some repositories (still not such a common practice), __look for `hubconf.py` at the root of github project__ (and see next sections)\n",
    "\n",
    "## More PyTorch Hub commands\n",
    "\n",
    "> Watch out, some models are really large!\n",
    "\n",
    "There are more commands useful for exploration, let's see the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/archive/master.zip\" to /tmp/tmpaqud08rt/master.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alexnet', 'deeplabv3_mobilenet_v3_large', 'deeplabv3_resnet101', 'deeplabv3_resnet50', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'fcn_resnet101', 'fcn_resnet50', 'googlenet', 'inception_v3', 'lraspp_mobilenet_v3_large', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'mobilenet_v3_large', 'mobilenet_v3_small', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'wide_resnet101_2', 'wide_resnet50_2']\n",
      "\n",
      "    Constructs a large MobileNetV3 architecture from\n",
      "    `\"Searching for MobileNetV3\" <https://arxiv.org/abs/1905.02244>`_.\n",
      "\n",
      "    Args:\n",
      "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
      "        progress (bool): If True, displays a progress bar of the download to stderr\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /tmp/tmpaqud08rt/pytorch_vision_master\n",
      "Using cache found in /tmp/tmpaqud08rt/pytorch_vision_master\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /tmp/tmpaqud08rt/checkpoints/mobilenet_v3_large-8738ca79.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c872f4238f46499df19f18e74684be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/21.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "# This directory will be removed after we leave context manager\n",
    "with tempfile.TemporaryDirectory() as directory:\n",
    "    # Where model will be downloaded\n",
    "    torch.hub.set_dir(directory)\n",
    "\n",
    "    print(torch.hub.list(\"pytorch/vision\"))\n",
    "\n",
    "    print(torch.hub.help(\"pytorch/vision\", model=\"mobilenet_v3_large\"))\n",
    "\n",
    "    model = torch.hub.load(\n",
    "        \"pytorch/vision\", model=\"mobilenet_v3_large\", pretrained=True, progress=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method xpu in module torch.nn.modules.module:\n",
      "\n",
      "xpu(device: Union[int, torch.device, NoneType] = None) -> ~T method of torchvision.models.mobilenetv3.MobileNetV3 instance\n",
      "    Moves all model parameters and buffers to the XPU.\n",
      "    \n",
      "    This also makes associated parameters and buffers different objects. So\n",
      "    it should be called before constructing optimizer if the module will\n",
      "    live on XPU while being optimized.\n",
      "    \n",
      "    Arguments:\n",
      "        device (int, optional): if specified, all parameters will be\n",
      "            copied to that device\n",
      "    \n",
      "    Returns:\n",
      "        Module: self\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding out more aoubt downloaded methods\n",
    "\n",
    "methods = dir(model) # available methods\n",
    "\n",
    "# Info about specific model's method\n",
    "help(model.xpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch-Lightning\n",
    "\n",
    "> Lightning Bolts provide a few well recognized models , see [documentation](https://pytorch-lightning-bolts.readthedocs.io/en/latest/#vision)\n",
    "\n",
    "- Provides architectures, __rarely pretrained weights__ (or you have to get them on your own)\n",
    "- Selection of models is not very large currently\n",
    "- __Useful to understand specific models and see them implemented__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_bolts.models.gans import GAN\n",
    "\n",
    "# Basic GAN network\n",
    "# IT DOES NOT HAVE PRETRAINED WEIGHTS!\n",
    "model = GAN(input_channels=3, input_height=28, input_width=28, latent_dim=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other sources\n",
    "\n",
    "What if we can't find a desirable model? There are a few available alternatives:\n",
    "- [paperswithcode](https://paperswithcode.com/) - outline current SOTA results including only papers with available source code (__quality of implementation not measured!__)\n",
    "- [arxiv](https://arxiv.org/) - except research models, links to GitHub repositories are __sometimes__ provided, __usually in the abstract__\n",
    "- GitHub accounts of respected research labs (also includes interesting technical solutions):\n",
    "    - [Facebook Research](https://github.com/facebookresearch) - General | Vision\n",
    "    - [DeepMind](https://github.com/deepmind) - General | Reinforcement Learning\n",
    "    - [Google Research](https://github.com/google-research) - General | Health, Business use\n",
    "    - [OpenAI](https://github.com/openai/) - General | NLP, large networks\n",
    "    - [Microsoft Research](https://github.com/MicrosoftResearch) (more technical and less DL based, General)\n",
    "    - [NVIDIA Research](https://github.com/NVlabs) - General | GANs, large scale networks\n",
    "- [Distill.pub](https://distill.pub/) - research reviews & other publications, sometimes with code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Conversion\n",
    "\n",
    "> Some models are implemented in different frameworks (usually Tensorflow). We can use `ONNX` to make a conversion\n",
    "\n",
    "# ONNX\n",
    "\n",
    "> [ONNX](https://github.com/onnx/onnx) provides an open source format for AI models, both deep learning and traditional ML\n",
    "\n",
    "- Transform models into open exchange framework `.onnx`\n",
    "- Supported by major frameworks/tools\n",
    "\n",
    "## Downsides\n",
    "\n",
    "- Not all operations between frameworks are interchange'able\n",
    "- For SOTA models conversion might be hard\n",
    "- Puts constrains on some of the frameworks (e.g. PyTorch)\n",
    "\n",
    "> We will see another way to export models for usage in different than Python environments later during `torchscript` lesson\n",
    "\n",
    "> __`ONNX` should be used with care and only for inter-framework conversions.__\n",
    "\n",
    "> We don't want you to know `ONNX` in and out, just keep this tool in mind when the right time comes!\n",
    "\n",
    "## Why would I leave my framework?\n",
    "\n",
    "PyTorch is great, but there are a few cases you might encounter were you need to switch, including:\n",
    "- Part of team (or another team) uses different technology\n",
    "- PyTorch does not support some form of deployment (which Tensorflow might)\n",
    "- Hardware specific optimization is required and not possible in PyTorch\n",
    "- Other parts of the pipeline are implemented in different framework\n",
    "\n",
    "> Above (and many more) reasons also apply to other deep learning/machine learnig frameworks\n",
    "\n",
    "## PyTorch front end\n",
    "\n",
    "Let's see how we can export our PyTorch models to `ONNX` format using `torch.onnx` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "dummy_input = torch.randn(10, 3, 224, 224, device=\"cuda\")\n",
    "model = torchvision.models.alexnet(pretrained=True).cuda()\n",
    "\n",
    "# Providing input and output names sets the display names for values\n",
    "# within the model's graph. Setting these does not change the semantics\n",
    "# of the graph; it is only for readability.\n",
    "#\n",
    "# The inputs to the network consist of the flat list of inputs (i.e.\n",
    "# the values you would pass to the forward() method) followed by the\n",
    "# flat list of parameters. You can partially specify names, i.e. provide\n",
    "# a list here shorter than the number of inputs to the model, and we will\n",
    "# only set that subset of names, starting from the beginning.\n",
    "input_names = [\"actual_input_1\"] + [\"learned_%d\" % i for i in range(16)]\n",
    "output_names = [\"output1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "\n",
    "> Transfer learning is a process of reusing model(s) taught on another task and adjusting to our needs\n",
    "\n",
    "## Per-domain models\n",
    "\n",
    "There are some rough guidelines for different tasks:\n",
    "- Vision:\n",
    "    - ImageNet models (classification)\n",
    "    - COCO pretrained models (with pretrained backbones from ImageNet classification)\n",
    "- NLP:\n",
    "    - Pretrained word embeddings\n",
    "    - Large Transformer based architectures (usually BERT and it's variations)\n",
    "    - __Still emerging approach__\n",
    "    \n",
    "For other tasks (e.g. reinforcement learning, one shot learning, GANs) transfer learning is not yet so widespread.\n",
    "\n",
    "> Probably more pretrained models for different domains will emerge, as we have seen with vision and NLP tasks after that\n",
    "\n",
    "Aforementioned domain-specific models use pretrained networks from vision (most often) as part of their model though.\n",
    "\n",
    "## How to finetune?\n",
    "\n",
    "We will focus on vision and classification tasks, though similar approach is used for NLP.\n",
    "\n",
    "## Weight freezing\n",
    "\n",
    "> Weight freezing means freezing __backbone__ (layers creating features) so __those will not learn anything__ and __only enabling last layer to learn on provided data__\n",
    "\n",
    "### Pros\n",
    "\n",
    "- __The more you freeze, the faster your neural network will run and less memory it will take!__\n",
    "- Easier to finetune and \"get right\"\n",
    "- We surely will not \"destroy\" weights learned on other task (which may sometimes occur at the beginning of training due to random initialization of layer)\n",
    "\n",
    "### Cons\n",
    "\n",
    "- Representational power is limited (as we cannot change frozen weights)\n",
    "- We usually will not get best possible result (though we will get it faster)\n",
    "\n",
    "### Tips\n",
    "\n",
    "- There is no strict rule, you may unfreeze more parts of the network (though less common)\n",
    "- You may start with weight freezing, unfreeze afterwards and finish with small learning rate (or disciminative learning rate), though __this will make the optimization procedure significantly harder__ to implement and reason about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminative learning rates\n",
    "    \n",
    "> Discriminative learning mean setting different learning rates for different part of the neural network\n",
    "\n",
    "### Pros\n",
    "\n",
    "- Larger representation space\n",
    "- Probably better accuracy score\n",
    "- We won't destroy pretrained weights (as their learning rate is smaller\n",
    "\n",
    "### Cons\n",
    "\n",
    "- __Way longer__ time to train as the whole network is used\n",
    "- __Harder to finetune__ and \"get right\"\n",
    "\n",
    "### Tips\n",
    "\n",
    "- Divide your neural networks into few regions:\n",
    "    - head should have standard learning rate\n",
    "    - middle of the network should have it the same, but divided by `10`\n",
    "    - first layers (finding general features) should have it the same, but divided by `10`\n",
    "- `10` is not a strict rule but seems to work well in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "> __THIS EXERCISE HAS TIME LIMIT AND BEST ACCURACY WINS (time limit will be set by instructor)__\n",
    "\n",
    "> __DO NOT WRITE ANYTHING AT THE START. YOU HAVE 5 MINUTES TO READ INSTRUCTIONS AND COME UP WITH STRATEGY (MAKE IT A TEAM WORK!)__\n",
    "\n",
    "- Implement `freeze` function taking in neural network  and setting `requires_grad_(False)` on each parameter\n",
    "- Do the same for `unfreeze` but set parameter's gradient to `True`\n",
    "- Load any model you want from `torchvision`:\n",
    "    - The larger the better, but may not fit on the GPU\n",
    "    - Use knowledge from the beginning when choosing it\n",
    "- Print the model to get a little info about it's structure (backbone, bottleneck etc.)\n",
    "- Write training loop for CIFAR100 (or use PyTorch Lightning/any other framework you feel most comfortable with)\n",
    "\n",
    "Now you can follow one of two ways (or mix them), take into consideration both ways and choose wisely!\n",
    "\n",
    "## Weight freezing\n",
    "\n",
    "- Use `freeze` to freeze part of the module, up to you how many layers will be frozen (start easy, like freezing whole backbone and training only head)\n",
    "- Train your neural network (remember about the time!)\n",
    "\n",
    "## Discriminative learning rates\n",
    "\n",
    "- Set different learning rates for different parts of the network (any optimizer/scheduler you want and think can fit in time)\n",
    "- Train your neural network (remember about the time!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(module: torch.nn.Module):\n",
    "    ...\n",
    "    \n",
    "def unfreeze(module: torch.nn.Module):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "### Assessment\n",
    "\n",
    "- What is \"knowledge distillation\"? Where is it used and what are the reasons?\n",
    "- What is \"quantization\"? Why is it useful? When should we use it? Read about it in [PyTorch documentation](https://pytorch.org/docs/stable/quantization.html)\n",
    "\n",
    "### Non-assessment\n",
    "\n",
    "- Read about necessary steps to publish your models to PyTorch Hub [here](https://pytorch.org/hub/)\n",
    "- What are [Adapters](https://arxiv.org/pdf/1902.00751.pdf)? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
