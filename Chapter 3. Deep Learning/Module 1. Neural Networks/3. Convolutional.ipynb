{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural networks\n",
    "\n",
    "## Why?\n",
    "\n",
    "- Fully Connected networks look at input vector as an independent features\n",
    "- Images are __clearly__ dependent; one can estimate pixel values from the ones surrounding it (or vice versa)\n",
    "- Convolution takes this interaction an account (relative position of feature is used)\n",
    "- Convolution models spatial relationships between features (not only images, but also soundwaves, 3D models, videos, words in sentences etc.)\n",
    "- Smaller number of parameters allows us to use deeper and more complicated layers\n",
    "- Simpler architecture (less parameters), tailored to spatially structured data\n",
    "- __Convolutional neural networks find higher level features (a.k.a. representations) useful for final classification layers__\n",
    "\n",
    "## What is convolution?\n",
    "\n",
    "> Kernel (vector/matrix/cube) \"sliding\" over input data (multiplying it with kernel values) and summing them together\n",
    "\n",
    "For binary case:\n",
    "\n",
    "![image](images/convolution_animation.gif)\n",
    "\n",
    "- Kernel is applied on first image patch. It's values are multiplied by respective values in the image\n",
    "- After multiplication all of those values are summed __returning single element__ (you can think of it as a new pixel.\n",
    "- This process is repeated until end of image is reached and new matrix is created.\n",
    "\n",
    "Below are change'able arguments of convolution (those are it's __hyperparameters__):\n",
    "\n",
    "# Convolution parameters\n",
    "\n",
    "## Kernel size\n",
    "\n",
    "> Dimensionality of kernel. In case above it was `3x3` kernel (pretty popular choice).\n",
    "\n",
    "- It can be specified as a tuple, e.g. `(3, 3)`\n",
    "- It can be irregular (e.g. `(3, 2)`), __though it is rarely a case__ (if it is, it almost always is `(N, 1)` or `(1, N)`)\n",
    "- The larger the kernel, the larger is it's __receptive field__ but more computations have to be performed\n",
    "\n",
    "## Stride\n",
    "\n",
    "> Number of pixels we shift our kernel in a certain direction. In case above it was `(1, 1)`\n",
    "\n",
    "- It can be specified as a tuple, e.g. `(1, 1)`\n",
    "- It can be irregular (e.g. `(2, 1)`), __though it is almost never a case__ (specific use cases, __we shouldn't be concerned with this possibility__\n",
    "- The larger the stride, the more features from original image we miss\n",
    "- The larger the stride, the smaller output image becomes\n",
    "- The larger the stride, the less operations have to be performed\n",
    "- __Due to above `stride=1` is the most common value__\n",
    "\n",
    "\n",
    "## Padding\n",
    "\n",
    "> Addition values around the image (usually zeros). In case above __there was no padding (a.k.a. \"valid\" padding)__\n",
    "\n",
    "![image](images/CNN_diagram.JPG)\n",
    "\n",
    "- It can be specified as a tuple, e.g. `(1, 1)`\n",
    "- One can choose from a few modes, specifically (see [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)):\n",
    "    - `zeros` - output is padded with zero values; __most common & default__\n",
    "    - `reflect` - output is reflected, same as `replicate` for `1`, after that consecutive neighboring values ared used; __used for medical imaging, sometimes image segmentation etc.__\n",
    "    - `replicate` - last value at the border is used; __rare__\n",
    "    - `circular` - first and consecutive values are used; __rare__\n",
    "\n",
    "### Okay but why?\n",
    "\n",
    "> Without padding image shrinks (when `kernel_size` > 1)\n",
    "\n",
    "Assume we stack a few convolutions, one after another. __After a while our image will be a single pixel!__\n",
    "\n",
    "- If we add padding we can preserve image's size without introducing noise (or at least too much of it)\n",
    "- Pixels on the edge do not contribute as much to the kernel. If we add appropriate padding they contribute the same as the rest.\n",
    "- Acts as a mild regularizer (depending on the mode)\n",
    "\n",
    "### Tips\n",
    "\n",
    "- Specifying `padding` with \"default values\" (like `stride=1`, `dilation=1`) is easy and can be done using:\n",
    "\n",
    "$$\n",
    "\\lfloor\\frac{\\text{kernel_size}}{2}\\rfloor\n",
    "$$\n",
    "\n",
    "- Exact output size dependent on parameters is provided by PyTorch in [Shape](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)\n",
    "- Some libraries provide padding `\"same\"` which calculates padding for you dynamically ([torchlayers](https://github.com/szymonmaszke/torchlayers) for PyTorch, Keras & Tensorflow have it out of the box)\n",
    "\n",
    "## Dilation\n",
    "\n",
    "> Dilation means spacing between kernel elements\n",
    "\n",
    "![dilated](images/dilated.jpg)\n",
    "\n",
    "- Increased receptive field\n",
    "- Some pixels are missed but this information can be approximated\n",
    "- __Also called `atrous convolution`__\n",
    "\n",
    "### Tips\n",
    "\n",
    "- Useful for very large images\n",
    "- Useful when we want to get more global representations of images\n",
    "- Most useful in the early layers\n",
    "- Can be concatenated with standard convolution\n",
    "\n",
    "# Convolutional layer\n",
    "\n",
    "## History\n",
    "\n",
    "> Originally, people created those kernels \"by hand\"\n",
    "\n",
    "For example __Sobel filter__ is used to find edges in the image (read more [here](https://en.wikipedia.org/wiki/Sobel_operator))\n",
    "\n",
    "- Those were very specific (edge detection, face detection, image gradient detection)\n",
    "- Hard to come up with\n",
    "- Non-specific to images\n",
    "\n",
    "In 1989 [Yann LeCun](http://yann.lecun.com/ex/research/index.html) came up with an idea to make convolution a neural network layer to solve above shortcomings.\n",
    "\n",
    "## Modern era\n",
    "\n",
    "> Convolutional filter in Deep Learning has learnable connections instead of hard-fixed values\n",
    "\n",
    "This approach solves all of the problems outlined in `History`, but we need some more nomenclature to fully understand this idea:\n",
    "\n",
    "## Input channels\n",
    "\n",
    "> Number of channels entering convolutional layer\n",
    "\n",
    "![image](images/CNN_RGB.JPG)\n",
    "\n",
    "- __Each input channel has it's own set of kernels (which are now weights)!__\n",
    "- Given above we already have weights of shape `(in_channels, width, height)`\n",
    "\n",
    "__Usually, during first convolutional layer, `in_channels=3` (Red, Green Blue) or `1` (grayscale images)\n",
    "\n",
    "## Output channels\n",
    "\n",
    "> Number of channels created by convolution operation (number of input channels doesn't matter)\n",
    "\n",
    "- Each output channel (with it's filters) convolves over __all input channels__ and sums the result\n",
    "- Given above, we have weights of shape `(out_channels, in_channels, width, height)`\n",
    "\n",
    "__Finally data is produced of shape `(batch, out_channels, width, height)`__\n",
    "\n",
    "## Filters\n",
    "\n",
    "> Collection of kernels (sometimes named as channels)\n",
    "\n",
    "In case of `Conv2d` it will be `(in_channels * out_channels)`\n",
    "\n",
    "# Benefits of convolution\n",
    "\n",
    "- Much smaller amount of parameters (when compared to `nn.Linear`)\n",
    "- Same parameters go over regions of an image (with `nn.Linear` each parameter would be responsible for one pixel)\n",
    "- Tailored for this specific task (architecture)\n",
    "- __Much__ higher performance on spatial tasks\n",
    "\n",
    "### Exercise\n",
    "\n",
    "__Use `torch.nn` package for layer creation__\n",
    "\n",
    "__Layers should be placed in a generator/list and iterated over at the end!__\n",
    "\n",
    "- Create `Conv1d`, `Conv2d` and `Conv3d` with given `in_channels`, `out_channels` and `kernel_size`.\n",
    "- Create `nn.Linear` with specified `in_features` and `out_features` by `flattened_input_images_sizes` and `flattened_output_images_sizes` elements respectively (__tip:__ use `zip` function and list comprehension inside the top level list)\n",
    "- Iterate over created layers:\n",
    "    - `print` their name (each layer has it's textual representation defined with `__str__`)\n",
    "    - `print` their shape\n",
    "    - `print` total number of parameters\n",
    "    \n",
    "__Analyze the results in groups and come up with conclusions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d(32, 32, kernel_size=(5,), stride=(1,))\n",
      "torch.Size([32, 32, 5])\n",
      "69\n",
      "Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "torch.Size([32, 32, 5, 5])\n",
      "74\n",
      "Conv3d(32, 32, kernel_size=(5, 5, 5), stride=(1, 1, 1))\n",
      "torch.Size([32, 32, 5, 5, 5])\n",
      "79\n",
      "Linear(in_features=784, out_features=784, bias=True)\n",
      "torch.Size([784, 784])\n",
      "1568\n",
      "Linear(in_features=1024, out_features=1024, bias=True)\n",
      "torch.Size([1024, 1024])\n",
      "2048\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "in_channels = 32\n",
    "out_channels = 32\n",
    "kernel_size = 5\n",
    "\n",
    "flattened_input_images_sizes = [28 * 28, 32 * 32]\n",
    "flattened_output_images_sizes = [28 * 28, 32 * 32]\n",
    "\n",
    "# Your code here \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does each filter look for?\n",
    "\n",
    "> Here's what some simple, small, 1 channel filters might look like after they've been trained.\n",
    "\n",
    "![](images/filters.png)\n",
    "\n",
    "> Convolutional neural networks are often represented by diagrams like the one below:\n",
    "\n",
    "![](images/cnn.png)\n",
    "\n",
    "# Pooling\n",
    "\n",
    "> Pooling allows us to control when we want to shrink image's width and height\n",
    "\n",
    "![](images/maxpoolfig.gif)\n",
    "\n",
    "## Why?\n",
    "\n",
    "- As the network gets depeer __we increase number of channels__ in order to learn more abstract representations, hence __computational cost increases quickly__\n",
    "- Pooling allows us to control computational cost of operations\n",
    "- Pooling chooses the most important features from the image\n",
    "\n",
    "## Versions\n",
    "\n",
    "There are a couple versions of pooling, most prevalent are:\n",
    "- `MaxPooling`\n",
    "- `AvgPooling` (taking average of kernel values)\n",
    "\n",
    "### MaxPooling\n",
    "\n",
    "- Chooses most important features\n",
    "- Sharper decisions\n",
    "- __Might__ be easier to train but __might__ be worse on validation\n",
    "- __Most popular__\n",
    "- Suitable for large networks with enough capacity (layers) to find most important features\n",
    "\n",
    "### AvgPooling\n",
    "\n",
    "- Takes all features into an account\n",
    "- Smoother decision boundary\n",
    "- __Might__ be harder to train but __might__ be better on validation\n",
    "- __Less popular__\n",
    "- Suitable for smaller networks with lower capacity as it doesn't leave any feature behind\n",
    "\n",
    "## Tips\n",
    "\n",
    "- At the start of neural network __do not use pooling__. Go for a couple layers/blocks (like 3/4) and pool after that\n",
    "- After initial convolution layers you may pool every 2 layers/blocks (though you might go for more)\n",
    "- Provided as [torch.nn.AvgPool2d](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html) \n",
    "\n",
    "\n",
    "# Global Pooling\n",
    "\n",
    "> At the end of neural network we often need to output `(batch_size, classes)` (for classification\n",
    "\n",
    "Due to above, we need to go from shape `(batch_size, channels, width, height)`. Given that:\n",
    "- Abstract features are gathered in channels, __not width and height__\n",
    "\n",
    "We can use so called `GlobalPooling`, provided in PyTorch as [`AdaptiveMaxPool2d`](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool2d.html) (specify `1` to get `GlobalPooling`):\n",
    "\n",
    "> Global pooling works like a normal pooling, but __always return image with single pixel__, hence output shape will be `(batch_size, channels, 1, 1)` (__remember to `squeeze` dimensions before passing them to `torch.nn.Linear`!)\n",
    "\n",
    "__Global pooling also comes in a few flavors, including `max` and `avg`__\n",
    "\n",
    "# Exercise\n",
    "\n",
    "__Create convolutional neural network on your own!__\n",
    "\n",
    "See comments below for characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your neural network here\n",
    "# in_channels=3, classes=1000\n",
    "# Go for 10 layers, pooling after layer 4, 6, 8\n",
    "# Global pooling after 10th layer and end with `nn.Linear(channels, classes)\n",
    "# use nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNets\n",
    "\n",
    "__One of the most important papers in deep learning [link](https://arxiv.org/abs/1512.03385)__\n",
    "\n",
    "> Instead of learning feature transformations as we add convolutional layers, __we learn corrections__ to the previous layers\n",
    "\n",
    "Let's assume `F(x)` is a block containing two convolutional layers transforming `x` inputs. In ResNets case we would add a __skip connection__:\n",
    "\n",
    "![resnet_equation](images/resnet_equation.png)\n",
    "\n",
    "## Degradation problem\n",
    "\n",
    "> When the layer is too deep (on the order of `100` convolutional layers) __it is too hard to optimize__\n",
    "\n",
    "Due to that:\n",
    "- __Deeper networks have larger train loss than more shallow ones!__ `50` layers would perform better than `1000` (which is not intuitive)\n",
    "\n",
    "> __ResNets main addition is solving degradation problem__, due to that we can easily go with `1000` layer networks (though it's __almost always__ not needed for most tasks)\n",
    "\n",
    "> Anywhere from `18` to `152` layers should be enough for any task (use the smallest one satisfying your needs\n",
    "\n",
    "\n",
    "## Additional benefits\n",
    "\n",
    "- Further (after `BatchNorm`) loss landscape smoothing\n",
    "- Further reduction in vanishing/exploding gradient (though mostly taken care of by `BatchNorm` already)\n",
    "\n",
    "## Usage tips\n",
    "\n",
    "- Use any block and connect it using skip connection\n",
    "- Usability is not limited to `convolution`, same thing applies for `linear` (though rarely for recurrent neural networks)\n",
    "- __Try to keep the same size of `inputs` and `outputs` in order not to use projection__, a few resnet blocks with the same number of channels and up the number two times\n",
    "- __DO NOT USE ACTIVATION AT THE END OF LAST LAYER INSIDE RESNET BLOCK__\n",
    "\n",
    "## Exercise\n",
    "\n",
    "__Create `Residual` block!__\n",
    "\n",
    "- `__init__`:\n",
    "    - Take two arguments, one `module`, another one `projection` with default value `None`, both being `torch.nn.Module` instances!\n",
    "- `forward`:\n",
    "    - Pass `inputs` through `self.module` creating `outputs` variable\n",
    "    - If `self.projection` is not `None`, modify `inputs` by passing them through `self.projection`\n",
    "    - Finally add `outputs` and `inputs` (with optional projection) together\n",
    "    \n",
    "Create a few residual blocks (with any convolutions inside it), use projection if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(torch.nn.Module):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Convolutional layers are used with spatial dependent data (usually images)\n",
    "- Can be used for any task just like `nn.Linear` layers\n",
    "- Different versions exist (`1D`, `2D`, `3D`) for different tasks, but usually, respectively:\n",
    "    - Textual (encoded) data or audio\n",
    "    - Images\n",
    "    - Videos (rarest case due to large amount of data)\n",
    "- __Convolution can work on images of any size__ (only `channels` dimension has to be the same)\n",
    "- Convolution has trainable kernels of specified size which together form filters\n",
    "- We increase number of channels while simultaneuosly reducing image's size in order to learn more abstract features\n",
    "- Neural network usually ends with global pooling (and optionally some task specific Linear layers, though modern architectures tend to go for single `nn.Linear`)\n",
    "- Skip connections are used to combat degradation problem\n",
    "- Skip connections allow us to use way deeper networks and optimize the system easier\n",
    "\n",
    "## Challenges\n",
    "\n",
    "- What are pretrained models and how to use them properly?\n",
    "- What is depthwise convolution?\n",
    "- What is pointwise convolution?\n",
    "- If you know above, `Separable` convolution is depthwise followed by pointwise convolution\n",
    "- How does MobileNetV2 work and why is it useful?\n",
    "- What is Squeeze-Excitation block, what are the upsides and downsides of adding it? Where should we add it? Read [Squeeze-Excitation research paper](https://arxiv.org/abs/1709.01507)\n",
    "- Read [EfficientNet research paper ](https://arxiv.org/abs/1905.11946) to know current SOTA architecture on ImageNet\n",
    "- Some additional concepts that one might want to read: shuffle nets, inception blocks, \n",
    "- How does attention on images work? Check attention on your own or __come back after attention classes__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
