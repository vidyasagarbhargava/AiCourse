{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Networks (FCNs) & activations\n",
    "\n",
    "## What are they?\n",
    "\n",
    "- Powerful function approximators\n",
    "- Universal approximators (assuming infinite width and/or depth)\n",
    "- Essentially stacked linear regressions with multiple outputs\n",
    "\n",
    "## What they do?\n",
    "\n",
    "Previously we used polynomial features for non-linear datasets, but this comes up with downsides:\n",
    "- what degree of polynomial should we use?\n",
    "- maybe other functions would be better (they usually are)?\n",
    "- if so, what those functions are?\n",
    "\n",
    "![](./images/complex-fn.png)\n",
    "\n",
    "We might come to the conclusion that:\n",
    "\n",
    "> it would be best to learn those functions directly from data\n",
    "\n",
    "... and that's what neural networks do.\n",
    "\n",
    "## Perceptron and it's limitations\n",
    "\n",
    "> Perceptron is binary logistic regression, neural network with one output neuron and one layer\n",
    "\n",
    "It is able to learn __linearly separable data__ but it will fail for non-linear data.\n",
    "\n",
    "> __Let's see a very famous XOR problem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted logits:\n",
      " tensor([[ 0.1166],\n",
      "        [ 0.3109],\n",
      "        [-0.2655],\n",
      "        [-0.0712]], grad_fn=<AddmmBackward>)\n",
      "Predicted labels:\n",
      " tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "Targets:\n",
      " tensor([0., 1., 1., 0.])\n",
      "Weights:\n",
      "\n",
      "tensor([[-0.3821,  0.1943]])\n",
      "tensor([0.1166])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def xor_problem(model):\n",
    "\n",
    "    inputs = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]]).float()\n",
    "    targets = torch.tensor([0, 1, 1, 0]).float()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for _ in range(10000):\n",
    "        outputs = model(inputs).squeeze()\n",
    "\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(\"Predicted logits:\\n\", model(inputs))\n",
    "    print(\"Predicted labels:\\n\", (model(inputs) > 0).float())\n",
    "    print(\"Targets:\\n\", targets)\n",
    "    print(\"Weights:\\n\")\n",
    "\n",
    "    for parameter in model.parameters():\n",
    "        print(parameter.data)\n",
    "\n",
    "\n",
    "xor_problem(torch.nn.Linear(2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple model is unable to learn classification for XOR data\n",
    "\n",
    "![](./images/shallow-vs-deep.png)\n",
    "\n",
    "Let's try adding another layer with size equal to `2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted logits:\n",
      " tensor([[ 0.0873],\n",
      "        [-0.0803],\n",
      "        [ 0.0959],\n",
      "        [-0.0716]], grad_fn=<AddmmBackward>)\n",
      "Predicted labels:\n",
      " tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "Targets:\n",
      " tensor([0., 1., 1., 0.])\n",
      "Weights:\n",
      "\n",
      "tensor([[-0.5845, -0.2504],\n",
      "        [ 0.5905, -0.4012]])\n",
      "tensor([-0.3885, -0.5605])\n",
      "tensor([[0.2497, 0.2618]])\n",
      "tensor([0.3310])\n"
     ]
    }
   ],
   "source": [
    "xor_problem(torch.nn.Sequential(torch.nn.Linear(2, 2), torch.nn.Linear(2, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single linear transformation (multiplication by weights of model):\n",
    "- stretches the input space by a certain factor in some direction\n",
    "- adding a constant (bias) shifts it\n",
    "\n",
    "> If we add more linear layers the whole transformation is still linear!\n",
    "\n",
    "![](./images/factor-proof.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations\n",
    "\n",
    "To combat this phenomena, we need to apply __non-linear transformations__ after some (usually all) layers:\n",
    "\n",
    "![](./images/activation.png)\n",
    "\n",
    "> Composition of non-linear functions makes the whole transformation non-linear\n",
    "\n",
    "There are multiple available activation functions, including (but not limited to):\n",
    "- sigmoid\n",
    "- tanh\n",
    "- ReLU\n",
    "- Leaky ReLU\n",
    "\n",
    "![](./images/activ-fns.png)\n",
    "\n",
    "> Activation functions were introduced in order to combat issues of previous dominant activation functions\n",
    "\n",
    "## Sigmoid\n",
    "\n",
    "- Initial neural network activation function\n",
    "- Squashes input to `[0, 1]` range (neuron on or off)\n",
    "\n",
    "But this activation function has the following drawbacks:\n",
    "- Non zero centered\n",
    "- __Oversaturation__ (most severe drawback)\n",
    "\n",
    "### Non-zero centered\n",
    "\n",
    "> Neural networks expect data to be zero-centered\n",
    "\n",
    "If the data coming into neural network is always positive (as is the case with sigmoid) gradient will become either all positive for every neuron or negative.\n",
    "\n",
    "This leads to zig-zagging during training, especially for smaller batches\n",
    "\n",
    "> Larger batches mostly mitigate this issue, as the gradient will be averaged across many examples (some positive, some negative for different weights)\n",
    "\n",
    "## Tanh \n",
    "\n",
    "Hyperbolical tangens solves this issue, but:\n",
    "\n",
    "> Tanh also has oversaturation drawbacks\n",
    "\n",
    "## Oversaturation\n",
    "\n",
    "> When neuron activation saturates at the tails (large positive/negative values) local gradient becomes zero (close to zero)\n",
    "\n",
    "> This local gradient is multiplied by the previous layers local gradients and dies, phenomena known as __dying gradient__ (especially for deeper networks)\n",
    "\n",
    "## Exercise\n",
    "\n",
    "- Analyze `gradient_print` given below and make sure you know what it does.\n",
    "- Analyze `Initializer` class and make sure you know what it does\n",
    "- Create multiple layer neural network with `tanh` and `sigmoid` activation (try a few options, start with `7` layers and go lower)\n",
    "- Use `Initializer` and `torch.nn.Module`'s `apply` function to manipulate weights in order to see dying gradient after backpropagation.\n",
    "- How many layers do we need in order __not to__ use `Initializer`? Check multiple values (you might do that in a loop and adjust the code as needed)\n",
    "- Finally, create a neural network which solves the xor problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_print(model):\n",
    "\n",
    "    inputs = torch.rand(32, 10)\n",
    "    targets = torch.randint(low=0, high=5, size=(32,))\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    loss = torch.nn.functional.cross_entropy(outputs, targets)\n",
    "    loss.backward()\n",
    "\n",
    "    for i, parameter in enumerate(model.parameters()):\n",
    "        print(f\"\\n\\n--------- PARAMETER {i} GRADIENT ---------\\n\\n\")\n",
    "        print(parameter.grad)\n",
    "\n",
    "\n",
    "class Initializer:\n",
    "    def __init__(self, target_layer, multiplier):\n",
    "        self._counter = -1\n",
    "        self.target_layer = target_layer\n",
    "        self.multiplier = multiplier\n",
    "\n",
    "    def __call__(self, submodule):\n",
    "        self._counter += 1\n",
    "        if self._counter == self.target_layer:\n",
    "            submodule.weight.data = (\n",
    "                torch.ones_like(submodule.weight.data) * self.multiplier\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------- PARAMETER 0 GRADIENT ---------\n",
      "\n",
      "\n",
      "tensor([[3.9513e-08, 7.9359e-08, 6.6509e-08, 5.6311e-08, 7.1541e-08, 2.8979e-08,\n",
      "         8.8074e-08, 4.1238e-08, 8.5469e-08, 3.5543e-08],\n",
      "        [3.8386e-08, 6.8035e-08, 5.6585e-08, 4.7749e-08, 6.6983e-08, 2.8352e-08,\n",
      "         7.5741e-08, 3.8296e-08, 7.5845e-08, 3.4881e-08],\n",
      "        [3.8629e-08, 7.7392e-08, 6.4328e-08, 5.2117e-08, 7.0474e-08, 2.5492e-08,\n",
      "         8.0053e-08, 3.7269e-08, 8.1557e-08, 3.0715e-08],\n",
      "        [3.4803e-08, 7.3151e-08, 6.2568e-08, 5.2504e-08, 6.8592e-08, 2.6935e-08,\n",
      "         8.1884e-08, 3.8729e-08, 7.9126e-08, 3.0244e-08],\n",
      "        [3.9992e-08, 8.3039e-08, 6.5890e-08, 5.7668e-08, 7.0517e-08, 3.3804e-08,\n",
      "         8.7699e-08, 3.9604e-08, 8.7403e-08, 3.7433e-08]])\n",
      "\n",
      "\n",
      "--------- PARAMETER 1 GRADIENT ---------\n",
      "\n",
      "\n",
      "tensor([1.4060e-07, 1.2718e-07, 1.3352e-07, 1.3168e-07, 1.4006e-07])\n",
      "\n",
      "\n",
      "--------- PARAMETER 2 GRADIENT ---------\n",
      "\n",
      "\n",
      "tensor([[-0.0253,  0.0593, -0.0144, -0.0412, -0.0458],\n",
      "        [-0.0153,  0.0196, -0.0109, -0.0280,  0.0012],\n",
      "        [ 0.0276, -0.0387,  0.0161,  0.0259,  0.0284],\n",
      "        [-0.0047,  0.0036, -0.0118,  0.0097, -0.0242],\n",
      "        [ 0.0177, -0.0439,  0.0211,  0.0336,  0.0405]])\n",
      "\n",
      "\n",
      "--------- PARAMETER 3 GRADIENT ---------\n",
      "\n",
      "\n",
      "tensor([-0.1241, -0.0316,  0.0708, -0.0371,  0.1220])\n"
     ]
    }
   ],
   "source": [
    "# your model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a moment to see the neural network as a whole:\n",
    "\n",
    "\n",
    "![](./images/nn.png)\n",
    "\n",
    "- __Depth:__ - how many layers are in a neural network\n",
    "- __Width:__ - `out_features` in PyTorch, how many neurons are in a certain layer\n",
    "\n",
    "> In general, we create a bottleneck with `nn.Linear` layers, starting with `N` features and finishing with `M` outputs\n",
    "\n",
    "> This is a rule of thumb, not a hard knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU\n",
    "\n",
    "`ReLU` was designed to combat oversaturation and dying gradient problem.\n",
    "\n",
    "> `ReLU` is given by `max(0, x)` equation\n",
    "\n",
    "> In the linear part of activation, gradient will always be `1` or `0` (for negative and zero values)\n",
    "\n",
    "> Combination of piece-wise linear function can approximate any non-linearity (especially with increasing depth)\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- Reportedly much faster training times (initially `6x` improvements)\n",
    "- Faster implementation (thresholding values on zero)\n",
    "- No oversaturation\n",
    "\n",
    "### Drawbacks\n",
    "\n",
    "- Dying ReLU - large updates to neural networks may \"knock off\" a neuron into negative regime in which it will stay forever (in many neural networks there are \"dead\" neurons, around 50% sometimes)\n",
    "- Too high learning rate may be a cause\n",
    "- In practice it seems not to be too much of a problem (if the network is large it can compensate with other neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "- Create `NegativeInitializer` (similar to the previous exercise)\n",
    "- It should initialize submodule (specified by `target_layer`) with negative values (any negative values)\n",
    "- Check initialization for a few layers, see what happens with gradient. Why some gradient is non-zero anyway, what should we change?\n",
    "- If you know, change it in the `NegativeInitializer` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------- PARAMETER 0 GRADIENT ---------\n",
      "\n",
      "\n",
      "tensor([[ 9.4033e-04,  3.4394e-03,  7.0130e-04,  3.0983e-03, -1.3231e-04,\n",
      "          1.7180e-03,  3.7560e-03,  7.5737e-04,  1.8372e-03,  2.4804e-03],\n",
      "        [ 8.5365e-03,  3.1757e-03,  4.6065e-03,  1.3585e-02,  7.0140e-03,\n",
      "          6.2525e-03, -3.1818e-04,  4.8624e-03,  8.1392e-03,  1.3574e-02],\n",
      "        [ 8.4892e-06,  1.4374e-05,  3.4557e-06,  5.8296e-05,  7.2065e-05,\n",
      "          1.8505e-05,  6.0031e-05,  2.1434e-05,  1.5608e-06,  7.9357e-06],\n",
      "        [-7.3002e-04, -1.4709e-03,  1.3133e-03, -2.6724e-03, -1.4767e-03,\n",
      "         -5.8764e-04, -3.8302e-03, -8.0369e-04,  2.1004e-04, -6.0261e-04],\n",
      "        [-2.1563e-03, -6.2478e-04,  1.6013e-04, -3.1905e-03, -3.7933e-04,\n",
      "         -3.9026e-04,  1.3713e-03,  6.8202e-04, -2.1437e-03, -3.3584e-03]])\n",
      "\n",
      "\n",
      "--------- PARAMETER 1 GRADIENT ---------\n",
      "\n",
      "\n",
      "tensor([ 3.4787e-03,  1.2699e-02,  7.3402e-05, -2.8507e-03, -7.2081e-04])\n",
      "\n",
      "\n",
      "--------- PARAMETER 2 GRADIENT ---------\n",
      "\n",
      "\n",
      "tensor([[ 1.8009e-04, -2.0210e-03,  7.7468e-05,  1.3564e-03, -1.1143e-03],\n",
      "        [ 1.8009e-04, -4.1811e-04,  7.7468e-05,  2.0199e-03, -1.4661e-04],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 1.8009e-04, -4.1811e-04,  7.7468e-05,  2.0199e-03, -1.4661e-04],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
      "\n",
      "\n",
      "--------- PARAMETER 3 GRADIENT ---------\n",
      "\n",
      "\n",
      "tensor([-0.0186, -0.0124,  0.0000, -0.0124,  0.0000])\n",
      "\n",
      "\n",
      "--------- PARAMETER 4 GRADIENT ---------\n",
      "\n",
      "\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0015, 0.0017, 0.0000, 0.0021, 0.0000]])\n",
      "\n",
      "\n",
      "--------- PARAMETER 5 GRADIENT ---------\n",
      "\n",
      "\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0124])\n",
      "\n",
      "\n",
      "--------- PARAMETER 6 GRADIENT ---------\n",
      "\n",
      "\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0008],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0033],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0028],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0021],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0033]])\n",
      "\n",
      "\n",
      "--------- PARAMETER 7 GRADIENT ---------\n",
      "\n",
      "\n",
      "tensor([ 0.0075,  0.0491, -0.0876,  0.0576, -0.0267])\n"
     ]
    }
   ],
   "source": [
    "class NegativeInitializer:\n",
    "    ...\n",
    "\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(10, 5),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(5, 5),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(5, 5),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(5, 5),\n",
    ")\n",
    "\n",
    "# apply with negative initializer here\n",
    "gradient_print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky ReLU\n",
    "\n",
    "> Leaky ReLU solves dying gradient problem using __a small negative slope__ for negative values\n",
    "\n",
    "$$\n",
    "\\text{LeakyReLU}_s(x) = max(0, x) + s \\times min(0, x)\n",
    "$$\n",
    "\n",
    "> `s` is usually around `0.01`\n",
    "\n",
    "## Disadvantages\n",
    "\n",
    "- Higher computational cost\n",
    "- Solves a problem which is not a problem in many cases\n",
    "\n",
    "## Advantages\n",
    "\n",
    "- Neurons can recover from \"dead\" state and be useful for neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can neural networks do?\n",
    "\n",
    "The motivation that led us to deriving neural networks was that we wanted to model more complex functions. But what functions can a neural network actually represent?\n",
    "\n",
    "> Neural Networks can represent any continuous function and are **general function approximators**.\n",
    "\n",
    "![](./images/univ-approx.png)\n",
    "\n",
    "## Quick look at backpropagation\n",
    "\n",
    "When `backward` is called, gradient is calculated on a per-layer basis and passed to the previous ones.\n",
    "\n",
    "![](./images/backprop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Linear layers work like multiclass logistic regression with `in_features` and `out_features`.\n",
    "- Perceptron has __no hidden layers__.\n",
    "- __Multilayer Perceptron (MLP)__ is standard neural network with multiple layers.\n",
    "- We need to use multiple layers interspreded with activations in order to achieve non-linear behaviour\n",
    "- Main activation function is currently `ReLU` or `LeakyReLU`\n",
    "- Main problem with `sigmoid` and `tanh` is saturation and dying gradient (though there are used in some neural network blocks like recurrent)\n",
    "- `ReLU` may suffer from dying neurons phenomena which may impact neural network\n",
    "- Though it is not the most probable cause of poor network performance\n",
    "- Remember to use wide layers (say `50`, `100` neurons), depending on task (if the model does not learn, it might need more parameters)\n",
    "- Sufficiently wide and/or deep neural networks can approximate any function.\n",
    "\n",
    "## Challenges\n",
    "\n",
    "- Play around with [Tensorflow Neural Network playground](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.97988&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "- Run a few experiments with LeakyReLU (dying gradients, dying neurons)\n",
    "- What is neural network prunning?\n",
    "- How Maxout activation function works? What are the upsides and downsides of using it? \n",
    "- How SeLU activation function works? What are the upsides and downsides of using it?\n",
    "- Can you somehow show the impact of non-zero centered activation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
