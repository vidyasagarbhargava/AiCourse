{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Dropout.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "10d7c41029bd48a2ab3c04cf02ed3f7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_208fde76bd9c45ae841be2ee94eb831a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_328bb940fc414d129b4a743001eafcec",
              "IPY_MODEL_c0d9d47645f24d6bbd638bc4b75d6c28"
            ]
          }
        },
        "208fde76bd9c45ae841be2ee94eb831a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "328bb940fc414d129b4a743001eafcec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_54d64f3df44843c1b466983c0d631dcf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6351a1d12d7d4001a17bd9caf3fee2d7"
          }
        },
        "c0d9d47645f24d6bbd638bc4b75d6c28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6f41e5e3117e4a9eb1ccaf8c475ff667",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9920512/? [00:20&lt;00:00, 1063339.51it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_03fc3575dae3489b9b699e93fab4bb71"
          }
        },
        "54d64f3df44843c1b466983c0d631dcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6351a1d12d7d4001a17bd9caf3fee2d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6f41e5e3117e4a9eb1ccaf8c475ff667": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "03fc3575dae3489b9b699e93fab4bb71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "839548b6d38b43b58b47cda64315ab62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b0f6d71f40434d8f8ec2a78842c2a6c9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7a5ee060878b423883a8f10d18cd1400",
              "IPY_MODEL_595b454727624949aac688faf6227943"
            ]
          }
        },
        "b0f6d71f40434d8f8ec2a78842c2a6c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7a5ee060878b423883a8f10d18cd1400": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f4cb22cb8544480582f8df6a57441efa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2d527bd0c1854038a3fcd463be97a01a"
          }
        },
        "595b454727624949aac688faf6227943": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b4b7d2cd18e64506990e8ffb33d62779",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 32768/? [00:01&lt;00:00, 23821.78it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_93def7e72aa544a7b1f71f81d6780f5d"
          }
        },
        "f4cb22cb8544480582f8df6a57441efa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2d527bd0c1854038a3fcd463be97a01a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b4b7d2cd18e64506990e8ffb33d62779": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "93def7e72aa544a7b1f71f81d6780f5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8f089135e7984d2d8a4aa74b1a513476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2f7397dbf26b4598abe22602f5f80b26",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d8f566b9dd224d80a6d72d117d34218c",
              "IPY_MODEL_721d1ed588434ad38c719c5efd0a0f4a"
            ]
          }
        },
        "2f7397dbf26b4598abe22602f5f80b26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d8f566b9dd224d80a6d72d117d34218c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e23dc7227ef44e6584565024a4f14f0a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_12b345ad0f3f4a6d927c72e3e15d861c"
          }
        },
        "721d1ed588434ad38c719c5efd0a0f4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6b6b396c9b6a4d3aa621ff74640df50e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1654784/? [00:01&lt;00:00, 1504111.24it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d99ac37e2e02450e801118b466f613c3"
          }
        },
        "e23dc7227ef44e6584565024a4f14f0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "12b345ad0f3f4a6d927c72e3e15d861c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6b6b396c9b6a4d3aa621ff74640df50e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d99ac37e2e02450e801118b466f613c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e7fb52e8d3934ca295d066c08f0638b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_de9b9ec300684278947d3fc0c6becf15",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_09dba1e7c20c42ce82dd13984dc59d74",
              "IPY_MODEL_0b2a140d63d94ff88cba966c4343cce3"
            ]
          }
        },
        "de9b9ec300684278947d3fc0c6becf15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "09dba1e7c20c42ce82dd13984dc59d74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c3e4b55658d3428b97f865d1d02ce83d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dc0dcc46676d4179af62a6b047c95a8d"
          }
        },
        "0b2a140d63d94ff88cba966c4343cce3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c364d80535c84497b684845320bb26cc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 8192/? [00:00&lt;00:00, 21274.02it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cc82918197fd433fa025372b24aa070c"
          }
        },
        "c3e4b55658d3428b97f865d1d02ce83d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dc0dcc46676d4179af62a6b047c95a8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c364d80535c84497b684845320bb26cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cc82918197fd433fa025372b24aa070c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKyXAcU1By3l",
        "colab_type": "text"
      },
      "source": [
        "# Dropout Notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibIqmDynBy3m",
        "colab_type": "text"
      },
      "source": [
        "#### Prerequisite\n",
        "* Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffx3HatiBy3n",
        "colab_type": "text"
      },
      "source": [
        "***What will we cover in this notebook?***\n",
        "\n",
        "* Explain dropout in nn\n",
        "* Implementation in PyTorch\n",
        "-----\n",
        "1. What is dropout and why do we need it? What was the idea behind it?\n",
        "2. How is dropout similar to ensembling a large variety of neural network architectures?\n",
        "3. Implementation in PyTorch on a basic neural network\n",
        "4. Visualising the difference between the non-dropout and dropout implemented models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nijNnGn9By3o",
        "colab_type": "text"
      },
      "source": [
        "### 1. What was the idea behind Dropout?\n",
        "   * explain why we need dropout? Which problem leads us to do dropout\n",
        "   * show a dropout example on a basic neural network\n",
        "   * explain dropout parameters\n",
        "   * important aspects of dropout\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI90VAZcBy3p",
        "colab_type": "text"
      },
      "source": [
        "### 2. Similarity between dropout and ensembles methods\n",
        "   * explain what the ensemble method is\n",
        "   * list the similary between them\n",
        "   * what makes dropout unique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHAOhDYsBy3p",
        "colab_type": "text"
      },
      "source": [
        "![ensemble.png](attachment:ensemble.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66sQs4SKBy3q",
        "colab_type": "text"
      },
      "source": [
        "### 3. Implementation in PyTorch on a basic neural network \n",
        "* Copy the code in Neural Network Repo\n",
        "* Implement dropout method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wsr9zfC-By3r",
        "colab_type": "text"
      },
      "source": [
        "### 4. Visualizing the difference between a non-dropout and dropout implemented nn\n",
        "* Copy the nn repo model performance metrics etc\n",
        "* Explain the metrics used\n",
        "* Evaluate the dropout model\n",
        "* Compare training curves\n",
        "* Compare model performance metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_Ty3EwfBy3s",
        "colab_type": "text"
      },
      "source": [
        "### **Non-dropout implementation can be found below:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xRIWITEBy3s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680,
          "referenced_widgets": [
            "10d7c41029bd48a2ab3c04cf02ed3f7b",
            "208fde76bd9c45ae841be2ee94eb831a",
            "328bb940fc414d129b4a743001eafcec",
            "c0d9d47645f24d6bbd638bc4b75d6c28",
            "54d64f3df44843c1b466983c0d631dcf",
            "6351a1d12d7d4001a17bd9caf3fee2d7",
            "6f41e5e3117e4a9eb1ccaf8c475ff667",
            "03fc3575dae3489b9b699e93fab4bb71",
            "839548b6d38b43b58b47cda64315ab62",
            "b0f6d71f40434d8f8ec2a78842c2a6c9",
            "7a5ee060878b423883a8f10d18cd1400",
            "595b454727624949aac688faf6227943",
            "f4cb22cb8544480582f8df6a57441efa",
            "2d527bd0c1854038a3fcd463be97a01a",
            "b4b7d2cd18e64506990e8ffb33d62779",
            "93def7e72aa544a7b1f71f81d6780f5d",
            "8f089135e7984d2d8a4aa74b1a513476",
            "2f7397dbf26b4598abe22602f5f80b26",
            "d8f566b9dd224d80a6d72d117d34218c",
            "721d1ed588434ad38c719c5efd0a0f4a",
            "e23dc7227ef44e6584565024a4f14f0a",
            "12b345ad0f3f4a6d927c72e3e15d861c",
            "6b6b396c9b6a4d3aa621ff74640df50e",
            "d99ac37e2e02450e801118b466f613c3",
            "e7fb52e8d3934ca295d066c08f0638b1",
            "de9b9ec300684278947d3fc0c6becf15",
            "09dba1e7c20c42ce82dd13984dc59d74",
            "0b2a140d63d94ff88cba966c4343cce3",
            "c3e4b55658d3428b97f865d1d02ce83d",
            "dc0dcc46676d4179af62a6b047c95a8d",
            "c364d80535c84497b684845320bb26cc",
            "cc82918197fd433fa025372b24aa070c"
          ]
        },
        "outputId": "2be401eb-5f2d-4bc0-eb84-2b3d9eb9b282"
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# GET THE TRAINING DATASET\n",
        "train_data = datasets.MNIST(root='MNIST-data',                        # where is the data (going to be) stored\n",
        "                            transform=transforms.ToTensor(),          # transform the data from a PIL image to a tensor\n",
        "                            train=True,                               # is this training data?\n",
        "                            download=True                             # should i download it if it's not already here?\n",
        "                           )\n",
        "\n",
        "# GET THE TEST DATASET\n",
        "test_data = datasets.MNIST(root='MNIST-data',\n",
        "                           transform=transforms.ToTensor(),\n",
        "                           train=False,\n",
        "                          )\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# PRINT THEIR LENGTHS AND VISUALISE AN EXAMPLE\n",
        "x = train_data[np.random.randint(0, 300)][0]    # get a random example image\n",
        "plt.imshow(x[0].numpy(),cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# FURTHER SPLIT THE TRAINING INTO TRAINING AND VALIDATION\n",
        "train_data, val_data = torch.utils.data.random_split(train_data, [50000, 10000])    # split into 50K training & 10K validation\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST-data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10d7c41029bd48a2ab3c04cf02ed3f7b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST-data/MNIST/raw/train-images-idx3-ubyte.gz to MNIST-data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST-data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "839548b6d38b43b58b47cda64315ab62",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST-data/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST-data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST-data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f089135e7984d2d8a4aa74b1a513476",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST-data/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST-data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST-data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7fb52e8d3934ca295d066c08f0638b1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST-data/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST-data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K493e-gfBy3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 256\n",
        "\n",
        "# MAKE TRAINING DATALOADER\n",
        "train_loader = torch.utils.data.DataLoader( # create a data loader\n",
        "    train_data, # what dataset should it sample from?\n",
        "    shuffle=True, # should it shuffle the examples?\n",
        "    batch_size=batch_size # how large should the batches that it samples be?\n",
        ")\n",
        "\n",
        "# MAKE VALIDATION DATALOADER\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_data,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# MAKE TEST DATALOADER\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_data,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ApcwYNDBy31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F \n",
        "class NN(torch.nn.Module): # create a neural network class\n",
        "    def __init__(self): # initialiser\n",
        "        super().__init__() # initialise the parent class\n",
        "        self.layer1 = torch.nn.Linear(784, 1024) # create our first linear layer\n",
        "        self.layer2 = torch.nn.Linear(1024, 256) # create our second linear layer\n",
        "        self.layer3 = torch.nn.Linear(256, 10) # create our third linear layer\n",
        "        \n",
        "    def forward(self, x): # define the forward pass\n",
        "        x = x.view(-1, 784) # flatten out our image features into vectors\n",
        "        x = self.layer1(x) # pass through the first linear layer\n",
        "        x = F.relu(x) # apply activation function\n",
        "        x = self.layer2(x) # pass through the second linear layer\n",
        "        x = F.relu(x) # apply activation function\n",
        "        x = self.layer3(x) # pass through the third linear layer\n",
        "        x = F.softmax(x) # apply activation function\n",
        "        return x # return output"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wiTQuX7By36",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dad56a15-b708-493d-8448-19169293026b"
      },
      "source": [
        "learning_rate = 0.001\n",
        "my_nn = NN()\n",
        "\n",
        "# CREATE OUR OPTIMISER\n",
        "optimiser = torch.optim.Adam(              # what optimiser should we use?\n",
        "    my_nn.parameters(),                    # what should it optimise?\n",
        "    lr=learning_rate                       # using what learning rate?\n",
        ")\n",
        "\n",
        "# CREATE OUR CRITERION\n",
        "criterion = torch.nn.CrossEntropyLoss()             # callable class that compares our predictions to our labels and returns our loss\n",
        "\n",
        "# SET UP TRAINING VISUALISATION\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()                            # we will use this to show our models performance on a graph\n",
        "    \n",
        "# TRAINING LOOP\n",
        "def train(model, epochs):\n",
        "    model.train()                                  # put the model into training mode (more on this later)\n",
        "    for epoch in range(epochs):\n",
        "        for idx, minibatch in enumerate(train_loader):\n",
        "            inputs, labels = minibatch\n",
        "            prediction = model(inputs)             # pass the data forward through the model\n",
        "            loss = criterion(prediction, labels)   # compute the loss\n",
        "            print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss)\n",
        "            optimiser.zero_grad()                  # reset the gradients attribute of each of the model's params to zero\n",
        "            loss.backward()                        # backward pass to compute and set all of the model param's gradients\n",
        "            optimiser.step()                       # update the model's parameters\n",
        "            writer.add_scalar('Loss/Train', loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
        "\n",
        "train(my_nn, 8)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 \tBatch: 0 \tLoss: tensor(2.3029, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 1 \tLoss: tensor(2.2923, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 2 \tLoss: tensor(2.2794, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 3 \tLoss: tensor(2.2517, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 4 \tLoss: tensor(2.2246, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 5 \tLoss: tensor(2.1754, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 6 \tLoss: tensor(2.1397, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 7 \tLoss: tensor(2.0849, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 8 \tLoss: tensor(1.9830, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 9 \tLoss: tensor(1.9610, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 10 \tLoss: tensor(1.9317, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 11 \tLoss: tensor(1.8852, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 12 \tLoss: tensor(1.9137, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 13 \tLoss: tensor(1.8429, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 14 \tLoss: tensor(1.8047, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 15 \tLoss: tensor(1.8649, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 16 \tLoss: tensor(1.8443, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 17 \tLoss: tensor(1.7906, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 18 \tLoss: tensor(1.8136, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 19 \tLoss: tensor(1.7875, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 20 \tLoss: tensor(1.7570, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 21 \tLoss: tensor(1.7497, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 22 \tLoss: tensor(1.6858, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 23 \tLoss: tensor(1.7030, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 24 \tLoss: tensor(1.7096, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 25 \tLoss: tensor(1.7192, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 26 \tLoss: tensor(1.6999, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 27 \tLoss: tensor(1.7154, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 28 \tLoss: tensor(1.6441, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 29 \tLoss: tensor(1.7107, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 30 \tLoss: tensor(1.6894, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 31 \tLoss: tensor(1.6770, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 32 \tLoss: tensor(1.7024, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 33 \tLoss: tensor(1.6757, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 34 \tLoss: tensor(1.6538, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 35 \tLoss: tensor(1.6401, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 36 \tLoss: tensor(1.6396, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 37 \tLoss: tensor(1.6668, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 38 \tLoss: tensor(1.6772, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 39 \tLoss: tensor(1.6944, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 40 \tLoss: tensor(1.6561, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 41 \tLoss: tensor(1.6904, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 42 \tLoss: tensor(1.6316, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 43 \tLoss: tensor(1.6779, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 44 \tLoss: tensor(1.6618, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 45 \tLoss: tensor(1.6477, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 46 \tLoss: tensor(1.6160, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 47 \tLoss: tensor(1.6388, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 48 \tLoss: tensor(1.6525, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 49 \tLoss: tensor(1.6683, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 50 \tLoss: tensor(1.6644, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 51 \tLoss: tensor(1.6231, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 52 \tLoss: tensor(1.6583, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 53 \tLoss: tensor(1.6099, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 54 \tLoss: tensor(1.6386, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 55 \tLoss: tensor(1.6171, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 56 \tLoss: tensor(1.6612, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 57 \tLoss: tensor(1.6355, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 58 \tLoss: tensor(1.6869, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 59 \tLoss: tensor(1.6199, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 60 \tLoss: tensor(1.6440, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 61 \tLoss: tensor(1.6051, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 62 \tLoss: tensor(1.6428, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 63 \tLoss: tensor(1.6491, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 64 \tLoss: tensor(1.6141, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 65 \tLoss: tensor(1.6435, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 66 \tLoss: tensor(1.6371, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 67 \tLoss: tensor(1.6682, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 68 \tLoss: tensor(1.6078, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 69 \tLoss: tensor(1.6364, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 70 \tLoss: tensor(1.6563, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 71 \tLoss: tensor(1.6126, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 72 \tLoss: tensor(1.6067, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 73 \tLoss: tensor(1.6482, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 74 \tLoss: tensor(1.6127, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 75 \tLoss: tensor(1.6100, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 76 \tLoss: tensor(1.5813, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 77 \tLoss: tensor(1.5997, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 78 \tLoss: tensor(1.6269, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 79 \tLoss: tensor(1.6025, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 80 \tLoss: tensor(1.6083, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 81 \tLoss: tensor(1.5767, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 82 \tLoss: tensor(1.5983, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 83 \tLoss: tensor(1.5586, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 84 \tLoss: tensor(1.5780, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 85 \tLoss: tensor(1.5507, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 86 \tLoss: tensor(1.5609, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 87 \tLoss: tensor(1.6142, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 88 \tLoss: tensor(1.5986, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 89 \tLoss: tensor(1.6048, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 90 \tLoss: tensor(1.5885, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 91 \tLoss: tensor(1.5803, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 92 \tLoss: tensor(1.6000, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 93 \tLoss: tensor(1.5512, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 94 \tLoss: tensor(1.5524, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 95 \tLoss: tensor(1.5331, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 96 \tLoss: tensor(1.5426, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 97 \tLoss: tensor(1.5801, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 98 \tLoss: tensor(1.5663, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 99 \tLoss: tensor(1.5929, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 100 \tLoss: tensor(1.5416, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 101 \tLoss: tensor(1.5753, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 102 \tLoss: tensor(1.5509, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 103 \tLoss: tensor(1.5401, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 104 \tLoss: tensor(1.5587, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 105 \tLoss: tensor(1.5555, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 106 \tLoss: tensor(1.5736, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 107 \tLoss: tensor(1.5484, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 108 \tLoss: tensor(1.5419, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 109 \tLoss: tensor(1.5802, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 110 \tLoss: tensor(1.5469, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 111 \tLoss: tensor(1.5501, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 112 \tLoss: tensor(1.5674, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 113 \tLoss: tensor(1.5631, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 114 \tLoss: tensor(1.5744, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 115 \tLoss: tensor(1.5212, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 116 \tLoss: tensor(1.5601, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 117 \tLoss: tensor(1.5433, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 118 \tLoss: tensor(1.5598, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 119 \tLoss: tensor(1.5479, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 120 \tLoss: tensor(1.5624, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 121 \tLoss: tensor(1.5467, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 122 \tLoss: tensor(1.5716, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 123 \tLoss: tensor(1.5767, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 124 \tLoss: tensor(1.5437, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 125 \tLoss: tensor(1.5268, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 126 \tLoss: tensor(1.5483, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 127 \tLoss: tensor(1.5793, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 128 \tLoss: tensor(1.5655, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 129 \tLoss: tensor(1.5518, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 130 \tLoss: tensor(1.5662, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 131 \tLoss: tensor(1.5642, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 132 \tLoss: tensor(1.5988, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 133 \tLoss: tensor(1.5304, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 134 \tLoss: tensor(1.5526, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 135 \tLoss: tensor(1.5588, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 136 \tLoss: tensor(1.5681, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 137 \tLoss: tensor(1.5289, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 138 \tLoss: tensor(1.5413, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 139 \tLoss: tensor(1.5402, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 140 \tLoss: tensor(1.5398, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 141 \tLoss: tensor(1.5543, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 142 \tLoss: tensor(1.5495, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 143 \tLoss: tensor(1.5354, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 144 \tLoss: tensor(1.5545, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 145 \tLoss: tensor(1.5595, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 146 \tLoss: tensor(1.5450, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 147 \tLoss: tensor(1.5490, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 148 \tLoss: tensor(1.5388, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 149 \tLoss: tensor(1.5292, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 150 \tLoss: tensor(1.5587, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 151 \tLoss: tensor(1.5584, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 152 \tLoss: tensor(1.5415, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 153 \tLoss: tensor(1.5276, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 154 \tLoss: tensor(1.5466, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 155 \tLoss: tensor(1.5345, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 156 \tLoss: tensor(1.5291, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 157 \tLoss: tensor(1.5787, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 158 \tLoss: tensor(1.5290, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 159 \tLoss: tensor(1.5418, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 160 \tLoss: tensor(1.5544, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 161 \tLoss: tensor(1.5415, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 162 \tLoss: tensor(1.5328, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 163 \tLoss: tensor(1.5624, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 164 \tLoss: tensor(1.5671, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 165 \tLoss: tensor(1.5353, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 166 \tLoss: tensor(1.5206, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 167 \tLoss: tensor(1.5291, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 168 \tLoss: tensor(1.5048, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 169 \tLoss: tensor(1.5329, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 170 \tLoss: tensor(1.5470, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 171 \tLoss: tensor(1.5352, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 172 \tLoss: tensor(1.5274, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 173 \tLoss: tensor(1.5380, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 174 \tLoss: tensor(1.5860, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 175 \tLoss: tensor(1.5279, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 176 \tLoss: tensor(1.5448, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 177 \tLoss: tensor(1.5269, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 178 \tLoss: tensor(1.5461, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 179 \tLoss: tensor(1.5596, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 180 \tLoss: tensor(1.5516, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 181 \tLoss: tensor(1.5233, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 182 \tLoss: tensor(1.5489, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 183 \tLoss: tensor(1.5550, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 184 \tLoss: tensor(1.5459, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 185 \tLoss: tensor(1.5336, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 186 \tLoss: tensor(1.5196, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 187 \tLoss: tensor(1.5381, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 188 \tLoss: tensor(1.5419, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 189 \tLoss: tensor(1.5006, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 190 \tLoss: tensor(1.5253, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 191 \tLoss: tensor(1.5402, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 192 \tLoss: tensor(1.5372, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 193 \tLoss: tensor(1.5250, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 194 \tLoss: tensor(1.5483, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 195 \tLoss: tensor(1.5834, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 0 \tLoss: tensor(1.5219, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 1 \tLoss: tensor(1.5089, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 2 \tLoss: tensor(1.5484, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 3 \tLoss: tensor(1.5226, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 4 \tLoss: tensor(1.5215, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 5 \tLoss: tensor(1.5589, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 6 \tLoss: tensor(1.5228, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 7 \tLoss: tensor(1.5568, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 8 \tLoss: tensor(1.5212, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 9 \tLoss: tensor(1.5404, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 10 \tLoss: tensor(1.5411, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 11 \tLoss: tensor(1.5330, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 12 \tLoss: tensor(1.5464, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 13 \tLoss: tensor(1.5233, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 14 \tLoss: tensor(1.5133, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 15 \tLoss: tensor(1.5141, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 16 \tLoss: tensor(1.5316, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 17 \tLoss: tensor(1.5312, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 18 \tLoss: tensor(1.5263, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 19 \tLoss: tensor(1.5283, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 20 \tLoss: tensor(1.5704, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 21 \tLoss: tensor(1.5170, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 22 \tLoss: tensor(1.5332, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 23 \tLoss: tensor(1.5397, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 24 \tLoss: tensor(1.5254, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 25 \tLoss: tensor(1.5341, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 26 \tLoss: tensor(1.5361, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 27 \tLoss: tensor(1.4985, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 28 \tLoss: tensor(1.5349, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 29 \tLoss: tensor(1.5194, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 30 \tLoss: tensor(1.5203, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 31 \tLoss: tensor(1.5477, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 32 \tLoss: tensor(1.5061, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 33 \tLoss: tensor(1.5165, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 34 \tLoss: tensor(1.5544, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 35 \tLoss: tensor(1.5347, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 36 \tLoss: tensor(1.5271, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 37 \tLoss: tensor(1.5109, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 38 \tLoss: tensor(1.5008, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 39 \tLoss: tensor(1.5341, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 40 \tLoss: tensor(1.5381, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 41 \tLoss: tensor(1.5310, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 42 \tLoss: tensor(1.5406, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 43 \tLoss: tensor(1.5341, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 44 \tLoss: tensor(1.5162, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 45 \tLoss: tensor(1.5149, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 46 \tLoss: tensor(1.5255, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 47 \tLoss: tensor(1.5238, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 48 \tLoss: tensor(1.5097, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 49 \tLoss: tensor(1.5326, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 50 \tLoss: tensor(1.5365, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 51 \tLoss: tensor(1.5004, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 52 \tLoss: tensor(1.5321, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 53 \tLoss: tensor(1.5440, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 54 \tLoss: tensor(1.5262, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 55 \tLoss: tensor(1.5163, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 56 \tLoss: tensor(1.5097, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 57 \tLoss: tensor(1.5514, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 58 \tLoss: tensor(1.5445, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 59 \tLoss: tensor(1.4952, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 60 \tLoss: tensor(1.5298, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 61 \tLoss: tensor(1.5454, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 62 \tLoss: tensor(1.5029, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 63 \tLoss: tensor(1.5231, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 64 \tLoss: tensor(1.5151, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 65 \tLoss: tensor(1.5366, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 66 \tLoss: tensor(1.5152, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 67 \tLoss: tensor(1.5236, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 68 \tLoss: tensor(1.5189, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 69 \tLoss: tensor(1.5366, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 70 \tLoss: tensor(1.5096, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 71 \tLoss: tensor(1.5225, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 72 \tLoss: tensor(1.5097, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 73 \tLoss: tensor(1.5325, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 74 \tLoss: tensor(1.5409, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 75 \tLoss: tensor(1.5222, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 76 \tLoss: tensor(1.5375, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 77 \tLoss: tensor(1.5463, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 78 \tLoss: tensor(1.5518, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 79 \tLoss: tensor(1.5158, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 80 \tLoss: tensor(1.5439, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 81 \tLoss: tensor(1.5134, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 82 \tLoss: tensor(1.5187, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 83 \tLoss: tensor(1.5299, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 84 \tLoss: tensor(1.5339, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 85 \tLoss: tensor(1.5221, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 86 \tLoss: tensor(1.5376, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 87 \tLoss: tensor(1.5251, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 88 \tLoss: tensor(1.5313, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 89 \tLoss: tensor(1.5129, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 90 \tLoss: tensor(1.5320, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 91 \tLoss: tensor(1.5227, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 92 \tLoss: tensor(1.5384, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 93 \tLoss: tensor(1.5237, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 94 \tLoss: tensor(1.5160, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 95 \tLoss: tensor(1.5287, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 96 \tLoss: tensor(1.5139, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 97 \tLoss: tensor(1.5493, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 98 \tLoss: tensor(1.5324, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 99 \tLoss: tensor(1.5126, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 100 \tLoss: tensor(1.5239, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 101 \tLoss: tensor(1.5334, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 102 \tLoss: tensor(1.4945, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 103 \tLoss: tensor(1.4952, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 104 \tLoss: tensor(1.5218, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 105 \tLoss: tensor(1.5238, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 106 \tLoss: tensor(1.5534, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 107 \tLoss: tensor(1.5007, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 108 \tLoss: tensor(1.5539, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 109 \tLoss: tensor(1.5267, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 110 \tLoss: tensor(1.5156, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 111 \tLoss: tensor(1.5203, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 112 \tLoss: tensor(1.5169, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 113 \tLoss: tensor(1.5410, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 114 \tLoss: tensor(1.5206, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 115 \tLoss: tensor(1.5077, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 116 \tLoss: tensor(1.5079, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 117 \tLoss: tensor(1.5244, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 118 \tLoss: tensor(1.5327, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 119 \tLoss: tensor(1.5308, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 120 \tLoss: tensor(1.5251, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 121 \tLoss: tensor(1.5269, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 122 \tLoss: tensor(1.5135, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 123 \tLoss: tensor(1.5084, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 124 \tLoss: tensor(1.5342, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 125 \tLoss: tensor(1.5339, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 126 \tLoss: tensor(1.5312, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 127 \tLoss: tensor(1.4953, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 128 \tLoss: tensor(1.5252, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 129 \tLoss: tensor(1.5269, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 130 \tLoss: tensor(1.5170, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 131 \tLoss: tensor(1.5224, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 132 \tLoss: tensor(1.5214, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 133 \tLoss: tensor(1.5479, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 134 \tLoss: tensor(1.5169, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 135 \tLoss: tensor(1.5044, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 136 \tLoss: tensor(1.5185, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 137 \tLoss: tensor(1.5149, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 138 \tLoss: tensor(1.5136, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 139 \tLoss: tensor(1.5244, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 140 \tLoss: tensor(1.5028, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 141 \tLoss: tensor(1.4962, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 142 \tLoss: tensor(1.5231, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 143 \tLoss: tensor(1.5066, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 144 \tLoss: tensor(1.5186, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 145 \tLoss: tensor(1.5189, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 146 \tLoss: tensor(1.5005, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 147 \tLoss: tensor(1.5338, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 148 \tLoss: tensor(1.5128, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 149 \tLoss: tensor(1.5216, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 150 \tLoss: tensor(1.5316, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 151 \tLoss: tensor(1.5221, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 152 \tLoss: tensor(1.5290, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 153 \tLoss: tensor(1.5179, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 154 \tLoss: tensor(1.5055, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 155 \tLoss: tensor(1.5089, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 156 \tLoss: tensor(1.5230, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 157 \tLoss: tensor(1.5161, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 158 \tLoss: tensor(1.5047, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 159 \tLoss: tensor(1.5372, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 160 \tLoss: tensor(1.5282, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 161 \tLoss: tensor(1.5109, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 162 \tLoss: tensor(1.5145, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 163 \tLoss: tensor(1.5217, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 164 \tLoss: tensor(1.5204, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 165 \tLoss: tensor(1.5036, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 166 \tLoss: tensor(1.5185, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 167 \tLoss: tensor(1.5112, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 168 \tLoss: tensor(1.5033, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 169 \tLoss: tensor(1.5085, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 170 \tLoss: tensor(1.5317, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 171 \tLoss: tensor(1.5062, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 172 \tLoss: tensor(1.5135, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 173 \tLoss: tensor(1.5037, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 174 \tLoss: tensor(1.5184, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 175 \tLoss: tensor(1.5211, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 176 \tLoss: tensor(1.5106, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 177 \tLoss: tensor(1.5117, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 178 \tLoss: tensor(1.5304, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 179 \tLoss: tensor(1.5522, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 180 \tLoss: tensor(1.5042, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 181 \tLoss: tensor(1.4967, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 182 \tLoss: tensor(1.5034, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 183 \tLoss: tensor(1.5276, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 184 \tLoss: tensor(1.5048, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 185 \tLoss: tensor(1.4966, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 186 \tLoss: tensor(1.5443, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 187 \tLoss: tensor(1.5123, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 188 \tLoss: tensor(1.4962, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 189 \tLoss: tensor(1.4940, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 190 \tLoss: tensor(1.5137, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 191 \tLoss: tensor(1.5050, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 192 \tLoss: tensor(1.4972, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 193 \tLoss: tensor(1.5228, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 194 \tLoss: tensor(1.5104, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 195 \tLoss: tensor(1.4902, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 0 \tLoss: tensor(1.5115, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 1 \tLoss: tensor(1.5103, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 2 \tLoss: tensor(1.5206, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 3 \tLoss: tensor(1.5136, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 4 \tLoss: tensor(1.4985, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 5 \tLoss: tensor(1.4943, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 6 \tLoss: tensor(1.5233, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 7 \tLoss: tensor(1.5275, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 8 \tLoss: tensor(1.4886, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 9 \tLoss: tensor(1.4950, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 10 \tLoss: tensor(1.5002, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 11 \tLoss: tensor(1.5251, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 12 \tLoss: tensor(1.5141, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 13 \tLoss: tensor(1.5062, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 14 \tLoss: tensor(1.4904, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 15 \tLoss: tensor(1.5038, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 16 \tLoss: tensor(1.5163, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 17 \tLoss: tensor(1.5100, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 18 \tLoss: tensor(1.5026, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 19 \tLoss: tensor(1.5203, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 20 \tLoss: tensor(1.4990, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 21 \tLoss: tensor(1.5249, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 22 \tLoss: tensor(1.5217, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 23 \tLoss: tensor(1.5008, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 24 \tLoss: tensor(1.5199, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 25 \tLoss: tensor(1.5093, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 26 \tLoss: tensor(1.5216, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 27 \tLoss: tensor(1.5011, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 28 \tLoss: tensor(1.5213, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 29 \tLoss: tensor(1.5094, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 30 \tLoss: tensor(1.4969, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 31 \tLoss: tensor(1.4963, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 32 \tLoss: tensor(1.4976, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 33 \tLoss: tensor(1.5091, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 34 \tLoss: tensor(1.5062, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 35 \tLoss: tensor(1.5174, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 36 \tLoss: tensor(1.5112, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 37 \tLoss: tensor(1.4904, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 38 \tLoss: tensor(1.5104, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 39 \tLoss: tensor(1.4938, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 40 \tLoss: tensor(1.4982, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 41 \tLoss: tensor(1.5295, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 42 \tLoss: tensor(1.5120, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 43 \tLoss: tensor(1.5008, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 44 \tLoss: tensor(1.5226, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 45 \tLoss: tensor(1.5288, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 46 \tLoss: tensor(1.5047, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 47 \tLoss: tensor(1.5204, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 48 \tLoss: tensor(1.5026, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 49 \tLoss: tensor(1.4887, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 50 \tLoss: tensor(1.4865, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 51 \tLoss: tensor(1.5022, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 52 \tLoss: tensor(1.5269, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 53 \tLoss: tensor(1.5085, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 54 \tLoss: tensor(1.4838, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 55 \tLoss: tensor(1.5087, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 56 \tLoss: tensor(1.5308, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 57 \tLoss: tensor(1.5063, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 58 \tLoss: tensor(1.5069, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 59 \tLoss: tensor(1.4873, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 60 \tLoss: tensor(1.5116, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 61 \tLoss: tensor(1.4886, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 62 \tLoss: tensor(1.5029, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 63 \tLoss: tensor(1.5040, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 64 \tLoss: tensor(1.5023, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 65 \tLoss: tensor(1.4956, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 66 \tLoss: tensor(1.5149, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 67 \tLoss: tensor(1.5140, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 68 \tLoss: tensor(1.5143, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 69 \tLoss: tensor(1.4885, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 70 \tLoss: tensor(1.4929, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 71 \tLoss: tensor(1.5078, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 72 \tLoss: tensor(1.4863, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 73 \tLoss: tensor(1.5030, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 74 \tLoss: tensor(1.4949, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 75 \tLoss: tensor(1.5065, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 76 \tLoss: tensor(1.4989, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 77 \tLoss: tensor(1.5275, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 78 \tLoss: tensor(1.4989, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 79 \tLoss: tensor(1.5058, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 80 \tLoss: tensor(1.4968, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 81 \tLoss: tensor(1.5053, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 82 \tLoss: tensor(1.5102, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 83 \tLoss: tensor(1.5050, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 84 \tLoss: tensor(1.5037, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 85 \tLoss: tensor(1.5198, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 86 \tLoss: tensor(1.5007, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 87 \tLoss: tensor(1.5107, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 88 \tLoss: tensor(1.4713, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 89 \tLoss: tensor(1.4927, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 90 \tLoss: tensor(1.4728, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 91 \tLoss: tensor(1.5211, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 92 \tLoss: tensor(1.5163, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 93 \tLoss: tensor(1.5023, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 94 \tLoss: tensor(1.5049, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 95 \tLoss: tensor(1.4881, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 96 \tLoss: tensor(1.4990, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 97 \tLoss: tensor(1.4947, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 98 \tLoss: tensor(1.5064, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 99 \tLoss: tensor(1.4970, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 100 \tLoss: tensor(1.4971, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 101 \tLoss: tensor(1.5217, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 102 \tLoss: tensor(1.5053, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 103 \tLoss: tensor(1.5089, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 104 \tLoss: tensor(1.4982, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 105 \tLoss: tensor(1.5108, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 106 \tLoss: tensor(1.5083, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 107 \tLoss: tensor(1.5077, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 108 \tLoss: tensor(1.4955, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 109 \tLoss: tensor(1.5086, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 110 \tLoss: tensor(1.4933, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 111 \tLoss: tensor(1.5029, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 112 \tLoss: tensor(1.4900, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 113 \tLoss: tensor(1.5037, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 114 \tLoss: tensor(1.5006, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 115 \tLoss: tensor(1.5185, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 116 \tLoss: tensor(1.5122, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 117 \tLoss: tensor(1.5183, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 118 \tLoss: tensor(1.5023, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 119 \tLoss: tensor(1.5096, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 120 \tLoss: tensor(1.5030, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 121 \tLoss: tensor(1.5080, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 122 \tLoss: tensor(1.5022, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 123 \tLoss: tensor(1.5016, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 124 \tLoss: tensor(1.5025, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 125 \tLoss: tensor(1.5153, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 126 \tLoss: tensor(1.5147, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 127 \tLoss: tensor(1.4947, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 128 \tLoss: tensor(1.5084, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 129 \tLoss: tensor(1.4978, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 130 \tLoss: tensor(1.5206, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 131 \tLoss: tensor(1.5056, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 132 \tLoss: tensor(1.4999, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 133 \tLoss: tensor(1.5239, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 134 \tLoss: tensor(1.5112, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 135 \tLoss: tensor(1.5168, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 136 \tLoss: tensor(1.5306, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 137 \tLoss: tensor(1.4989, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 138 \tLoss: tensor(1.5010, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 139 \tLoss: tensor(1.4964, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 140 \tLoss: tensor(1.4841, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 141 \tLoss: tensor(1.5038, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 142 \tLoss: tensor(1.5116, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 143 \tLoss: tensor(1.4961, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 144 \tLoss: tensor(1.5019, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 145 \tLoss: tensor(1.5091, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 146 \tLoss: tensor(1.5227, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 147 \tLoss: tensor(1.5002, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 148 \tLoss: tensor(1.5220, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 149 \tLoss: tensor(1.4861, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 150 \tLoss: tensor(1.4869, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 151 \tLoss: tensor(1.4964, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 152 \tLoss: tensor(1.5036, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 153 \tLoss: tensor(1.5016, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 154 \tLoss: tensor(1.5221, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 155 \tLoss: tensor(1.4888, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 156 \tLoss: tensor(1.5123, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 157 \tLoss: tensor(1.5100, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 158 \tLoss: tensor(1.4971, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 159 \tLoss: tensor(1.5033, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 160 \tLoss: tensor(1.5252, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 161 \tLoss: tensor(1.4852, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 162 \tLoss: tensor(1.4897, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 163 \tLoss: tensor(1.4948, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 164 \tLoss: tensor(1.5109, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 165 \tLoss: tensor(1.5188, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 166 \tLoss: tensor(1.4942, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 167 \tLoss: tensor(1.4889, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 168 \tLoss: tensor(1.5021, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 169 \tLoss: tensor(1.4995, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 170 \tLoss: tensor(1.5097, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 171 \tLoss: tensor(1.5003, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 172 \tLoss: tensor(1.4865, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 173 \tLoss: tensor(1.4845, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 174 \tLoss: tensor(1.5158, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 175 \tLoss: tensor(1.5317, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 176 \tLoss: tensor(1.5146, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 177 \tLoss: tensor(1.4847, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 178 \tLoss: tensor(1.4957, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 179 \tLoss: tensor(1.4981, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 180 \tLoss: tensor(1.5147, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 181 \tLoss: tensor(1.5034, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 182 \tLoss: tensor(1.5122, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 183 \tLoss: tensor(1.5064, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 184 \tLoss: tensor(1.5092, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 185 \tLoss: tensor(1.5044, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 186 \tLoss: tensor(1.4987, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 187 \tLoss: tensor(1.4906, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 188 \tLoss: tensor(1.4936, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 189 \tLoss: tensor(1.4954, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 190 \tLoss: tensor(1.4989, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 191 \tLoss: tensor(1.4953, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 192 \tLoss: tensor(1.5023, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 193 \tLoss: tensor(1.5066, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 194 \tLoss: tensor(1.5073, grad_fn=<NllLossBackward>)\n",
            "Epoch: 2 \tBatch: 195 \tLoss: tensor(1.4645, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 0 \tLoss: tensor(1.5046, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 1 \tLoss: tensor(1.5004, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 2 \tLoss: tensor(1.4780, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 3 \tLoss: tensor(1.4915, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 4 \tLoss: tensor(1.4890, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 5 \tLoss: tensor(1.5058, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 6 \tLoss: tensor(1.4942, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 7 \tLoss: tensor(1.4908, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 8 \tLoss: tensor(1.4958, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 9 \tLoss: tensor(1.4903, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 10 \tLoss: tensor(1.5297, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 11 \tLoss: tensor(1.4882, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 12 \tLoss: tensor(1.4971, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 13 \tLoss: tensor(1.4994, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 14 \tLoss: tensor(1.4869, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 15 \tLoss: tensor(1.5133, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 16 \tLoss: tensor(1.5119, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 17 \tLoss: tensor(1.5006, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 18 \tLoss: tensor(1.4838, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 19 \tLoss: tensor(1.5079, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 20 \tLoss: tensor(1.4913, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 21 \tLoss: tensor(1.5087, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 22 \tLoss: tensor(1.4952, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 23 \tLoss: tensor(1.4867, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 24 \tLoss: tensor(1.5080, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 25 \tLoss: tensor(1.5131, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 26 \tLoss: tensor(1.4781, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 27 \tLoss: tensor(1.4891, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 28 \tLoss: tensor(1.4818, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 29 \tLoss: tensor(1.4738, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 30 \tLoss: tensor(1.4844, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 31 \tLoss: tensor(1.4766, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 32 \tLoss: tensor(1.4952, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 33 \tLoss: tensor(1.5052, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 34 \tLoss: tensor(1.4858, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 35 \tLoss: tensor(1.4876, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 36 \tLoss: tensor(1.4784, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 37 \tLoss: tensor(1.5081, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 38 \tLoss: tensor(1.4932, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 39 \tLoss: tensor(1.5009, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 40 \tLoss: tensor(1.4898, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 41 \tLoss: tensor(1.5012, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 42 \tLoss: tensor(1.4905, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 43 \tLoss: tensor(1.4800, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 44 \tLoss: tensor(1.5038, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 45 \tLoss: tensor(1.4905, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 46 \tLoss: tensor(1.4839, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 47 \tLoss: tensor(1.4857, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 48 \tLoss: tensor(1.4922, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 49 \tLoss: tensor(1.4790, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 50 \tLoss: tensor(1.5050, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 51 \tLoss: tensor(1.5092, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 52 \tLoss: tensor(1.4929, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 53 \tLoss: tensor(1.4850, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 54 \tLoss: tensor(1.4911, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 55 \tLoss: tensor(1.5170, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 56 \tLoss: tensor(1.4847, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 57 \tLoss: tensor(1.4998, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 58 \tLoss: tensor(1.4839, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 59 \tLoss: tensor(1.4861, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 60 \tLoss: tensor(1.5134, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 61 \tLoss: tensor(1.4993, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 62 \tLoss: tensor(1.5168, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 63 \tLoss: tensor(1.5021, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 64 \tLoss: tensor(1.4900, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 65 \tLoss: tensor(1.5072, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 66 \tLoss: tensor(1.5074, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 67 \tLoss: tensor(1.4959, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 68 \tLoss: tensor(1.4819, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 69 \tLoss: tensor(1.4941, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 70 \tLoss: tensor(1.4784, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 71 \tLoss: tensor(1.4998, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 72 \tLoss: tensor(1.4876, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 73 \tLoss: tensor(1.5052, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 74 \tLoss: tensor(1.4959, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 75 \tLoss: tensor(1.5195, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 76 \tLoss: tensor(1.4880, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 77 \tLoss: tensor(1.4904, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 78 \tLoss: tensor(1.4851, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 79 \tLoss: tensor(1.4840, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 80 \tLoss: tensor(1.4966, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 81 \tLoss: tensor(1.5018, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 82 \tLoss: tensor(1.4990, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 83 \tLoss: tensor(1.5077, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 84 \tLoss: tensor(1.4913, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 85 \tLoss: tensor(1.4976, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 86 \tLoss: tensor(1.4920, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 87 \tLoss: tensor(1.5088, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 88 \tLoss: tensor(1.4875, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 89 \tLoss: tensor(1.4885, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 90 \tLoss: tensor(1.4881, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 91 \tLoss: tensor(1.4955, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 92 \tLoss: tensor(1.4860, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 93 \tLoss: tensor(1.4880, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 94 \tLoss: tensor(1.5050, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 95 \tLoss: tensor(1.4872, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 96 \tLoss: tensor(1.4772, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 97 \tLoss: tensor(1.4970, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 98 \tLoss: tensor(1.4843, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 99 \tLoss: tensor(1.5081, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 100 \tLoss: tensor(1.5140, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 101 \tLoss: tensor(1.4856, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 102 \tLoss: tensor(1.5148, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 103 \tLoss: tensor(1.4880, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 104 \tLoss: tensor(1.5078, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 105 \tLoss: tensor(1.4974, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 106 \tLoss: tensor(1.5097, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 107 \tLoss: tensor(1.5093, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 108 \tLoss: tensor(1.4967, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 109 \tLoss: tensor(1.5103, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 110 \tLoss: tensor(1.4976, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 111 \tLoss: tensor(1.5188, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 112 \tLoss: tensor(1.5041, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 113 \tLoss: tensor(1.4977, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 114 \tLoss: tensor(1.4939, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 115 \tLoss: tensor(1.4893, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 116 \tLoss: tensor(1.4923, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 117 \tLoss: tensor(1.5100, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 118 \tLoss: tensor(1.4910, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 119 \tLoss: tensor(1.4847, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 120 \tLoss: tensor(1.4906, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 121 \tLoss: tensor(1.4848, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 122 \tLoss: tensor(1.4858, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 123 \tLoss: tensor(1.4987, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 124 \tLoss: tensor(1.4890, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 125 \tLoss: tensor(1.4910, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 126 \tLoss: tensor(1.5017, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 127 \tLoss: tensor(1.5129, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 128 \tLoss: tensor(1.5079, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 129 \tLoss: tensor(1.4853, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 130 \tLoss: tensor(1.4956, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 131 \tLoss: tensor(1.5180, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 132 \tLoss: tensor(1.4936, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 133 \tLoss: tensor(1.4863, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 134 \tLoss: tensor(1.4828, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 135 \tLoss: tensor(1.4948, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 136 \tLoss: tensor(1.4952, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 137 \tLoss: tensor(1.4883, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 138 \tLoss: tensor(1.4916, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 139 \tLoss: tensor(1.4940, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 140 \tLoss: tensor(1.4930, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 141 \tLoss: tensor(1.4847, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 142 \tLoss: tensor(1.4996, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 143 \tLoss: tensor(1.4715, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 144 \tLoss: tensor(1.4887, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 145 \tLoss: tensor(1.5018, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 146 \tLoss: tensor(1.4948, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 147 \tLoss: tensor(1.4886, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 148 \tLoss: tensor(1.4779, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 149 \tLoss: tensor(1.4785, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 150 \tLoss: tensor(1.4899, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 151 \tLoss: tensor(1.4958, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 152 \tLoss: tensor(1.4887, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 153 \tLoss: tensor(1.5182, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 154 \tLoss: tensor(1.4965, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 155 \tLoss: tensor(1.4973, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 156 \tLoss: tensor(1.4906, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 157 \tLoss: tensor(1.4929, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 158 \tLoss: tensor(1.5064, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 159 \tLoss: tensor(1.4874, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 160 \tLoss: tensor(1.4808, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 161 \tLoss: tensor(1.4967, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 162 \tLoss: tensor(1.4855, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 163 \tLoss: tensor(1.4916, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 164 \tLoss: tensor(1.4852, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 165 \tLoss: tensor(1.4910, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 166 \tLoss: tensor(1.5236, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 167 \tLoss: tensor(1.5122, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 168 \tLoss: tensor(1.5000, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 169 \tLoss: tensor(1.4988, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 170 \tLoss: tensor(1.4975, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 171 \tLoss: tensor(1.5004, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 172 \tLoss: tensor(1.5176, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 173 \tLoss: tensor(1.4897, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 174 \tLoss: tensor(1.4932, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 175 \tLoss: tensor(1.5082, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 176 \tLoss: tensor(1.4930, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 177 \tLoss: tensor(1.4848, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 178 \tLoss: tensor(1.5000, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 179 \tLoss: tensor(1.4921, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 180 \tLoss: tensor(1.4979, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 181 \tLoss: tensor(1.4866, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 182 \tLoss: tensor(1.4859, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 183 \tLoss: tensor(1.4887, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 184 \tLoss: tensor(1.5016, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 185 \tLoss: tensor(1.4827, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 186 \tLoss: tensor(1.5073, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 187 \tLoss: tensor(1.4865, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 188 \tLoss: tensor(1.5093, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 189 \tLoss: tensor(1.5040, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 190 \tLoss: tensor(1.4870, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 191 \tLoss: tensor(1.4849, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 192 \tLoss: tensor(1.4954, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 193 \tLoss: tensor(1.4960, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 194 \tLoss: tensor(1.4934, grad_fn=<NllLossBackward>)\n",
            "Epoch: 3 \tBatch: 195 \tLoss: tensor(1.4818, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 0 \tLoss: tensor(1.4960, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 1 \tLoss: tensor(1.4712, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 2 \tLoss: tensor(1.4972, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 3 \tLoss: tensor(1.4904, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 4 \tLoss: tensor(1.5025, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 5 \tLoss: tensor(1.4964, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 6 \tLoss: tensor(1.4912, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 7 \tLoss: tensor(1.5081, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 8 \tLoss: tensor(1.4909, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 9 \tLoss: tensor(1.5043, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 10 \tLoss: tensor(1.4845, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 11 \tLoss: tensor(1.5082, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 12 \tLoss: tensor(1.4925, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 13 \tLoss: tensor(1.4970, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 14 \tLoss: tensor(1.4887, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 15 \tLoss: tensor(1.4818, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 16 \tLoss: tensor(1.4811, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 17 \tLoss: tensor(1.5067, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 18 \tLoss: tensor(1.4945, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 19 \tLoss: tensor(1.4913, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 20 \tLoss: tensor(1.5058, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 21 \tLoss: tensor(1.4859, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 22 \tLoss: tensor(1.4930, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 23 \tLoss: tensor(1.4892, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 24 \tLoss: tensor(1.4840, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 25 \tLoss: tensor(1.4903, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 26 \tLoss: tensor(1.4850, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 27 \tLoss: tensor(1.4871, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 28 \tLoss: tensor(1.4771, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 29 \tLoss: tensor(1.4846, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 30 \tLoss: tensor(1.4831, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 31 \tLoss: tensor(1.4740, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 32 \tLoss: tensor(1.4836, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 33 \tLoss: tensor(1.4781, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 34 \tLoss: tensor(1.4979, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 35 \tLoss: tensor(1.4868, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 36 \tLoss: tensor(1.4820, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 37 \tLoss: tensor(1.4906, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 38 \tLoss: tensor(1.4805, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 39 \tLoss: tensor(1.4917, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 40 \tLoss: tensor(1.4817, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 41 \tLoss: tensor(1.4833, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 42 \tLoss: tensor(1.4894, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 43 \tLoss: tensor(1.4752, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 44 \tLoss: tensor(1.4730, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 45 \tLoss: tensor(1.4758, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 46 \tLoss: tensor(1.4812, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 47 \tLoss: tensor(1.4845, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 48 \tLoss: tensor(1.5005, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 49 \tLoss: tensor(1.4945, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 50 \tLoss: tensor(1.4960, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 51 \tLoss: tensor(1.4887, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 52 \tLoss: tensor(1.4785, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 53 \tLoss: tensor(1.4877, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 54 \tLoss: tensor(1.5075, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 55 \tLoss: tensor(1.4861, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 56 \tLoss: tensor(1.4892, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 57 \tLoss: tensor(1.4896, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 58 \tLoss: tensor(1.4765, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 59 \tLoss: tensor(1.4779, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 60 \tLoss: tensor(1.4803, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 61 \tLoss: tensor(1.5253, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 62 \tLoss: tensor(1.5025, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 63 \tLoss: tensor(1.4835, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 64 \tLoss: tensor(1.4908, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 65 \tLoss: tensor(1.4881, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 66 \tLoss: tensor(1.4913, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 67 \tLoss: tensor(1.4991, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 68 \tLoss: tensor(1.5028, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 69 \tLoss: tensor(1.4865, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 70 \tLoss: tensor(1.4925, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 71 \tLoss: tensor(1.4849, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 72 \tLoss: tensor(1.4865, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 73 \tLoss: tensor(1.4975, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 74 \tLoss: tensor(1.4830, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 75 \tLoss: tensor(1.4904, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 76 \tLoss: tensor(1.4674, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 77 \tLoss: tensor(1.4956, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 78 \tLoss: tensor(1.4667, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 79 \tLoss: tensor(1.5018, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 80 \tLoss: tensor(1.4761, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 81 \tLoss: tensor(1.4772, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 82 \tLoss: tensor(1.4977, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 83 \tLoss: tensor(1.4944, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 84 \tLoss: tensor(1.4826, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 85 \tLoss: tensor(1.4942, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 86 \tLoss: tensor(1.4825, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 87 \tLoss: tensor(1.4919, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 88 \tLoss: tensor(1.4751, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 89 \tLoss: tensor(1.4967, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 90 \tLoss: tensor(1.4977, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 91 \tLoss: tensor(1.4771, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 92 \tLoss: tensor(1.4905, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 93 \tLoss: tensor(1.5036, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 94 \tLoss: tensor(1.4828, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 95 \tLoss: tensor(1.4798, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 96 \tLoss: tensor(1.4849, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 97 \tLoss: tensor(1.4904, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 98 \tLoss: tensor(1.4974, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 99 \tLoss: tensor(1.5082, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 100 \tLoss: tensor(1.4859, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 101 \tLoss: tensor(1.4934, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 102 \tLoss: tensor(1.4975, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 103 \tLoss: tensor(1.4828, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 104 \tLoss: tensor(1.4926, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 105 \tLoss: tensor(1.4895, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 106 \tLoss: tensor(1.4733, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 107 \tLoss: tensor(1.4748, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 108 \tLoss: tensor(1.4890, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 109 \tLoss: tensor(1.5138, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 110 \tLoss: tensor(1.4764, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 111 \tLoss: tensor(1.4872, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 112 \tLoss: tensor(1.5078, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 113 \tLoss: tensor(1.4937, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 114 \tLoss: tensor(1.4973, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 115 \tLoss: tensor(1.5021, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 116 \tLoss: tensor(1.4758, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 117 \tLoss: tensor(1.4851, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 118 \tLoss: tensor(1.4808, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 119 \tLoss: tensor(1.4841, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 120 \tLoss: tensor(1.4969, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 121 \tLoss: tensor(1.4726, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 122 \tLoss: tensor(1.4962, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 123 \tLoss: tensor(1.4922, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 124 \tLoss: tensor(1.4957, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 125 \tLoss: tensor(1.4849, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 126 \tLoss: tensor(1.4831, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 127 \tLoss: tensor(1.4921, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 128 \tLoss: tensor(1.4796, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 129 \tLoss: tensor(1.4818, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 130 \tLoss: tensor(1.5015, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 131 \tLoss: tensor(1.4862, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 132 \tLoss: tensor(1.4958, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 133 \tLoss: tensor(1.4944, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 134 \tLoss: tensor(1.4919, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 135 \tLoss: tensor(1.4875, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 136 \tLoss: tensor(1.4837, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 137 \tLoss: tensor(1.4855, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 138 \tLoss: tensor(1.4807, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 139 \tLoss: tensor(1.4967, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 140 \tLoss: tensor(1.4863, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 141 \tLoss: tensor(1.4821, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 142 \tLoss: tensor(1.4854, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 143 \tLoss: tensor(1.4898, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 144 \tLoss: tensor(1.4843, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 145 \tLoss: tensor(1.4848, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 146 \tLoss: tensor(1.4962, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 147 \tLoss: tensor(1.4960, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 148 \tLoss: tensor(1.4833, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 149 \tLoss: tensor(1.4951, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 150 \tLoss: tensor(1.4908, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 151 \tLoss: tensor(1.5087, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 152 \tLoss: tensor(1.4993, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 153 \tLoss: tensor(1.5033, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 154 \tLoss: tensor(1.4906, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 155 \tLoss: tensor(1.4920, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 156 \tLoss: tensor(1.4911, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 157 \tLoss: tensor(1.4965, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 158 \tLoss: tensor(1.4856, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 159 \tLoss: tensor(1.4780, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 160 \tLoss: tensor(1.4933, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 161 \tLoss: tensor(1.4988, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 162 \tLoss: tensor(1.4855, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 163 \tLoss: tensor(1.5013, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 164 \tLoss: tensor(1.4735, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 165 \tLoss: tensor(1.4801, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 166 \tLoss: tensor(1.4833, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 167 \tLoss: tensor(1.4959, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 168 \tLoss: tensor(1.4835, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 169 \tLoss: tensor(1.4781, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 170 \tLoss: tensor(1.4859, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 171 \tLoss: tensor(1.4882, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 172 \tLoss: tensor(1.4753, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 173 \tLoss: tensor(1.4944, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 174 \tLoss: tensor(1.4790, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 175 \tLoss: tensor(1.4903, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 176 \tLoss: tensor(1.5084, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 177 \tLoss: tensor(1.4831, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 178 \tLoss: tensor(1.4788, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 179 \tLoss: tensor(1.4862, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 180 \tLoss: tensor(1.4688, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 181 \tLoss: tensor(1.4868, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 182 \tLoss: tensor(1.4752, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 183 \tLoss: tensor(1.4818, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 184 \tLoss: tensor(1.5009, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 185 \tLoss: tensor(1.5088, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 186 \tLoss: tensor(1.4714, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 187 \tLoss: tensor(1.4861, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 188 \tLoss: tensor(1.4796, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 189 \tLoss: tensor(1.4926, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 190 \tLoss: tensor(1.5025, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 191 \tLoss: tensor(1.4903, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 192 \tLoss: tensor(1.4934, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 193 \tLoss: tensor(1.5016, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 194 \tLoss: tensor(1.4823, grad_fn=<NllLossBackward>)\n",
            "Epoch: 4 \tBatch: 195 \tLoss: tensor(1.4942, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 0 \tLoss: tensor(1.4769, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 1 \tLoss: tensor(1.4712, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 2 \tLoss: tensor(1.4806, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 3 \tLoss: tensor(1.4796, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 4 \tLoss: tensor(1.4894, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 5 \tLoss: tensor(1.4722, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 6 \tLoss: tensor(1.4720, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 7 \tLoss: tensor(1.4899, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 8 \tLoss: tensor(1.4970, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 9 \tLoss: tensor(1.4742, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 10 \tLoss: tensor(1.4780, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 11 \tLoss: tensor(1.4862, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 12 \tLoss: tensor(1.4836, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 13 \tLoss: tensor(1.4782, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 14 \tLoss: tensor(1.4900, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 15 \tLoss: tensor(1.4927, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 16 \tLoss: tensor(1.4737, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 17 \tLoss: tensor(1.4826, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 18 \tLoss: tensor(1.4983, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 19 \tLoss: tensor(1.4906, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 20 \tLoss: tensor(1.4816, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 21 \tLoss: tensor(1.4867, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 22 \tLoss: tensor(1.4803, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 23 \tLoss: tensor(1.4968, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 24 \tLoss: tensor(1.4812, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 25 \tLoss: tensor(1.4824, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 26 \tLoss: tensor(1.4773, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 27 \tLoss: tensor(1.4840, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 28 \tLoss: tensor(1.4867, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 29 \tLoss: tensor(1.4927, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 30 \tLoss: tensor(1.4726, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 31 \tLoss: tensor(1.4954, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 32 \tLoss: tensor(1.4756, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 33 \tLoss: tensor(1.4777, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 34 \tLoss: tensor(1.4849, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 35 \tLoss: tensor(1.4839, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 36 \tLoss: tensor(1.4828, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 37 \tLoss: tensor(1.4883, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 38 \tLoss: tensor(1.4806, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 39 \tLoss: tensor(1.4868, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 40 \tLoss: tensor(1.4815, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 41 \tLoss: tensor(1.4852, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 42 \tLoss: tensor(1.4804, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 43 \tLoss: tensor(1.4953, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 44 \tLoss: tensor(1.4744, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 45 \tLoss: tensor(1.4804, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 46 \tLoss: tensor(1.4880, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 47 \tLoss: tensor(1.4881, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 48 \tLoss: tensor(1.4888, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 49 \tLoss: tensor(1.4921, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 50 \tLoss: tensor(1.4701, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 51 \tLoss: tensor(1.4702, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 52 \tLoss: tensor(1.4880, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 53 \tLoss: tensor(1.4818, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 54 \tLoss: tensor(1.4864, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 55 \tLoss: tensor(1.4824, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 56 \tLoss: tensor(1.4772, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 57 \tLoss: tensor(1.4751, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 58 \tLoss: tensor(1.4956, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 59 \tLoss: tensor(1.4852, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 60 \tLoss: tensor(1.4967, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 61 \tLoss: tensor(1.4739, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 62 \tLoss: tensor(1.4834, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 63 \tLoss: tensor(1.4880, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 64 \tLoss: tensor(1.4890, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 65 \tLoss: tensor(1.4888, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 66 \tLoss: tensor(1.4835, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 67 \tLoss: tensor(1.4807, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 68 \tLoss: tensor(1.5058, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 69 \tLoss: tensor(1.4923, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 70 \tLoss: tensor(1.4972, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 71 \tLoss: tensor(1.4978, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 72 \tLoss: tensor(1.4963, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 73 \tLoss: tensor(1.4916, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 74 \tLoss: tensor(1.5074, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 75 \tLoss: tensor(1.5105, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 76 \tLoss: tensor(1.4924, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 77 \tLoss: tensor(1.5052, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 78 \tLoss: tensor(1.4838, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 79 \tLoss: tensor(1.4926, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 80 \tLoss: tensor(1.4812, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 81 \tLoss: tensor(1.4871, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 82 \tLoss: tensor(1.4767, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 83 \tLoss: tensor(1.4820, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 84 \tLoss: tensor(1.4824, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 85 \tLoss: tensor(1.4858, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 86 \tLoss: tensor(1.4919, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 87 \tLoss: tensor(1.4854, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 88 \tLoss: tensor(1.4833, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 89 \tLoss: tensor(1.4800, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 90 \tLoss: tensor(1.4769, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 91 \tLoss: tensor(1.4896, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 92 \tLoss: tensor(1.4759, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 93 \tLoss: tensor(1.4823, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 94 \tLoss: tensor(1.4839, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 95 \tLoss: tensor(1.4763, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 96 \tLoss: tensor(1.4943, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 97 \tLoss: tensor(1.4823, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 98 \tLoss: tensor(1.4852, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 99 \tLoss: tensor(1.4803, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 100 \tLoss: tensor(1.4978, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 101 \tLoss: tensor(1.4944, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 102 \tLoss: tensor(1.4889, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 103 \tLoss: tensor(1.4766, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 104 \tLoss: tensor(1.4835, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 105 \tLoss: tensor(1.4825, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 106 \tLoss: tensor(1.4698, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 107 \tLoss: tensor(1.4742, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 108 \tLoss: tensor(1.4783, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 109 \tLoss: tensor(1.4814, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 110 \tLoss: tensor(1.4908, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 111 \tLoss: tensor(1.4857, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 112 \tLoss: tensor(1.4729, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 113 \tLoss: tensor(1.4757, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 114 \tLoss: tensor(1.4698, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 115 \tLoss: tensor(1.4842, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 116 \tLoss: tensor(1.4729, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 117 \tLoss: tensor(1.4916, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 118 \tLoss: tensor(1.4807, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 119 \tLoss: tensor(1.4747, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 120 \tLoss: tensor(1.4873, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 121 \tLoss: tensor(1.4799, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 122 \tLoss: tensor(1.4960, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 123 \tLoss: tensor(1.4790, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 124 \tLoss: tensor(1.4735, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 125 \tLoss: tensor(1.4899, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 126 \tLoss: tensor(1.4731, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 127 \tLoss: tensor(1.4741, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 128 \tLoss: tensor(1.4817, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 129 \tLoss: tensor(1.4808, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 130 \tLoss: tensor(1.4853, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 131 \tLoss: tensor(1.4761, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 132 \tLoss: tensor(1.4770, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 133 \tLoss: tensor(1.4758, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 134 \tLoss: tensor(1.4812, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 135 \tLoss: tensor(1.4960, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 136 \tLoss: tensor(1.4886, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 137 \tLoss: tensor(1.4961, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 138 \tLoss: tensor(1.4744, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 139 \tLoss: tensor(1.4855, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 140 \tLoss: tensor(1.4890, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 141 \tLoss: tensor(1.4835, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 142 \tLoss: tensor(1.4973, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 143 \tLoss: tensor(1.4731, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 144 \tLoss: tensor(1.4706, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 145 \tLoss: tensor(1.4988, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 146 \tLoss: tensor(1.4747, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 147 \tLoss: tensor(1.4921, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 148 \tLoss: tensor(1.4923, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 149 \tLoss: tensor(1.5027, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 150 \tLoss: tensor(1.5050, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 151 \tLoss: tensor(1.4824, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 152 \tLoss: tensor(1.4895, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 153 \tLoss: tensor(1.4930, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 154 \tLoss: tensor(1.4877, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 155 \tLoss: tensor(1.4815, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 156 \tLoss: tensor(1.4906, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 157 \tLoss: tensor(1.4752, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 158 \tLoss: tensor(1.4804, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 159 \tLoss: tensor(1.4768, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 160 \tLoss: tensor(1.4760, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 161 \tLoss: tensor(1.4918, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 162 \tLoss: tensor(1.4944, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 163 \tLoss: tensor(1.4823, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 164 \tLoss: tensor(1.4922, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 165 \tLoss: tensor(1.4892, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 166 \tLoss: tensor(1.4839, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 167 \tLoss: tensor(1.4954, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 168 \tLoss: tensor(1.4802, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 169 \tLoss: tensor(1.4775, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 170 \tLoss: tensor(1.4941, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 171 \tLoss: tensor(1.4827, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 172 \tLoss: tensor(1.5079, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 173 \tLoss: tensor(1.4902, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 174 \tLoss: tensor(1.4778, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 175 \tLoss: tensor(1.4829, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 176 \tLoss: tensor(1.4929, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 177 \tLoss: tensor(1.4919, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 178 \tLoss: tensor(1.4899, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 179 \tLoss: tensor(1.4875, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 180 \tLoss: tensor(1.4800, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 181 \tLoss: tensor(1.4979, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 182 \tLoss: tensor(1.4896, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 183 \tLoss: tensor(1.4835, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 184 \tLoss: tensor(1.4884, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 185 \tLoss: tensor(1.4733, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 186 \tLoss: tensor(1.4850, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 187 \tLoss: tensor(1.4912, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 188 \tLoss: tensor(1.4827, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 189 \tLoss: tensor(1.4910, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 190 \tLoss: tensor(1.4846, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 191 \tLoss: tensor(1.4799, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 192 \tLoss: tensor(1.4795, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 193 \tLoss: tensor(1.4735, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 194 \tLoss: tensor(1.5008, grad_fn=<NllLossBackward>)\n",
            "Epoch: 5 \tBatch: 195 \tLoss: tensor(1.4973, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 0 \tLoss: tensor(1.4888, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 1 \tLoss: tensor(1.4847, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 2 \tLoss: tensor(1.4893, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 3 \tLoss: tensor(1.4793, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 4 \tLoss: tensor(1.4855, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 5 \tLoss: tensor(1.4783, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 6 \tLoss: tensor(1.4929, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 7 \tLoss: tensor(1.4982, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 8 \tLoss: tensor(1.4883, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 9 \tLoss: tensor(1.4907, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 10 \tLoss: tensor(1.4886, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 11 \tLoss: tensor(1.4836, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 12 \tLoss: tensor(1.4769, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 13 \tLoss: tensor(1.4868, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 14 \tLoss: tensor(1.4901, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 15 \tLoss: tensor(1.4816, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 16 \tLoss: tensor(1.4788, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 17 \tLoss: tensor(1.4686, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 18 \tLoss: tensor(1.4798, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 19 \tLoss: tensor(1.4801, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 20 \tLoss: tensor(1.4731, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 21 \tLoss: tensor(1.4842, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 22 \tLoss: tensor(1.4833, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 23 \tLoss: tensor(1.4801, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 24 \tLoss: tensor(1.4781, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 25 \tLoss: tensor(1.5160, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 26 \tLoss: tensor(1.4880, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 27 \tLoss: tensor(1.4901, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 28 \tLoss: tensor(1.5044, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 29 \tLoss: tensor(1.4809, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 30 \tLoss: tensor(1.4785, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 31 \tLoss: tensor(1.4909, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 32 \tLoss: tensor(1.4841, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 33 \tLoss: tensor(1.4850, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 34 \tLoss: tensor(1.4889, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 35 \tLoss: tensor(1.4887, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 36 \tLoss: tensor(1.4726, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 37 \tLoss: tensor(1.4840, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 38 \tLoss: tensor(1.4784, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 39 \tLoss: tensor(1.4762, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 40 \tLoss: tensor(1.4877, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 41 \tLoss: tensor(1.4701, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 42 \tLoss: tensor(1.4803, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 43 \tLoss: tensor(1.4869, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 44 \tLoss: tensor(1.4710, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 45 \tLoss: tensor(1.5032, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 46 \tLoss: tensor(1.4651, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 47 \tLoss: tensor(1.4890, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 48 \tLoss: tensor(1.4928, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 49 \tLoss: tensor(1.4786, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 50 \tLoss: tensor(1.4780, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 51 \tLoss: tensor(1.4815, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 52 \tLoss: tensor(1.4929, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 53 \tLoss: tensor(1.4809, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 54 \tLoss: tensor(1.4870, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 55 \tLoss: tensor(1.4822, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 56 \tLoss: tensor(1.4901, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 57 \tLoss: tensor(1.4848, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 58 \tLoss: tensor(1.4889, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 59 \tLoss: tensor(1.4829, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 60 \tLoss: tensor(1.4711, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 61 \tLoss: tensor(1.4736, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 62 \tLoss: tensor(1.4885, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 63 \tLoss: tensor(1.4887, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 64 \tLoss: tensor(1.4839, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 65 \tLoss: tensor(1.4847, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 66 \tLoss: tensor(1.4733, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 67 \tLoss: tensor(1.4735, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 68 \tLoss: tensor(1.4850, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 69 \tLoss: tensor(1.4917, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 70 \tLoss: tensor(1.4830, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 71 \tLoss: tensor(1.4819, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 72 \tLoss: tensor(1.4765, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 73 \tLoss: tensor(1.4814, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 74 \tLoss: tensor(1.4769, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 75 \tLoss: tensor(1.4782, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 76 \tLoss: tensor(1.4865, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 77 \tLoss: tensor(1.4720, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 78 \tLoss: tensor(1.4717, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 79 \tLoss: tensor(1.4924, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 80 \tLoss: tensor(1.4759, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 81 \tLoss: tensor(1.4891, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 82 \tLoss: tensor(1.4726, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 83 \tLoss: tensor(1.4667, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 84 \tLoss: tensor(1.4799, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 85 \tLoss: tensor(1.4810, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 86 \tLoss: tensor(1.4892, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 87 \tLoss: tensor(1.4793, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 88 \tLoss: tensor(1.4976, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 89 \tLoss: tensor(1.4763, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 90 \tLoss: tensor(1.4796, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 91 \tLoss: tensor(1.4677, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 92 \tLoss: tensor(1.4648, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 93 \tLoss: tensor(1.4864, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 94 \tLoss: tensor(1.4741, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 95 \tLoss: tensor(1.4896, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 96 \tLoss: tensor(1.4880, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 97 \tLoss: tensor(1.4931, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 98 \tLoss: tensor(1.4817, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 99 \tLoss: tensor(1.4897, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 100 \tLoss: tensor(1.4787, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 101 \tLoss: tensor(1.4876, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 102 \tLoss: tensor(1.4707, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 103 \tLoss: tensor(1.4884, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 104 \tLoss: tensor(1.4786, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 105 \tLoss: tensor(1.4660, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 106 \tLoss: tensor(1.4908, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 107 \tLoss: tensor(1.4882, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 108 \tLoss: tensor(1.4884, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 109 \tLoss: tensor(1.4818, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 110 \tLoss: tensor(1.4694, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 111 \tLoss: tensor(1.4830, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 112 \tLoss: tensor(1.4960, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 113 \tLoss: tensor(1.4898, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 114 \tLoss: tensor(1.4724, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 115 \tLoss: tensor(1.4826, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 116 \tLoss: tensor(1.4828, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 117 \tLoss: tensor(1.4795, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 118 \tLoss: tensor(1.4863, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 119 \tLoss: tensor(1.4872, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 120 \tLoss: tensor(1.4828, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 121 \tLoss: tensor(1.4806, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 122 \tLoss: tensor(1.4934, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 123 \tLoss: tensor(1.4824, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 124 \tLoss: tensor(1.4771, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 125 \tLoss: tensor(1.4817, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 126 \tLoss: tensor(1.4887, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 127 \tLoss: tensor(1.4817, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 128 \tLoss: tensor(1.4822, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 129 \tLoss: tensor(1.4715, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 130 \tLoss: tensor(1.4949, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 131 \tLoss: tensor(1.4889, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 132 \tLoss: tensor(1.4843, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 133 \tLoss: tensor(1.4784, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 134 \tLoss: tensor(1.4729, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 135 \tLoss: tensor(1.4879, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 136 \tLoss: tensor(1.4731, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 137 \tLoss: tensor(1.4824, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 138 \tLoss: tensor(1.4744, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 139 \tLoss: tensor(1.4807, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 140 \tLoss: tensor(1.4750, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 141 \tLoss: tensor(1.4933, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 142 \tLoss: tensor(1.4804, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 143 \tLoss: tensor(1.4824, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 144 \tLoss: tensor(1.4740, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 145 \tLoss: tensor(1.4920, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 146 \tLoss: tensor(1.4749, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 147 \tLoss: tensor(1.4873, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 148 \tLoss: tensor(1.4864, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 149 \tLoss: tensor(1.4784, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 150 \tLoss: tensor(1.4760, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 151 \tLoss: tensor(1.4804, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 152 \tLoss: tensor(1.4783, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 153 \tLoss: tensor(1.4759, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 154 \tLoss: tensor(1.4678, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 155 \tLoss: tensor(1.4859, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 156 \tLoss: tensor(1.4781, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 157 \tLoss: tensor(1.4813, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 158 \tLoss: tensor(1.4772, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 159 \tLoss: tensor(1.4791, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 160 \tLoss: tensor(1.4847, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 161 \tLoss: tensor(1.4812, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 162 \tLoss: tensor(1.4901, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 163 \tLoss: tensor(1.4672, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 164 \tLoss: tensor(1.4957, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 165 \tLoss: tensor(1.4821, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 166 \tLoss: tensor(1.4851, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 167 \tLoss: tensor(1.4815, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 168 \tLoss: tensor(1.4719, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 169 \tLoss: tensor(1.4870, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 170 \tLoss: tensor(1.5100, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 171 \tLoss: tensor(1.4843, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 172 \tLoss: tensor(1.4763, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 173 \tLoss: tensor(1.4764, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 174 \tLoss: tensor(1.4787, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 175 \tLoss: tensor(1.4808, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 176 \tLoss: tensor(1.4885, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 177 \tLoss: tensor(1.4878, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 178 \tLoss: tensor(1.4835, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 179 \tLoss: tensor(1.4958, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 180 \tLoss: tensor(1.4861, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 181 \tLoss: tensor(1.4771, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 182 \tLoss: tensor(1.4804, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 183 \tLoss: tensor(1.4705, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 184 \tLoss: tensor(1.4917, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 185 \tLoss: tensor(1.4733, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 186 \tLoss: tensor(1.4777, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 187 \tLoss: tensor(1.4746, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 188 \tLoss: tensor(1.4876, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 189 \tLoss: tensor(1.4959, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 190 \tLoss: tensor(1.4696, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 191 \tLoss: tensor(1.4820, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 192 \tLoss: tensor(1.4785, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 193 \tLoss: tensor(1.4677, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 194 \tLoss: tensor(1.4713, grad_fn=<NllLossBackward>)\n",
            "Epoch: 6 \tBatch: 195 \tLoss: tensor(1.4858, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 0 \tLoss: tensor(1.4719, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 1 \tLoss: tensor(1.4793, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 2 \tLoss: tensor(1.4777, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 3 \tLoss: tensor(1.4748, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 4 \tLoss: tensor(1.4785, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 5 \tLoss: tensor(1.4764, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 6 \tLoss: tensor(1.4718, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 7 \tLoss: tensor(1.4656, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 8 \tLoss: tensor(1.4691, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 9 \tLoss: tensor(1.4747, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 10 \tLoss: tensor(1.4880, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 11 \tLoss: tensor(1.4833, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 12 \tLoss: tensor(1.4807, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 13 \tLoss: tensor(1.4800, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 14 \tLoss: tensor(1.4777, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 15 \tLoss: tensor(1.5005, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 16 \tLoss: tensor(1.4830, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 17 \tLoss: tensor(1.4791, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 18 \tLoss: tensor(1.4706, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 19 \tLoss: tensor(1.4872, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 20 \tLoss: tensor(1.4752, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 21 \tLoss: tensor(1.4847, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 22 \tLoss: tensor(1.4764, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 23 \tLoss: tensor(1.4796, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 24 \tLoss: tensor(1.4862, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 25 \tLoss: tensor(1.4820, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 26 \tLoss: tensor(1.4712, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 27 \tLoss: tensor(1.4672, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 28 \tLoss: tensor(1.4819, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 29 \tLoss: tensor(1.4872, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 30 \tLoss: tensor(1.4858, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 31 \tLoss: tensor(1.5060, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 32 \tLoss: tensor(1.4871, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 33 \tLoss: tensor(1.4775, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 34 \tLoss: tensor(1.4802, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 35 \tLoss: tensor(1.4681, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 36 \tLoss: tensor(1.4869, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 37 \tLoss: tensor(1.4778, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 38 \tLoss: tensor(1.4777, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 39 \tLoss: tensor(1.4925, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 40 \tLoss: tensor(1.4749, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 41 \tLoss: tensor(1.4755, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 42 \tLoss: tensor(1.4822, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 43 \tLoss: tensor(1.4928, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 44 \tLoss: tensor(1.4736, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 45 \tLoss: tensor(1.4730, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 46 \tLoss: tensor(1.4733, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 47 \tLoss: tensor(1.4911, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 48 \tLoss: tensor(1.4803, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 49 \tLoss: tensor(1.4746, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 50 \tLoss: tensor(1.4786, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 51 \tLoss: tensor(1.4706, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 52 \tLoss: tensor(1.4740, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 53 \tLoss: tensor(1.4990, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 54 \tLoss: tensor(1.4746, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 55 \tLoss: tensor(1.4684, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 56 \tLoss: tensor(1.4752, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 57 \tLoss: tensor(1.4783, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 58 \tLoss: tensor(1.4798, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 59 \tLoss: tensor(1.4918, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 60 \tLoss: tensor(1.4780, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 61 \tLoss: tensor(1.4823, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 62 \tLoss: tensor(1.4697, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 63 \tLoss: tensor(1.4801, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 64 \tLoss: tensor(1.4860, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 65 \tLoss: tensor(1.4695, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 66 \tLoss: tensor(1.4748, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 67 \tLoss: tensor(1.4788, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 68 \tLoss: tensor(1.4807, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 69 \tLoss: tensor(1.4749, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 70 \tLoss: tensor(1.4833, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 71 \tLoss: tensor(1.4989, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 72 \tLoss: tensor(1.4868, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 73 \tLoss: tensor(1.4943, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 74 \tLoss: tensor(1.4822, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 75 \tLoss: tensor(1.4811, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 76 \tLoss: tensor(1.4794, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 77 \tLoss: tensor(1.4662, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 78 \tLoss: tensor(1.4846, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 79 \tLoss: tensor(1.4789, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 80 \tLoss: tensor(1.4778, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 81 \tLoss: tensor(1.4671, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 82 \tLoss: tensor(1.4821, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 83 \tLoss: tensor(1.4841, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 84 \tLoss: tensor(1.4749, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 85 \tLoss: tensor(1.4903, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 86 \tLoss: tensor(1.4802, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 87 \tLoss: tensor(1.4856, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 88 \tLoss: tensor(1.4897, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 89 \tLoss: tensor(1.4764, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 90 \tLoss: tensor(1.4873, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 91 \tLoss: tensor(1.4901, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 92 \tLoss: tensor(1.4896, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 93 \tLoss: tensor(1.4696, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 94 \tLoss: tensor(1.4760, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 95 \tLoss: tensor(1.4754, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 96 \tLoss: tensor(1.4862, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 97 \tLoss: tensor(1.4705, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 98 \tLoss: tensor(1.4757, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 99 \tLoss: tensor(1.4753, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 100 \tLoss: tensor(1.4881, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 101 \tLoss: tensor(1.4774, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 102 \tLoss: tensor(1.4769, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 103 \tLoss: tensor(1.4881, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 104 \tLoss: tensor(1.4848, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 105 \tLoss: tensor(1.4726, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 106 \tLoss: tensor(1.5047, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 107 \tLoss: tensor(1.4728, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 108 \tLoss: tensor(1.4854, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 109 \tLoss: tensor(1.4772, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 110 \tLoss: tensor(1.4840, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 111 \tLoss: tensor(1.4892, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 112 \tLoss: tensor(1.4909, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 113 \tLoss: tensor(1.4855, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 114 \tLoss: tensor(1.4734, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 115 \tLoss: tensor(1.4931, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 116 \tLoss: tensor(1.4781, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 117 \tLoss: tensor(1.4884, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 118 \tLoss: tensor(1.4840, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 119 \tLoss: tensor(1.4691, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 120 \tLoss: tensor(1.4823, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 121 \tLoss: tensor(1.4736, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 122 \tLoss: tensor(1.4760, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 123 \tLoss: tensor(1.4855, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 124 \tLoss: tensor(1.4821, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 125 \tLoss: tensor(1.4767, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 126 \tLoss: tensor(1.4827, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 127 \tLoss: tensor(1.5050, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 128 \tLoss: tensor(1.4787, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 129 \tLoss: tensor(1.4825, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 130 \tLoss: tensor(1.4942, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 131 \tLoss: tensor(1.4815, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 132 \tLoss: tensor(1.4859, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 133 \tLoss: tensor(1.4885, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 134 \tLoss: tensor(1.4803, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 135 \tLoss: tensor(1.4800, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 136 \tLoss: tensor(1.4793, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 137 \tLoss: tensor(1.4852, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 138 \tLoss: tensor(1.4923, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 139 \tLoss: tensor(1.4794, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 140 \tLoss: tensor(1.4896, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 141 \tLoss: tensor(1.4930, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 142 \tLoss: tensor(1.4768, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 143 \tLoss: tensor(1.4818, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 144 \tLoss: tensor(1.4720, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 145 \tLoss: tensor(1.4770, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 146 \tLoss: tensor(1.4846, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 147 \tLoss: tensor(1.4825, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 148 \tLoss: tensor(1.4849, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 149 \tLoss: tensor(1.4769, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 150 \tLoss: tensor(1.4838, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 151 \tLoss: tensor(1.4729, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 152 \tLoss: tensor(1.4857, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 153 \tLoss: tensor(1.4829, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 154 \tLoss: tensor(1.4803, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 155 \tLoss: tensor(1.4784, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 156 \tLoss: tensor(1.4776, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 157 \tLoss: tensor(1.4860, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 158 \tLoss: tensor(1.4734, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 159 \tLoss: tensor(1.4893, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 160 \tLoss: tensor(1.4751, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 161 \tLoss: tensor(1.4896, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 162 \tLoss: tensor(1.4693, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 163 \tLoss: tensor(1.4906, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 164 \tLoss: tensor(1.4794, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 165 \tLoss: tensor(1.4851, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 166 \tLoss: tensor(1.4759, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 167 \tLoss: tensor(1.5026, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 168 \tLoss: tensor(1.4821, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 169 \tLoss: tensor(1.4794, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 170 \tLoss: tensor(1.4769, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 171 \tLoss: tensor(1.4730, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 172 \tLoss: tensor(1.4818, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 173 \tLoss: tensor(1.4830, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 174 \tLoss: tensor(1.4779, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 175 \tLoss: tensor(1.4921, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 176 \tLoss: tensor(1.4713, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 177 \tLoss: tensor(1.4791, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 178 \tLoss: tensor(1.4771, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 179 \tLoss: tensor(1.4755, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 180 \tLoss: tensor(1.4742, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 181 \tLoss: tensor(1.4787, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 182 \tLoss: tensor(1.4669, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 183 \tLoss: tensor(1.4842, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 184 \tLoss: tensor(1.4847, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 185 \tLoss: tensor(1.4826, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 186 \tLoss: tensor(1.4719, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 187 \tLoss: tensor(1.4747, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 188 \tLoss: tensor(1.4749, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 189 \tLoss: tensor(1.4755, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 190 \tLoss: tensor(1.4853, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 191 \tLoss: tensor(1.4767, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 192 \tLoss: tensor(1.4815, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 193 \tLoss: tensor(1.4745, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 194 \tLoss: tensor(1.4818, grad_fn=<NllLossBackward>)\n",
            "Epoch: 7 \tBatch: 195 \tLoss: tensor(1.4767, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtttGgrrBy3-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "01434307-760a-49fe-e1f0-b32e1d2587ad"
      },
      "source": [
        "def test(model):\n",
        "    num_correct = 0\n",
        "    num_examples = len(test_data)                       # test DATA not test LOADER\n",
        "    for inputs, labels in test_loader:                  # for all exampls, over all mini-batches in the test dataset\n",
        "        predictions = model(inputs)\n",
        "        predictions = torch.max(predictions, axis=1)    # reduce to find max indices along direction which column varies\n",
        "        predictions = predictions[1]                    # torch.max returns (values, indices)\n",
        "        num_correct += int(sum(predictions == labels))\n",
        "    percent_correct = num_correct / num_examples * 100\n",
        "    print('Accuracy:', percent_correct)\n",
        "    \n",
        "test(my_nn)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 97.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE4wQp9ADHUV",
        "colab_type": "text"
      },
      "source": [
        "Dropout Implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7ohCJIPDMRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh0RTz2ADMmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX6_CNxDDMaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K29OXl5DMEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSYOGP4bBy4B",
        "colab_type": "text"
      },
      "source": [
        "### 5. Summary\n",
        "***List the key points like:***\n",
        "\n",
        "* dropout definition in a sentence\n",
        "* ensemble method\n",
        "* how to overcome overfitting after applying dropout, show model performance metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBQ_028yBy4C",
        "colab_type": "text"
      },
      "source": [
        "### 6. What to do next?\n",
        "* Challenge about dropout, search on net, 3 example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQETT-qaBy4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD2L1lkgBy4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}