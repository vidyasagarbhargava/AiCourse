{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "## Prerequisites\n",
    "- Basic Python\n",
    "- Linear algebra\n",
    "\n",
    "## Learning objectives\n",
    "- Implement your first machine learning algorithm from scratch, in Python\n",
    "- Understand the 4 parts of the general machine learning algorithm framework\n",
    "    - Data\n",
    "    - Models\n",
    "    - Criteria\n",
    "    - Optimisation\n",
    "- Implement a random search algorithm as a trivial optimisation technique\n",
    "\n",
    "## Intro - Classification vs regression\n",
    "\n",
    "Most machine learning problems are one of two types: regression problems or classification problems.\n",
    "As you can probably guess, classification problems are those where the output is categorical.\n",
    "On the other hand, regression problems are those where the output that we want to predict is continuous.\n",
    "\n",
    "For example, predicting house prices from features of houses is a regression problem because those prices can take any value. \n",
    "It could be a decimal. \n",
    "It could be \\\\$10,320,302.50. \n",
    "It could be \\\\$35. \n",
    "It could even be negative if someone wants to pay you to take it off their hands (in some cases this might not make sense, but the problem can still be framed as regression).\n",
    "\n",
    "On the other hand, consider using a house's features to predict it's type, as one of detached, semi-detached or terraced. \n",
    "In this case, there are only a finite number of possible output values. As such, this is a classification problem.\n",
    "\n",
    "## What's linear regression?\n",
    "\n",
    "In this notebook, we will implement a **linear regression** algorithm.\n",
    "This is the classic starting point for jumping into machine learning.\n",
    "\n",
    "Linear regression predicts continuous outputs - hence the regression part of the name.\n",
    "Linear regression makes predictions that are simply a weighted combination (a linear combination) of the inputs (plus some offset). \n",
    "That is a linear equation $y = mx + c$, hence the linear part of the name.\n",
    "\n",
    "## The general framework for machine learning algorithms\n",
    "\n",
    "Almost all machine learning algorithms consist of 4 components:\n",
    "\n",
    "1. the data\n",
    "\n",
    "2. the model\n",
    "\n",
    "3. the criterion\n",
    "\n",
    "4. the optimiser\n",
    "\n",
    "This notebook will introduce you to all of those. \n",
    "By the end, we will have used all of them to implement our first machine learning algorithm - linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data\n",
    "\n",
    "The data represents the input-output relationship that our algorithm will learn.\n",
    "\n",
    "Our aim is to produce a mathematical function that takes in an example and makes a prediction. \n",
    "The data will determine the meaning and shape of our function takes in, and what it outputs as a prediction.\n",
    "\n",
    "Below is a picture of a CSV (comma separated value) file that contains some data about red wines.\n",
    "Yes, red wines (just to highlight that the data can represent anything).\n",
    "\n",
    "![](images/wine.png)\n",
    "\n",
    "One application of this dataset, could be to use the information to predict the quality of the wine from the rest of it's features. \n",
    "\n",
    "### Firstly, how can we represent these examples numerically?\n",
    "\n",
    "To start with, we need to separate the **label** (output) from the **features** (input). So for each example, we will have a single scalar label $y$.\n",
    "\n",
    "In our case, each example has several features. We can group these together mathematically as a vector, $x$. It will have as many rows as there are features in the example. Let's call this number of features per example $n$. \n",
    "\n",
    "![](images/single_data_point.jpg)\n",
    "\n",
    "We have $m$ examples, and indicate an arbitrary example's index with an $i$.\n",
    "\n",
    "We can stack these examples in columns to produce a **design matrix**, $X$, which will then contain all of our data, as shown below.\n",
    "\n",
    "![](images/design_matrix.jpg)\n",
    "\n",
    "The scalar labels for each example can also be arranged into a single vector.\n",
    "\n",
    "![image](images/labels.jpg)\n",
    "\n",
    "Again, please note that this is just one specific example, and that other problems may have wildly different input output formats. \n",
    "The label could consist of many more values; it could even be something like an image. \n",
    "The same is true for the input. \n",
    "As long as it can be represented mathematically, it will be possible to create a model that processes examples of that type.\n",
    "\n",
    "Can you think of any data type that might be hard to represent mathematically?\n",
    "\n",
    "For the rest of this notebook, we will just use a **dummy** (fake/made up) dataset with a single feature for each example. \n",
    "This will make things easier to visualise and prevent some problems which we will address [later](\"Multivariate Regression & Feature Normalisation.ipynb\")\n",
    "\n",
    "Now we know what our data should look like, let's get it in that format. Here, we'll just use a function that we've got in a utils (utilities) file (```utils.py```) to get some visualisable, dummy regression data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import get_regression_data\n",
    " \n",
    "X, Y = get_regression_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data, $X$, represented in the form we want. Can we get any closer to predicting the label, $Y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Model\n",
    "\n",
    "## What does \"model\" mean?\n",
    "\n",
    "Imagine having a complete and perfect simulation of the world. This would allow you to take whatever action you wanted and see the outcome. This would mean that it would allow you to simulate different trajectories of events. That is, it would allow us to model it. Using your experience of what trajectories led to what outcomes, you'd be able to make more and more accurate predictions. \n",
    "On a smaller scale than the whole world/universe, we can build models of specific things we care about, so that we can again query them to make predictions from.\n",
    "\n",
    "## Our model\n",
    "\n",
    "In this dataset, we can see something like a straight-line relationship between the acidity and the quality.\n",
    "This indicates that the relationship may be well modelled by a linear equation, which is generally given in the form $y=wx + b$.\n",
    "\n",
    "Linear regression uses such straight line equations to model the input-output relationship of the data.\n",
    "\n",
    "In real problems of interest, this is rarely the case and we are likely to experience much more complex, nonlinear relationships between features and labels, as you can imagine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearHypothesis:\n",
    "    def __init__(self): # initalize parameters \n",
    "        self.w = np.random.randn() ## randomly initialise weight\n",
    "        self.b = np.random.randn() ## randomly initialise bias\n",
    "        \n",
    "    def __call__(self, X): # how do we calculate output from an input in our model?\n",
    "        ypred = self.w * X + self.b ## make a prediction using a linear hypothesis\n",
    "        return ypred # return prediction\n",
    "    \n",
    "    def update_params(self, new_w, new_b):\n",
    "        self.w = new_w ## set this instance's weights to the new weight value passed to the function\n",
    "        self.b = new_b ## do the same for the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "H = LinearHypothesis() # instantiate our linear model\n",
    "y_hat = H(X) # make prediction\n",
    "print('Input:',X, '\\n')\n",
    "print('W:', H.w, 'B:', H.b, '\\n')\n",
    "print('Prediction:', y_hat, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_h_vs_y(X, y_hat, Y):\n",
    "    plt.figure()\n",
    "    plt.scatter(X, Y, c='r', label='Label')\n",
    "    plt.scatter(X, y_hat, c='b', label='Hypothesis', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_h_vs_y(X, y_hat, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our model sucks to begin with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The criterion - How do we know how good our model is?\n",
    "\n",
    "Our **criterion** should be a measure of how bad our model is. We will use it to compare different models. As the model gets worse the loss function should return larger values.\n",
    "\n",
    "Criterions need to return a **single number**, not a vector, not a matrix, or anything else.\n",
    "This is because the whole point of it is to have a measure of how bad our model is.\n",
    "\n",
    "This criterion is the thing which we will try to minimise.\n",
    "Note that minimising the objective is equivalent to maximising the negative objective. \n",
    "\n",
    "<strong>Common synonyms</strong>\n",
    "- Loss funtion = cost function = criterion = error function\n",
    "\n",
    "### Mean squared error (MSE) loss\n",
    "\n",
    "**One way** to evaluate the performance of a model that predicts continuous (not discrete or bounded) outputs is to use the mean squared error loss. This does exactly what you think: it calculates the error (difference between our model's prediction and the true label) and then squares it and takes the mean of those square errors for each example. Squaring any value makes it positive, so as long as the error is not zero it will increase the value of the loss - regardless of whether our prediction is below (negative error) or above (positive error) the value of the label, the values of that **squared** difference will increase the returned loss.\n",
    "\n",
    "There are many other criterions that are useful for different tasks (e.g. the binary cross entropy (BCE) loss for classification, which we will cover later).\n",
    "\n",
    "Let's write a function to calculate the cost using the mean squared error loss function. It should take in an array of predictions for different example inputs as well as an array of corresponding example labels. It should return a single number (scalar) that represents the MSE loss. \n",
    "\n",
    "![title](images/NN1_cost_function.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(y_hat, labels): # define our criterion (loss function)\n",
    "    errors = y_hat - labels ## calculate errors\n",
    "    squared_errors = errors ** 2 ## square errors\n",
    "    mean_squared_error = sum(squared_errors) / len(squared_errors) ## calculate mean \n",
    "    return mean_squared_error # return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cost = L(y_hat, Y)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The analytical solution to minimising mean square error\n",
    "\n",
    "Given the data, model and criterion that we have used above, we can now express the objective that we wish to minimise mathematically. \n",
    "\n",
    "![](images/mse_vector.jpg)\n",
    "\n",
    "Note: this equation does not explicitly mention the bias. How can it be applied to our problem in the same form? What would we need to change about our input example vectors and weight matrix? \n",
    "\n",
    "We can now simply find the minimum point on this curve by finding an expression for it's derivative and setting this to zero.\n",
    "\n",
    "![](images/analytical_linear_reg.jpg)\n",
    "\n",
    "Now let's implement this analytical solution for least squares regression in code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "X_ = np.hstack((np.ones((X.shape[0], 1)),X)) # what's happening here? hint: where did the bias go?\n",
    "print(X_.shape)\n",
    "print(X_.round(2))\n",
    "\n",
    "optimal_w = np.matmul(np.linalg.inv(np.matmul(X_.T, X_)), np.matmul(X_.T, Y)) ## evaluate the analytical solution\n",
    "\n",
    "print(optimal_w)\n",
    "print(optimal_w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you didn't notice, this analytical solution has no mention of the model bias. \n",
    "In fact, we incorporate the model bias into our weights matrix by adding an extra element in a new column.\n",
    "This bias is then multiplied my a corresponding new features in each example, which has the value 1 for each example.\n",
    "\n",
    "# bias in weight matrix diagram\n",
    "\n",
    "Doing this makes the analytical solution much clearer and means we have to solve it only for one value $W$, rather than also for $b$.\n",
    "\n",
    "In practice, we treat them as separate variables.\n",
    "Our computer doesn't care for mathematical conveniences\n",
    "\n",
    "### Drawbacks of computing the analytical solution\n",
    "\n",
    "This solution involves inverting a matrix of size $R^{n \\times n}$. \n",
    "Here $n$ is the number of features that each example has. In our example $n=1$, so computing the analytical solution is feasible. \n",
    "However, as we will soon see, most problems of practical interest contain examples with many more features. \n",
    "For example, 1080p images have more than 1,000,000 features each. \n",
    "The time complexity of inverting a matrix of size $n \\times n$ is around $O(n^3)$. \n",
    "This means that computing the analytical solution for these kinds of real world problems is often computationally expensive, to the extent that it can become computationally infeasible.\n",
    "\n",
    "Analytical solutions however, are not the only approach that we can take. \n",
    "We can alternatively use one of many numerical optimisation techniques.\n",
    "These numerical optimisation techniques can be applied where analytical techniques are not feasible, and as such will be used throughout the course from here onward.\n",
    "\n",
    "## 4. The optimiser\n",
    "\n",
    "The optimiser adjusts the model such that it's performance improves with respect to the criterion. Most machine learning models are **parametric**, which means that the function which they represent depends on their parameters (in our case the weight (slope) and bias (intercept)). Different optimisers improve our models using different algorithms.\n",
    "\n",
    "In this notebook we will implement a trivial optimisation technique called **random search**.\n",
    "\n",
    "### Random Search\n",
    "Random seach is the process of randomly choosing values within a specified range and testing them to evaluate how good they are. E.g. test random values between 0 and 10.\n",
    "\n",
    "![](images/NN1_randomsearch.JPG)\n",
    "\n",
    "Let's implement a function that tries a bunch of possible values for the weight and bias of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(n_samples, limit=20):\n",
    "    \"\"\"Try out n_samples of random parameter pairs and return the best ones\"\"\"\n",
    "    best_weights = None ## no best weight found yet\n",
    "    best_bias = None ## no best bias found yet\n",
    "    lowest_cost = float('inf') ## initialize it very high (how high can it be?)\n",
    "    for i in range(0, n_samples): ## try this many different parameterisations\n",
    "        w = np.random.uniform(-limit, limit) ## randomly sample a weight within the limits of the search\n",
    "        b = np.random.uniform(-limit, limit) ## randomly sample a bias within the limits of the search\n",
    "        # print(w, b)\n",
    "        H.update_params(w, b) ## update our model with these random parameters\n",
    "        y_hat = H(X) ## make prediction\n",
    "        cost = L(y_hat, Y) ## calculate loss\n",
    "        if cost < lowest_cost: ## if this is the best parameterisation so far\n",
    "            lowest_cost = cost ## update the lowest running cost to the cost for this parameterisation\n",
    "            best_weights = w ## get best weights so far from the model\n",
    "            best_bias = b ## get best bias so far from the model\n",
    "    print('Lowest cost of', lowest_cost, 'achieved with weight of', best_weights, 'and bias of', best_bias)\n",
    "    return best_weights, best_bias ## return the best weight and best bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_weights, best_bias = random_search(10000) # do 10000 samples in a random search \n",
    "H.update_params(best_weights, best_bias) # make sure to set our model's weights to the best values we found\n",
    "plot_h_vs_y(X, H(X), Y) # plot model predictions agains labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened?\n",
    "\n",
    "Our random search optimisation was able to fit the input-output relationship of our data! Or at least it got close. \n",
    "\n",
    "#### What would happen if our true parameter values were outside of the range [-20, 20]? \n",
    "\n",
    "The model performance wouldn't improve much!\n",
    "\n",
    "This is because of the limits of the values of the parameters that we perform the grid search over. In this case, by default we are only trying parameters in the range from -20 to 20. But if the true bias were to be, say 30, which is outside of this range - then the model would never sample a value close to this. \n",
    "\n",
    "So here we've assumed the range of values that our optimal parameterisation might be included in. Feel free to change this limit in the function definition to see the how the model performance changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the limitations of random search?\n",
    "\n",
    "#### Non-heuristic search\n",
    "Random search keeps searching even if it finds the optimal value (they can't be sure it's the best parameterisation in the domain that they're checking until they've compared it to every other test parameterisation.\n",
    "It has no way of figuring out whether the next parameters that it is going to test are in the right direction towards their optimal values.\n",
    "That is, the search does not follow a heuristic - an indication of how to do better.\n",
    "\n",
    "#### The Curse of Dimensionality\n",
    "Aside from the issues showcased above, the major limitation of random search is how it scales with the number of parameters in our model. \n",
    "\n",
    "To model more complex functions we'll need more complex models - models with more parameters. \n",
    "\n",
    "But the time taken for random search scales **exponentially** with the number of parameters. This is because it has to search the whole parameter space, which has as many dimensions as the number of parameters.\n",
    "\n",
    "Imagine our parameters can only take the integer values $0$ or $1$.\n",
    "\n",
    "If we have one parameter, the entire parameter space is ${0, 1}$.\n",
    "That is, we have to check the criterion of $2$ possible parameterisations.\n",
    "\n",
    "If we have one parameter, the entire parameter space is ${(0, 0), (0, 1), (1, 0), (1, 1)}$.\n",
    "That is, we have to check the criterion of $4 \\ (=2^2)$ possible parameterisations.\n",
    "\n",
    "If we have one parameter, the entire parameter space is ${(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1)}$.\n",
    "That is, we have to check the criterion of $8 \\ (2^3)$ possible parameterisations.\n",
    "\n",
    "As you should be able to tell from the patter, the number of parameterisations that are possible is given by $n^d$.\n",
    "This is in the case where parameters can take only integer values, which massively simplifies the problem, and is optimistic because most models parameters can take continuous values.\n",
    "\n",
    "This is a manifestation of something known as the **curse of dimensionality**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression_model = LinearRegression()\n",
    "linear_regression_model.fit(X, Y)\n",
    "y_hat = linear_regression_model.predict(X)\n",
    "plot_h_vs_y(X, y_hat, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- the very basic recipe for making machine learning algorithms consists of:\n",
    "    1. The data - our examples of inputs and outputs (in the supervised case) which determine the function that our model will learn to represent and hence the problem that we are solving\n",
    "    2. The model - our mathematical function that we pass our data forward through to make a prediction for the output\n",
    "    3. The criterion - how we measure how bad our model is.\n",
    "    4. The optimiser - our method for updating the parameters of our models.\n",
    "- random search is a trivial optimisation strategy and will struggle to search over high dimensional spaces\n",
    "- the MSE loss is appropriate for this regression problem\n",
    "\n",
    "## Next steps\n",
    "- [Gradient based optimisation]() - in this notebook we will look at optimisation techniques that do scale to \n",
    "more complex models and problems. Can we use gradient based optimisation to find the best parameters for our model and establish whether this wine is drinkable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "1. Implement grid Search\n",
    "\n",
    "Grid search is the process of trying out values at common intervals within a specified range for each parameter, and testing them to evaluate how good they are. E.g. test the values [0, 1, 2, 3, 4, 5]. It is the same as random search, other than the fact that the test coordinates are chosen systematically rather than randomly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
