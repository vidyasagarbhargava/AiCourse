{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation graph\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Up to this point we used ready made algorithms from `sklearn` except for linear regression with analytical solution. When we called `fit` the algorithm was trained on our data.\n",
    "\n",
    "All machine learning algorithms are essentially functions taking some input and producing some output and could be written like this:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = f(X)\n",
    "$$\n",
    "\n",
    "where `X` is data and $\\hat{Y}$ is prediction. Those functions depend on `parameters` or `weights` (denoted by $\\theta$) so our mathematical description changes to:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = f_{\\theta}(X)\n",
    "$$\n",
    "\n",
    "Also there is `loss` which measures the error (always positive scalar value) compared to true values (`Y`). The less, the better as we saw previously. In maths:\n",
    "\n",
    "$$\n",
    "L(f_{\\theta}(X), Y)\n",
    "$$\n",
    "\n",
    "Some of those functions (including neural networks) are differentiable, hence we can __calculate partial derivatives__.\n",
    "\n",
    "Automatic differentiation graph allows us to compute `gradient` of `loss` ($\\nabla L$) with respect to (abbreviated often as w.r.t.) parameters $\\theta$ using chain rule, which one can write like this:\n",
    "\n",
    "$$\n",
    "\\nabla L_{\\theta}\n",
    "$$\n",
    "\n",
    "In this section we will only focus on `backpropagation` itself, loss and how it fits within Machine Learning domain will be shown in the next lesson.\n",
    "\n",
    "## Intuition\n",
    "\n",
    "Intuitively, we can think of parameters gradient as:\n",
    "\n",
    "> how and by how much we should change their value in order to improve loss (decrease it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is graph?\n",
    "\n",
    "> Graph is a data structure containing __nodes__ and __edges__ connecting one node to another\n",
    "\n",
    "Example image of graph will tell you everything you need to know:\n",
    "\n",
    "![](images/graph_data_structure.png)\n",
    "\n",
    "We can see you can travel across the nodes by moving along the edges, here are some questions:\n",
    "- What is the path to get to get from A to E?\n",
    "- Which path has the __largest__ and __smallest__ cost for A to E traversal?\n",
    "- What are the __cycles__ inside this graph?\n",
    "- What are the nodes which one __cannot reach__ from A and which from E?\n",
    "\n",
    "## Undirected vs directed graph\n",
    "\n",
    "Graphs can either __directed__ or __undirected__:\n",
    "\n",
    "### Undirected graph\n",
    "\n",
    "> Undirected graph can be traversed along edges __in any direction__\n",
    "\n",
    "![](images/undirected_graph.png)\n",
    "\n",
    "### Directed graph\n",
    "\n",
    "> Directed graph can be traversed along edges in a __specified direction__\n",
    "\n",
    "We have seen an example at the very top\n",
    "\n",
    "### Directed acyclic graph\n",
    "\n",
    "> Directed acyclic graph (DAG) can be traversed in a __specified direction__ and __has no cycles__\n",
    "\n",
    "![](images/dag.png)\n",
    "\n",
    "__This data structure is especially important for us!__\n",
    "\n",
    "It will be used as a structure which will keep our automatic differentiation graph and is used across machine learning all the time (especially in deep learning and neural networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph and chain rule\n",
    "\n",
    "Let's write an example chain of functions:\n",
    "\n",
    "$$\n",
    "a = b(c(d(e(X))))\n",
    "$$\n",
    "\n",
    "Now, we would like to know how change in `e(X)` influences `a` value. To do that, we have to calculate gradient of `a` w.r.t. to `e`. Using chain rule this would be (each derivative is evaluated at certain point starting with `X`, left it out for brevity):\n",
    "\n",
    "$$\n",
    "\\frac{da}{de} = \\frac{da}{db} \\frac{db}{dc} \\frac{dc}{dd} \\frac{dd}{de}\n",
    "$$\n",
    "\n",
    "> backpropagation is an algorithm which given output (`a` in this case) runs operations (__their derivative formulas__) backward in order to calculate gradient of inputs\n",
    "\n",
    "`backprop` in computer programming can be described as graph, conceptually:\n",
    "- when the value is calculated (forward pass) graph records each __operation__ on data\n",
    "- when special function is run on the graph (we will name it `backward`), __derivatives__ of recorded operations are run in opposite order\n",
    "- when we get to the __parameter__ we update it's gradient (in example above `e` would be a parameter)\n",
    "\n",
    "__Let's see in visual form how the graph looks like:__\n",
    "\n",
    "![](images/comp_graph.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic vs static graph\n",
    "\n",
    "There are out-of-the box frameworks which provide highly optimized graph implementations.\n",
    "Two major approaches exist:\n",
    "\n",
    "- static graph\n",
    "- dynamic graph\n",
    "\n",
    "They are mostly used in __neural networks__ (which we are gonna see in a separate module)\n",
    "\n",
    "### Static graph\n",
    "\n",
    "> graph is defined only once and cannot be updated afterwards\n",
    "\n",
    "Best known representant of this approach in industry is [Tensorflow](https://www.tensorflow.org/), especially with it's `1.x` version. Currently running out of favours among developers (and researchers especially) and is a good fit for specific usage (very large neural networks).\n",
    "\n",
    "#### Upsides\n",
    "\n",
    "- Easier to distribute graph among multiple machines. Structure is known and can be shared more easily\n",
    "- Space for optimization (though it's impact is highly debatable)\n",
    "- Hence __might__ run faster\n",
    "\n",
    "#### Downsides\n",
    "\n",
    "- Does not feel Pythonic and is hard to use\n",
    "- Cannot be changed based on any condition, only based on part of the framework\n",
    "- Cumbersome syntax and usage; `if`, `for` and other constructs from Python do not exist\n",
    "- Hard to debug\n",
    "\n",
    "### Dynamic graph\n",
    "\n",
    "> graph is defined \"on the fly\" and nodes are added as they are run\n",
    "\n",
    "Best known representant of this approach is [PyTorch](https://pytorch.org/), while [Tensorflow](https://www.tensorflow.org/) tries to go the same path since `2.x` version.\n",
    "Largely favored among researchers with weaker industry adoption (though that changes rapidly).\n",
    "\n",
    "#### Upsides\n",
    "\n",
    "- Easy to use. If implemented well can feel just like writing in Python and `numpy`\n",
    "- Easier to debug\n",
    "- Can be changed however we wish based on any condition\n",
    "\n",
    "#### Downsides\n",
    "\n",
    "- Graph is harder to distribute as it might change any time. Sharing it might be costly\n",
    "- Might be harder to optimize (though optimized versions exist and usually isn't a problem)\n",
    "- __Might__ run slower\n",
    "\n",
    "As the last one is getting more and more traction we will make our own simplistic version of dynamic graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph implementation\n",
    "\n",
    "Let's start with `graph` and how it works. Conceptually, it will need the following:\n",
    "- Keeping `parameters` and `operations` references inside\n",
    "- Functions to register `parameters` and `operations` inside graph\n",
    "- `backward` function to populate parameters with gradient\n",
    "\n",
    "Below is a `class` which does it. Also as it is a dynamic graph `operations` have to be cleared after each `backward` call so they can be redefined \"on-the-fly\".\n",
    "\n",
    "__Don't worry, we will walk you through it step by step!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "\n",
    "\n",
    "class Graph:\n",
    "    \"\"\"Graph class used for backward automatic differentiation (backpropagation).\n",
    "\n",
    "\n",
    "    Can only differentiate w.r.t. scalar values. `backpropagate` function\n",
    "    should be called on final parameter (after all operations\n",
    "    were performed).\n",
    "\n",
    "    Attributes:\n",
    "        operations (Dict[int, (Operation, Dict[int, (int, bool)])]):\n",
    "            List of operations which, when backpropagated produce gradients\n",
    "            for Parameters. Each item is a Tuple containing:\n",
    "            - Instance of operation\n",
    "            - Dictionary containing:\n",
    "                - index of input parameter (so usually it is [0, 1, 2, 3...])\n",
    "                - Tuple containing:\n",
    "                    - index of operation which created this input parameter\n",
    "                    - True/False value whether this node is a leaf\n",
    "\n",
    "            If node is a leaf it has to be parameter and backpropagation stops\n",
    "            at this call to `backward` (see `Parameter` class)\n",
    "\n",
    "        parameters (List[Parameter])\n",
    "            List of parameters added to this graph.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.operations = {}\n",
    "        self.parameters = []\n",
    "\n",
    "    def _register_parameter(self, parameter: \"Parameter\"):\n",
    "        \"\"\"Registers parameter inside the graph\n",
    "\n",
    "        Arguments:\n",
    "            Instance of Parameter to be registered\n",
    "\n",
    "        Returns:\n",
    "            Index of parameter inside the graph which is saved in parameter's instance.\n",
    "\n",
    "        \"\"\"\n",
    "        self.parameters.append(parameter)\n",
    "        return len(self.parameters) - 1\n",
    "\n",
    "    def _register_operation(self, operation: \"Operation\", inputs):\n",
    "        \"\"\"Registers operation inside the graph\n",
    "\n",
    "        Returns:\n",
    "            Index of operation inside the graph which is saved in parameter's instance.\n",
    "\n",
    "        \"\"\"\n",
    "        if has_grad():\n",
    "            last_index = len(list(self.operations.keys()))\n",
    "            self.operations[last_index] = (operation, inputs)\n",
    "            return last_index\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_gradient(upstream_gradient, output_index):\n",
    "        \"\"\"If gradient is a Tuple return element otherwise return upstream_gradient\n",
    "        as is.\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(upstream_gradient, (tuple, list)):\n",
    "            return upstream_gradient[output_index]\n",
    "        return upstream_gradient\n",
    "\n",
    "    def _backpropagate_node(\n",
    "        self, upstream_gradient, output_index, operation_index, is_leaf\n",
    "    ) -> None:\n",
    "        \"\"\"Backpropagate through single node (Operation or Parameter).\n",
    "\n",
    "        If Parameter (is_leaf=True) is reached backpropagation will end with\n",
    "        populating it's gradient.\n",
    "\n",
    "        If Operation (is_leaf=False) is reached the node will be run through\n",
    "        graph backpropagation again.\n",
    "\n",
    "        \"\"\"\n",
    "        gradient = Graph._get_gradient(upstream_gradient, output_index)\n",
    "        if is_leaf:\n",
    "            self.parameters[operation_index].backward(gradient)\n",
    "        else:\n",
    "            # If we went through this operation we should raise an error\n",
    "            new_operation = self.operations.pop(operation_index, None)\n",
    "            if new_operation is None:\n",
    "                raise ValueError(\n",
    "                    \"Trying to backpropagate through non-existent node. Are your paths disjoint?\"\n",
    "                )\n",
    "            self._backpropagate_graph(new_operation, gradient)\n",
    "\n",
    "    def _backpropagate_graph(self, operation_and_mapping, upstream_gradient):\n",
    "        \"\"\"Backpropagate through graph of operations.\n",
    "\n",
    "        For any incoming operation it will go over their inputs\n",
    "        (defined by mapping which points to input nodes) and propagate\n",
    "        current upstream gradient to them.\n",
    "\n",
    "        After calculation of gradient it's internal cached is clear by the graph\n",
    "\n",
    "        \"\"\"\n",
    "        operation, mapping = operation_and_mapping\n",
    "        upstream_gradient = operation.backward(upstream_gradient)\n",
    "        # Clean cache\n",
    "        operation.cache = None\n",
    "        # Multiple outputs\n",
    "        for output_index, (operation_index, is_leaf) in mapping.items():\n",
    "            self._backpropagate_node(\n",
    "                upstream_gradient, output_index, operation_index, is_leaf\n",
    "            )\n",
    "\n",
    "    def backward(self, upstream_gradient=1) -> None:\n",
    "        \"\"\"Entrypoint for backpropagation through registered nodes.\n",
    "\n",
    "        `backward` will run through all the nodes contained inside graph in succession,\n",
    "        starting with the one added as the last one.\n",
    "\n",
    "        If there are multiple __separable__ paths those will be backpropagated\n",
    "        as well. If they aren't separable an error will be raised.\n",
    "\n",
    "        When graph's `backward` is called it will be cleaned from\n",
    "        all operations (parameters stay inside graph until the graph instance\n",
    "        is available, usually throughout the whole program).\n",
    "\n",
    "        \"\"\"\n",
    "        if not has_grad():\n",
    "            raise ValueError(\"Cannot perform backward as tape recording is off.\")\n",
    "\n",
    "        while self.operations:\n",
    "            last_index = list(self.operations.keys())[-1]\n",
    "            self._backpropagate_graph(\n",
    "                self.operations.pop(last_index), upstream_gradient\n",
    "            )\n",
    "        for parameter in self.parameters:\n",
    "            parameter.last_operation_index = None\n",
    "\n",
    "\n",
    "class _GlobalGraph:\n",
    "    \"Class used to hide global state from the main namespace.\"\n",
    "    # even though this stuff is inside this class definition, it gets run!\n",
    "    graph = Graph() # accessed as _GlobalGraph.graph\n",
    "    on: bool = True # accessed as _GlobalGraph.on\n",
    "    print('Created graph')\n",
    "\n",
    "\n",
    "def get():\n",
    "    \"\"\"Return global graph\"\"\"\n",
    "    return _GlobalGraph.graph\n",
    "\n",
    "\n",
    "def has_grad():\n",
    "    return _GlobalGraph.on\n",
    "\n",
    "\n",
    "@contextlib.contextmanager # the following function can be used in \"with no_grad() as x:\"\n",
    "def no_grad():\n",
    "    \"\"\"\n",
    "    Creates context for perfoming graph operations without doing everyhing required for backpropagation (like storing gradients)\n",
    "\n",
    "    Usage: \n",
    "        with no_grad():\n",
    "            # at this point everthing before the yield statement happens\n",
    "\n",
    "            # do things without storing grads etc\n",
    "            # e.g. make predictions in production\n",
    "            # makes things happen as fast as possible\n",
    "            # makes things that not take up extra memory\n",
    "\n",
    "        # here (once i've exited the context i.e. unindented again) the stuff after the yield statement happens\n",
    "\n",
    "    \"\"\"\n",
    "    _GlobalGraph.on = False\n",
    "    yield\n",
    "    _GlobalGraph.on = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please notice that __graph is global__. It is simpler that way as `parameters` are registered only once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "__Why initial gradient is 1?__\n",
    "\n",
    "### Solution\n",
    "\n",
    "Simply, we take derivative of the last value w.r.t to itself. Let's call this value `x`:\n",
    "\n",
    "$$\n",
    "\\frac{dx}{dx} = 1\n",
    "$$\n",
    "\n",
    "And as we know derivative of `x` is simply `1`, hence that is the value we feed the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation\n",
    "\n",
    "Next let's create `Operation`. Let's recap what is needed:\n",
    "- take data value and calculate mathematical operation on it\n",
    "- register itself in graph so it can be run during `backward`\n",
    "- implement `backward`\n",
    "\n",
    "This is exactly what `base classes` are for. Later we can implement specific math operation by deriving from this class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class Operation(abc.ABC):\n",
    "    \"\"\"Base class of mathematical operation to be run on Parameter/np.array\n",
    "\n",
    "    Attributes:\n",
    "        cache (Optional[np.array])\n",
    "            Cache attribute one can use to save anything during forward pass\n",
    "            to reuse in backward\n",
    "        index_in_graph (int):\n",
    "            Index of operation in graph's operation dictionary\n",
    "        is_leaf (bool):\n",
    "            Always `False`, used by graph to easily discern between parameters\n",
    "            and operations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "        self.index_in_graph = None\n",
    "        self.is_leaf = False\n",
    "\n",
    "    def __call__(self, *arguments):\n",
    "        \"\"\"Run forward and register operation in graph.\n",
    "\n",
    "        Additionally operation's inputs will be registered using mapping and\n",
    "        whether those are leafs (parameters) or operations to be further\n",
    "        backpropagated.\n",
    "\n",
    "        \"\"\"\n",
    "        if has_grad():\n",
    "            mapping = {}\n",
    "            add_to_graph = False\n",
    "            for input_index, argument in enumerate(arguments):\n",
    "                if isinstance(argument, Parameter):\n",
    "                    add_to_graph = True\n",
    "                    is_first_operation = argument.last_operation_index is None\n",
    "                    if is_first_operation:\n",
    "                        mapping[input_index] = (argument.index_in_graph, True)\n",
    "                    else:\n",
    "                        mapping[input_index] = (argument.last_operation_index, False)\n",
    "\n",
    "            if add_to_graph:\n",
    "                self.index_in_graph = get()._register_operation(self, mapping)\n",
    "                for argument in arguments:\n",
    "                    if isinstance(argument, Parameter):\n",
    "                        argument.last_operation_index = self.index_in_graph\n",
    "\n",
    "        # Pack return value in tuple always\n",
    "        return self.forward(*arguments)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def forward(self, *_):\n",
    "        \"\"\"Define your forward pass here.\n",
    "\n",
    "        Use self.cache to cache anything needed during backpropagation.\n",
    "\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def backward(self, upstream_gradient):\n",
    "        \"\"\"Define your backward pass here.\n",
    "\n",
    "        Use self.cache in order to calculate gradient. There has to be as\n",
    "        many outputs as there was inputs to forward.\n",
    "\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter\n",
    "\n",
    "Last class we need is `Parameter`. As we know `np.array` and how it works we are going to base our implementation by inheriting from this class.\n",
    "\n",
    "`numpy.ndarray` requires special approach when inheriting from it. `__new__` instead of `__init__` and `__array_finalize__` special method. For those curious [you can read more about it here](https://numpy.org/doc/stable/user/basics.subclassing.html).\n",
    "\n",
    "What we need for our `Parameter`?\n",
    "- `gradient` attribute which keeps the gradient (or `None` if `backward` wasn't called)\n",
    "- when `backward` is called, `self.gradient` should be populated\n",
    "- `gradient` should be cleared when it isn't needed anymore. `clear` method will simply assign `None` to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# https://numpy.org/doc/stable/user/basics.subclassing.html\n",
    "class Parameter(np.ndarray):\n",
    "    \"\"\"Parameter class to be populated with gradient.\n",
    "\n",
    "    Attributes:\n",
    "        gradient (Optional[np.array]):\n",
    "            Array with gradients with which parameter can be optimized via\n",
    "            optimizer\n",
    "        index_in_graph (int):\n",
    "            Index of parameter in graph's list\n",
    "        is_leaf (bool):\n",
    "            Always True, used by graph to easily discern between parameters\n",
    "            and operations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __new__(cls, input_array):\n",
    "        # Input array is an already formed ndarray instance\n",
    "        # We first cast to be our class type\n",
    "        obj = np.asarray(input_array).view(cls)\n",
    "        # Gradient is None until populated\n",
    "        obj.gradient = None\n",
    "        obj.index_in_graph = get()._register_parameter(obj)\n",
    "        obj.is_leaf = True\n",
    "        obj.last_operation_index = None\n",
    "        # Return newly created object\n",
    "        return obj\n",
    "\n",
    "    # Don't sweat over it, just assigning the same attributes as in new\n",
    "    def __array_finalize__(self, obj):\n",
    "        \"\"\"Re-assign data contained in parameter.\n",
    "\n",
    "        Workaround for `numpy` subclassing.\n",
    "\n",
    "        \"\"\"\n",
    "        if obj is None:\n",
    "            return\n",
    "        self.gradient = getattr(obj, \"gradient\", None)\n",
    "        self.index_in_graph = getattr(obj, \"index_in_graph\", None)\n",
    "        self.is_leaf = getattr(obj, \"is_leaf\", True)\n",
    "        self.last_operation_index = getattr(obj, \"last_operation_index\", None)\n",
    "\n",
    "    def broadcast_fix(self, gradient):\n",
    "        \"\"\"Try to fix numpy's broadcasting with gradient.\n",
    "\n",
    "        `1` dimensions may be broadcasted to other automatically. There is no\n",
    "        clear way to know about that which could be easily implemented.\n",
    "\n",
    "        Broadcasting is equal to summing all the values, hence __any__ dimension\n",
    "        which might be off in gradient is summed by the function below.\n",
    "\n",
    "        \"\"\"\n",
    "        if not isinstance(gradient, np.ndarray):\n",
    "            return gradient\n",
    "\n",
    "        if gradient.flatten().shape == self.flatten().shape:\n",
    "            return gradient\n",
    "        flattened_gradient = gradient.flatten()\n",
    "        flattened_data = self.flatten()\n",
    "        if len(flattened_gradient.shape) < len(flattened_data.shape):\n",
    "            raise ValueError(\n",
    "                \"Data has more dimension than gradient, something went very wrong.\"\n",
    "            )\n",
    "\n",
    "        to_sum = []\n",
    "        # Gradient cannot be None as the condition is checked above\n",
    "        for index, (data_shape, gradient_shape) in enumerate(\n",
    "            itertools.zip_longest(flattened_data.shape, flattened_gradient.shape)\n",
    "        ):\n",
    "            if data_shape is None or gradient_shape > data_shape:\n",
    "                to_sum.append(index)\n",
    "            if data_shape > gradient_shape:\n",
    "                raise ValueError(\"Data has more elements than it's gradient\")\n",
    "        return np.sum(flattened_gradient, axis=tuple(to_sum)).flatten()\n",
    "\n",
    "    def backward(self, upstream_gradient) -> None:\n",
    "        \"\"\"Take upstream gradient and update param's gradient with it.\"\"\"\n",
    "        self.gradient = self.broadcast_fix(upstream_gradient)\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear gradient to save RAM memory.\"\"\"\n",
    "        self.gradient = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement operations\n",
    "\n",
    "Now that we have defined our classes we can implement concrete `operations`. Let's start easy with `addition` to see what this is all about.\n",
    "\n",
    "## Addition\n",
    "\n",
    "So all we have to implement, as seen previously, is `forward` and `backward`. We will also wrap the object in `add` function to use it easier afterwards.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Formula for addition is known to everyone:\n",
    "\n",
    "$$\n",
    "c = a + b\n",
    "$$\n",
    "\n",
    "`c` is influenced by `a` and `b`, hence we have to calculate gradient w.r.t. to both of them:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial c}{da}(a + b) = 1 \\\\\n",
    "    \\frac{\\partial c}{db}(a + b) = 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So now we have all we need, let's put this in code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concrete implementations\n",
    "class _Add(Operation):\n",
    "    # Simply add two values\n",
    "    def forward(self, a, b):\n",
    "        return a + b\n",
    "\n",
    "    # Return gradient for a and b (remember)\n",
    "    def backward(self, upstream_gradient):\n",
    "        return upstream_gradient, upstream_gradient\n",
    "\n",
    "# Create object and run it with (a, b)\n",
    "def add(a, b):\n",
    "    return _Add()(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean\n",
    "\n",
    "We will also implement second operation (for now, in the whole chapter we will implement a few more).\n",
    "\n",
    "## Exercise\n",
    "\n",
    "### Part 1\n",
    "\n",
    "Formula for mean (denoted `m` here) of `N` variables is:\n",
    "\n",
    "$$\n",
    "m = \\frac{1}{N}\\sum_{i=1}^{N}x_i\n",
    "$$\n",
    "\n",
    "As we have `N` variables we will have to return `N` values. Fortunately, during implementation, as we take `mean` of `np.array` we will return `np.array` of the same shape.\n",
    "\n",
    "Derivative this time will be a little harder, so let's so let's start small and calculate derivative of `1` variable:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial m}{\\partial x_1}(\\frac{1}{1} * x_1) = 1\n",
    "$$\n",
    "\n",
    "Try the same with two variables, maybe `3`. What would the result be for `N` variables?\n",
    "\n",
    "### Part 1 solution\n",
    "\n",
    "Let's try it for two variables:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial m}{\\partial x_1}(\\frac{1}{2} * (x_1 + x_2)) = \\frac{1}{2} \\\\\n",
    "    \\frac{\\partial m}{\\partial x_2}(\\frac{1}{2} * (x_1 + x_2)) = \\frac{1}{2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Finally for three:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial m}{\\partial x_1}(\\frac{1}{3} * (x_1 + x_2 + x_3)) = \\frac{1}{3} \\\\\n",
    "    \\frac{\\partial m}{\\partial x_2}(\\frac{1}{3} * (x_1 + x_2 + x_3)) = \\frac{1}{3} \\\\\n",
    "    \\frac{\\partial m}{\\partial x_3}(\\frac{1}{3} * (x_1 + x_2 + x_3)) = \\frac{1}{3} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So for `N` variables each will have `1/N` derivative. \n",
    "\n",
    "### Part 2\n",
    "\n",
    "Implement `_Mean` operation and `mean` function in analogous way we did with `_Add` and `add`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Mean(Operation):\n",
    "    def __init__(self, axis: int = None):\n",
    "        super().__init__()\n",
    "        self.axis = axis\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        mean = np.mean(inputs, axis=self.axis)\n",
    "        self.cache = np.ones_like(inputs) / inputs.size\n",
    "        return mean\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        return upstream_gradient * self.cache\n",
    "\n",
    "def mean(inputs, axis: int = None):\n",
    "    return _Mean(axis)(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running graph\n",
    "\n",
    "As we have everything in place, let's use it. __Remember to use functions NOT CLASSES__, it should be pretty natural.\n",
    "\n",
    "Your task is to create two `Parameters` from `np.random.randn` arrays of shape `10, 5`, __add__ them together, take the __mean__, backpropagate and check gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "x1 = Parameter(np.random.randn(10, 5))\n",
    "x2 = Parameter(np.random.randn(10, 5))\n",
    "\n",
    "forward_result = mean(add(x1, x2))\n",
    "get().backward()\n",
    "\n",
    "x1.gradient, x2.gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "- Learn more about graphs. You can start from [wikipedia](https://en.wikipedia.org/wiki/Graph_(abstract_data_type)) and move on to [graph traversal](https://en.wikipedia.org/wiki/Graph_traversal)\n",
    "- Code your own simple graph and implement [BFS](https://en.wikipedia.org/wiki/Breadth-first_search) and [DFS](https://en.wikipedia.org/wiki/Depth-first_search) on it. What are the advantages/disadvantages of one method over the other?\n",
    "- Go through this lesson multiple times. This one is hard so make sure you understand what is going on, ask questions if needed\n",
    "- Read about [reverse (backward) vs forward automatic differentiation](https://math.stackexchange.com/questions/2195377/reverse-mode-differentiation-vs-forward-mode-differentiation-where-are-the-be). Which frameworks implement both (tip: check out [JAX](https://github.com/google/jax))?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}