{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "- Understand how classification can be implemented when there are more than 2 classes\n",
    "- Implement a multiclass classifier from scratch\n",
    "\n",
    "## Binary vs Multiclass\n",
    "\n",
    "In binary classification the output must be either `True` or `False` as we already know.\n",
    "\n",
    "Either the example falls into this class, or it doesn't. We have seen that we can represent this by our model having a single output node whose value is forced between 0 and 1, and as such represents probability that the example belongs to the positive class.\n",
    "\n",
    "## Multiclass\n",
    "\n",
    "![](./images/binary-class.jpg)\n",
    "\n",
    "In the case where we have two nodes to represent true and false, we can think about it as having trained two separate models.\n",
    "\n",
    "Treating `True` and `False` as separate classes with separate output nodes shows us how we can extend this idea to do multiclass classification; \n",
    "\n",
    "> we simply add more nodes and ensure that their values are positive and sum to one.\n",
    "\n",
    "__Each node is a single `logit` and all of them combined are later passed to `softmax`__\n",
    "\n",
    "![](./images/multiclass.jpg)\n",
    "\n",
    "## Multiclass vs Multilabel\n",
    "\n",
    "In this course we will not talk about __multilabel__ case, but:\n",
    "\n",
    "> In multilabel problem, each label can exist simultaneously instead of exclusively like in multiclass\n",
    "\n",
    "This might be a single vector where we have `cat` and `dog` on a picture but not a turtle:\n",
    "\n",
    "$$\n",
    "[1, 0, 1]\n",
    "$$\n",
    "\n",
    "> In multiclass, __there is always a single `1` label__, not less, not more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logits\n",
    "\n",
    "Here we will also outputs logits, in case of multiclass the only change is __it will be a vector of values__. Each value in the output vector corresponds to certain class.\n",
    "\n",
    "Let's say we would like to classify our input image into one of threes classes: `{dog=0, cat=1, turtle=2}`. Output of our model might look like this:\n",
    "\n",
    "$$\n",
    "    [-5, -3, 2]\n",
    "$$\n",
    "\n",
    "This would be a prediction of class `turtle` as it's value is highest.\n",
    "When we want to get a label from this operation we use [`argmax`](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html):\n",
    "\n",
    "> argmax returns __index__ of array entry with __the highest value__\n",
    "\n",
    "As before, we can transform them into probabilities using...\n",
    "\n",
    "## Softmax\n",
    "\n",
    " > The **softmax function** exponentiates each value in a vector to make it positive and then divides each of them by their sum to normalise them (make them sum to 1). \n",
    "\n",
    "This ensures that the vector then can be interpreted as a probability distribution.\n",
    "\n",
    "![](./images/softmax.jpg)\n",
    "\n",
    "> Real life example with real values\n",
    "\n",
    "![](./images/softmax_example.jpg)\n",
    "\n",
    "## Differentiating the softmax\n",
    "\n",
    "- Softmax derivative is different based on the index of element with respect to which we take derivative. \n",
    "- If it is the same as the index of element we applied softmax, the derivative is the equation at the bottom\n",
    "- Otherwise the one above it. \n",
    "\n",
    "![](images/softmax_deriv.jpg)\n",
    "\n",
    "### Properties of softmax\n",
    "\n",
    "- increasing the value of any entry decreases the value of all of the others, because the whole vector must always sum to one. \n",
    "- an increase in one input element increases it's corresponding output element exponentially whilst pushing others down, \n",
    "- this means that __it is very easy for the one largest output element to become dominant__\n",
    "- because of that `softmax` is overconfident and there are ways to combat this like `label smoothing`\n",
    "\n",
    "### What does the name \"softmax\" mean?\n",
    "\n",
    "- as explained above, usually one input is near `1`, and all others close to `0`. That is, similar to the `argmax` operation mentioned previously but \"soft\" as it can be differentiated.\n",
    "- `argmax` changes abruptly, small difference between two values make it either `0` or `1`. Softmax on the other hand changes gradually when the maximum changes\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Let's implement our own softmax function (as simple function, __not inheriting from `g.Operation`!__).\n",
    "\n",
    "It should take `x` and divide by `sum` across `axis=1` (as we are normalizing along features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable softmax\n",
    "\n",
    "As seen in `sigmoid` case this version also suffers from numerical instability, check out below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(np.array([[1000, 9, 8], [11, 12, 15]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the result is even worse as we get `np.nan` due to overflow. Solution to the problem is to subtract maximum value from each row.\n",
    "\n",
    "### Subtracting, what?\n",
    "\n",
    "As `softmax` works along the horizontal axis (`1`) and all values sum to `1` the only thing that matters with the numbers in certain row is their absolute distance. \n",
    "\n",
    "This means we can divide __any value__ from them and still get the same results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = np.array([5, -2, 0]).reshape(1, -1)\n",
    "subtracted = original - 6\n",
    "softmax(original), softmax(subtracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to subtract?\n",
    "\n",
    "There is no way to know what is the right `const` value to remove from each row. What if we have `1000` or `1_000_000`? \n",
    "\n",
    "Fortunately, we can find maximum in whole batch of data and simply subtract that.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Implement `softmax` function again, this time a stable version:\n",
    "- subtract `np.max` from `logits` across `1` axis again\n",
    "- return exponential values calculated this way like previously\n",
    "\n",
    "Here is our `stable softmax`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(np.array([[1000, 9, 8], [11, 12, 15]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax as operation\n",
    "\n",
    "> As we have seen previously (binary case) there is no need to backpropagate through activation.\n",
    "\n",
    "Also, same as previously, we can combine `loss` and activation into one class for improved numerical stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding\n",
    "\n",
    "Our targets can be encoded in multiple ways. Usually, we would simply pass class numbers like this (for `5` samples):\n",
    "\n",
    "$$\n",
    "[0, 3, 1, 1, 4]\n",
    "$$\n",
    "\n",
    "We could also do that using one-hot encoding:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&[1, 0, 0, 0, 0]\\\\\n",
    "&[0, 0, 0, 1, 0]\\\\\n",
    "&[0, 1, 0, 0, 0]\\\\\n",
    "&[0, 1, 0, 0, 0]\\\\\n",
    "&[0, 0, 0, 0, 1]\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "As most of the data works with the first option, we will now code how to transform `labels` into one-hot-encoding and vice versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels, max_labels: int = None):\n",
    "    if max_labels is None:\n",
    "        max_labels = np.max(labels) + 1\n",
    "    return np.eye(max_labels)[labels]\n",
    "\n",
    "\n",
    "def to_labels(one_hot):\n",
    "    return np.argmax(one_hot, axis=-1)\n",
    "\n",
    "\n",
    "data = np.array([0, 1, 0, 3, 5])\n",
    "to_one_hot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cross entropy loss function\n",
    "\n",
    "An appropriate loss function to use for multiclass classification is the __cross entropy loss function__.\n",
    "\n",
    "- It is a __generalization of binary cross entropy loss__ so it will work in binary case as well\n",
    "- BCE (binary cross-entropy) is faster and more stable for binary case so __it should be created separately__ and __used separately__.\n",
    "\n",
    "Like BCE loss, cross entropy uses the same term: __the negative natural log of the output probability__ to penalise outputs exponentially as they stray from the ground truth.\n",
    "\n",
    "> We don't need to simultaneously push down the incorrect class probabilities and push up the correct class probabilities.\n",
    "\n",
    "So if we focus on increasing the correct class likelihood element, then we will implicitly be decreasing the incorrect class likelihood elements.\n",
    "\n",
    "![](images/cross_entropy_loss.jpg)\n",
    "\n",
    "## Exercise\n",
    "\n",
    "__This time we will code `forward` call of `_CrossEntropyWithLogits` only! (backward left for those who really want to get into it)_\n",
    "\n",
    "- Apply softmax across `logits`\n",
    "- transform targets to `one_hot` version\n",
    "- `cache` both softmaxed value (as `cache[0]`) and `one_hot` (as `cache[1]`) in `tuple` or `list`\n",
    "- Return formula above (change from `np.mean` to `np.sum`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import aicore.ml.graph as g\n",
    "\n",
    "class _CrossEntropyWithLogits(g.Operation):\n",
    "    def forward(self, logits, targets):\n",
    "        ...\n",
    "        return ...\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        expanded_output = upstream_gradient.reshape(-1, 1)\n",
    "        return (\n",
    "            expanded_output\n",
    "            * (\n",
    "                self.cache[0] * np.expand_dims(np.sum(self.cache[1], axis=1), axis=1)\n",
    "                - self.cache[1]\n",
    "            ),\n",
    "            -expanded_output * np.log(self.cache[0]),\n",
    "        )\n",
    "\n",
    "\n",
    "def ce_with_logits(logits, targets):\n",
    "    return _CrossEntropyWithLogits()(logits, targets)\n",
    "\n",
    "ce_with_logits(np.random.randn(100, 10), np.random.randint(low=0, high=10, size=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about numerical stability?\n",
    "\n",
    "Softmax is unlikely to return `0` with `log` for any practical case. Workarounds exist, including but not limited to:\n",
    "- Add small `epsilon` constant (what we did)\n",
    "- Calculate loss **only** for non-zero elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass logistic regression\n",
    "\n",
    "Multiclass logistic regression, as we know, is essentially multiple linear regression joined by `cross entropy` loss function.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "__Create `MulticlassLogisticRegression`!__\n",
    "\n",
    "- Start by copying `BinaryLogisticRegression` from previous lessons\n",
    "- Change `__init__` function:\n",
    "    - Now it should take three arguments:\n",
    "        - `n_classes` - number of classes used in classification\n",
    "        - `n_features` - number of features, just like previously\n",
    "        - `optimizer` - just like previously\n",
    "    - `self.W` should have a shape `(n_features, n_classes)` now (notice we are essentially creating multiple linear regression this way!)\n",
    "- `predict_proba` should have `softmax` instead of `sigmoid`\n",
    "- `predict` should use the hard version of `softmax` across the same `axis` as `softmax`\n",
    "- `fit` should use `ce_with_logits` instead of `bce_with_logits`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassLogisticRegression:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "The scheme still applies, normalization is still important, we will just use different task. You should be pretty familiar by now of how this goes (dataset is normalized so need for this step).\n",
    "\n",
    "Load [this dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aicore.ml import data\n",
    "from sklearn import datasets, model_selection\n",
    "\n",
    "(X_train, y_train), (X_validation, y_validation), (X_test, y_test) = data.split(\n",
    "    datasets.load_digits(return_X_y=True)\n",
    ")\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model, X, y_true):\n",
    "    with g.no_grad():\n",
    "        y_pred = model.predict_logits(X)\n",
    "        return g.mean(ce_with_logits(y_pred, y_true))\n",
    "\n",
    "\n",
    "optimizer = g.optimizers.SGD(lr=1e-3)\n",
    "model = MulticlassLogisticRegression(\n",
    "    n_classes=10, n_features=X_train.shape[1], optimizer=optimizer\n",
    ")\n",
    "\n",
    "print(f\"Training loss before fit: {calculate_loss(model, X_train, y_train)}\")\n",
    "print(\n",
    "    f\"Validation loss before fit: {calculate_loss(model, X_validation, y_validation)}\"\n",
    ")\n",
    "print(f\"Test loss before fit: {calculate_loss(model, X_validation, y_validation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=1000)\n",
    "\n",
    "print(f\"Training loss after fit: {calculate_loss(model, X_train, y_train)}\")\n",
    "print(\n",
    "    f\"Validation loss after fit: {calculate_loss(model, X_validation, y_validation)}\"\n",
    ")\n",
    "print(f\"Test loss after fit: {calculate_loss(model, X_validation, y_validation)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to use simple linear models?\n",
    "\n",
    "We have seen how to create and use `linear models` for regression and classification.\n",
    "Soon we will meet more powerful models but here is the rough summary of when one should use it in real life:\n",
    "- as a baseline - gives us an overview and \"starting point\" for improvement\n",
    "- when we want easily explaianble model. Each weight shows the impact of a factor onto our target\n",
    "- when we have a lot of features (even more than data point) and we do not want to overfit on data\n",
    "\n",
    "With experience and more models it will become increasingly clear where we should use each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "- What is the probability for `10` classes to be correctly predicted in __multiclass__ setting? What about __multilabel__ version?\n",
    "- What loss function should we use when working with multilabel task? __Tip:__ we already presented it\n",
    "- Code `cross_entropy` only by choosing elements which targets point to. Can you do it for `backward` as well?\n",
    "- How does `multiclass` differ from `multilabel`? Show an example for single sample\n",
    "- Try to implement alpha smoothing. Test on some datasets and check whether that helps on test\n",
    "- Check out [Don't Overfit II](https://www.kaggle.com/c/dont-overfit-ii) Kaggle challenge and available solution to get a better idea when to use simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- multiclass classification is multiple linear regression stacked together\n",
    "- multiclass classification requires a different loss function (cross entropy)\n",
    "- we can work directly on logits to take predictions by using `argmax`\n",
    "- softmax is a differentiable function that turns a vector of real numbers into a probability distribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
