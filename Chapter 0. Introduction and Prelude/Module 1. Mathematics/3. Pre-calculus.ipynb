{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-calculus\n",
    "\n",
    "### Learning Objectives:\n",
    "- [The Meaning Of Calculus](#The-Meaning-Of-Calculus)\n",
    "\n",
    "\n",
    "# The Meaning Of Calculus\n",
    "\n",
    "In short, __calculus__ is the study of change. More specifically, in the field of calculus, we concern ourselves with the question: \"How does a quantity Y change with respect to some quantity X?\". Before diving into calculus, we will go over __linear equations/functions__ , which are functions that describe straight lines. These functions are given in the general form of:\n",
    "\n",
    "$$ f(x) = y = mx + c$$\n",
    "\n",
    "Where, y is the __output__, x is the __input__, c is the __y-intercept__ and m is __slope or gradient__.\n",
    "\n",
    "$f(x)$ denotes a __function__ of x, which just means a quantity that is affected by the input, x. With different combinations of the parameters m and c we can describe any straight line on the 2-D plane. However, our main reason for covering this equation is the slope, m. Consider the 4 examples plotted below:\n",
    "\n",
    "$$(1) \\; y_{A} = 0.5x - 3$$\n",
    "$$(2) \\; y_{B} = x - 3$$\n",
    "$$(3) \\; y_{C} = 2x - 3$$\n",
    "$$(4) \\; y_{D} = 4x - 3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Code\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "x_vals = np.linspace(0, 10, 1000)\n",
    "m = [0.5, 1, 2, 4]\n",
    "c = -3\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=x_vals, \n",
    "    y=(m[0]*x_vals + c),\n",
    "    marker_color = \"orange\"\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Linear Equations\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"y\",\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=x_vals, y=m[1]*x_vals + c,marker_color=\"orange\"))\n",
    "fig.add_trace(go.Scatter(x=x_vals, y=m[2]*x_vals + c,marker_color=\"orange\"))\n",
    "fig.add_trace(go.Scatter(x=x_vals, y=m[3]*x_vals + c,marker_color=\"orange\"))\n",
    "# adding annotations\n",
    "fig.add_annotation(\n",
    "            x=6,\n",
    "            y=23,\n",
    "            text=\"m = 4\")\n",
    "fig.add_annotation(\n",
    "            x=6,\n",
    "            y=12,\n",
    "            text=\"m = 2\")\n",
    "fig.add_annotation(\n",
    "            x=6,\n",
    "            y=5,\n",
    "            text=\"m = 1\")\n",
    "fig.add_annotation(\n",
    "            x=6,\n",
    "            y=-1,\n",
    "            text=\"m = 0.5\")\n",
    "fig.update_annotations(dict(\n",
    "            xref=\"x\",\n",
    "            yref=\"y\",\n",
    "            showarrow=False,\n",
    "            ax=0,\n",
    "            ay=-40\n",
    "))\n",
    "\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graph, we can see that the parameter c just shifts lines up or down, so what is m responsible for? We can clearly see that as the gradient of a line increases, the line becomes steeper. Hence, the slope is a measure of the __steepness__ of the line. Practically, what does that mean? Consider the table of values below for the equations $y_{A} ($m=0.5$)$ and $y_{D} ($m=4$)$:\n",
    "\n",
    "| x | $y_{A}$ | $y_{D}$ |\n",
    "|---|---------|---------|\n",
    "| 0 | -3      | -3      |\n",
    "| 1 | -2.5    | 1       |\n",
    "| 2 | -2      | 5       |\n",
    "| 3 | -1.5    | 9       |\n",
    "| 4 | -1      | 13      |\n",
    "| 5 | -1.5    | 17      |\n",
    "\n",
    "We can see that at $x = 0$, both equations have the same output, which is in agreement with the graph above. However, whenever $x$ __changes__ by 1, $y_{A}$ changes by 0.5 and $y_{D}$ changes by 4 due to their different gradients. This $y_{D}$ changes more rapidly with respect to x compared to $y_{A}$. This means that the gradient determines the __rate of change of the output with respect to the input__. Since the gradient(steepness) is constant throughout for a straight line, we can calculate the gradient of a line with the following equation:\n",
    "\n",
    "$$m = \\frac{\\Delta y}{\\Delta x} = \\frac{y_{2} - y_{1}}{x_{2} - x_{1}}$$\n",
    "\n",
    "Where $\\Delta$ is the capital Greek letter delta and represents \"a change in \", and the values shown are from the coordinates $(x_{1}, y_{1})$ and $(x_{2}, y_{2})$.\n",
    "\n",
    "Even from the definition we can see that the gradient of a line represents how quickly an output $y$ changes with an input $x$. However, the concept of the gradient of an equation cannot directly be applied to other equations or functions that are not linear. Consider for instance, the function below:\n",
    "\n",
    "$$f_{1}(x) = x^{2}$$\n",
    "\n",
    "The question then becomes: \"what is the gradient of $f_{1}(x)$ and how do we find it?\". Consider the plot shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Code\n",
    "\n",
    "# Main function\n",
    "x_vals = np.linspace(0, 7, 1000)\n",
    "y_vals = x_vals**2\n",
    "\n",
    "# Secondary traces\n",
    "x2 = np.linspace(0, 2, 100)\n",
    "x3 = np.linspace(2, 4, 100)\n",
    "x4 = np.linspace(5, 7, 100)\n",
    "\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=x_vals, \n",
    "    y=y_vals,\n",
    "    marker_color = \"orange\"\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Quadratic Equation\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"y\",\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=x2, y=0*x2, marker_color=\"black\"))\n",
    "fig.add_trace(go.Scatter(x=x3, y=6*x3 - 9, marker_color=\"black\"))\n",
    "fig.add_trace(go.Scatter(x=x4, y=12*x4 - 36, marker_color=\"black\"))\n",
    "# adding annotations\n",
    "fig.add_annotation(\n",
    "            x=6,\n",
    "            y=40,\n",
    "            text=\"m = 12\")\n",
    "fig.add_annotation(\n",
    "            x=3,\n",
    "            y=12,\n",
    "            text=\"m = 6\")\n",
    "fig.add_annotation(\n",
    "            x=0.01,\n",
    "            y=2,\n",
    "            text=\"m = 0\")\n",
    "\n",
    "fig.update_annotations(dict(\n",
    "            xref=\"x\",\n",
    "            yref=\"y\",\n",
    "            showarrow=False,\n",
    "            ax=0,\n",
    "            ay=-40\n",
    "))\n",
    "\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we have plotted a quadratic function (orange). From the black traces and annotations, we can see that there is no single gradient/steepness throughout the function, but rather the gradient seems to increase with $x$. The black traces are known as __tangent lines__, which are lines that just barely touch a particular point. The gradient of these tangent lines is a measure of how rapidly $y$ changes with $x$ at a given point, or in other words, the __instantenous slope__, and we can see from the plot why, since they follow the changing steepness of any function.\n",
    "\n",
    "Since we don't have an exact equation for it yet, before going into the instantaneous slope, let us start with the concept of an __average slope__. The average slope tells us the average change of the output y relative to the output x, over a given change $\\Delta x$. In general, for two points: $P_{1} = (x_{1}, y_{1})$ and $P_{2} = (x_{2}, y_{2})$, the average slope is given by:\n",
    "\n",
    "$$ m = \\frac{\\Delta y}{\\Delta x} = \\frac{f(x + \\Delta x) + f(x)}{\\Delta x} = \\frac{y_{2} - y_{1}}{x_{2} - x_{1}}$$\n",
    "\n",
    "Or in other words, we draw a straight line through two points and find the gradient of that given line, assuming it to be straight, as shown below for $f_{1}(x)$ between $x=2$ and $x=4$:\n",
    "\n",
    "$$m = \\frac{(4)^{2} - (2)^{2}}{4 - 2} = 6$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Code\n",
    "\n",
    "# Main function\n",
    "x_vals = np.linspace(0, 7, 1000)\n",
    "y_vals = x_vals**2\n",
    "\n",
    "# Secondary traces\n",
    "x2 = np.linspace(1, 5, 100)\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=x_vals, \n",
    "    y=y_vals,\n",
    "    marker_color = \"orange\"\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Average Slope\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"y\",\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=x2, y=6*x2 - 8, marker_color=\"black\"))\n",
    "fig.add_trace(go.Scatter(x=[2, 4], y=[4, 16], marker_color=\"black\", marker_size=20))\n",
    "\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It tells us on average, how rapidly did $y$ change with respect to $x$ over a long distance. However, it doesn't tell us about any of the individual slopes in between the two points, so it is not the \"best\" approximation, and the more the function differs from a straight line, the worse it will be. Can we make it better? Well, yes. The closer the two points are, the better this approximation becomes, and in fact, when the two points are really, really close, the line of the average slope really starts to look like the tangent line. Can we use this notion for anything? Consider the diagrams below, all showing the average slope for a given change in $x$, $\\Delta x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Code\n",
    "\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles = ['$\\Delta x = 1.5$ ', '$\\Delta x = 1$', '$\\Delta x = 0.5$', '$\\Delta x = 0.01$ '])\n",
    "\n",
    "# Main function\n",
    "x_vals = np.linspace(0, 7, 1000)\n",
    "y_vals = x_vals**2\n",
    "\n",
    "# Secondary traces\n",
    "x2 = np.linspace(1.5, 4, 100)\n",
    "x3 = np.linspace(1.5, 3.5, 100)\n",
    "x4 = np.linspace(1.5, 3, 100)\n",
    "x5 = np.linspace(1.5, 2.5, 100)\n",
    "\n",
    "# Updating layout\n",
    "fig.update_layout(title=\"Average Slopes\")\n",
    "fig.update_yaxes(title_text=\"y\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"y\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"y\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"x\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"y\", row=2, col=2)\n",
    "fig.update_xaxes(title_text=\"x\", row=2, col=2)\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_vals, y=y_vals, marker_color=\"orange\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=x2, y=5.5*x2 - 7, marker_color=\"black\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=[2, 3.5], y=[4, 12.25], marker_color=\"black\", marker_size=10), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_vals, y=y_vals, marker_color=\"orange\"), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=x3, y=5*x3 - 6, marker_color=\"black\"), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=[2, 3], y=[4, 9], marker_color=\"black\", marker_size=10), row=1, col=2)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_vals, y=y_vals, marker_color=\"orange\"), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=x4, y=4.5*x4 - 5, marker_color=\"black\"), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=[2, 2.5], y=[4, 6.25], marker_color=\"black\", marker_size=10), row=2, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_vals, y=y_vals, marker_color=\"orange\"), row=2, col=2)\n",
    "fig.add_trace(go.Scatter(x=x5, y=4*x5 - 4, marker_color=\"black\"), row=2, col=2)\n",
    "fig.add_trace(go.Scatter(x=[2, 2.01], y=[4, 2.01*2.01], marker_color=\"black\", marker_size=5), row=2, col=2)\n",
    "\n",
    "\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we can approximate the gradient at any point along a function by making $\\Delta x$ really, really tiny (infinitely small, ideally), and calculating the average gradient. This gradient at every given point of a function is known as a __derivative__, and the process of determining the derivative is known as __differentiation__. It is denoted below as follows:\n",
    "\n",
    "$$f'(x) = \\frac{dy}{dx} \\approx \\frac{f(x + \\Delta x) + f(x)}{\\Delta x}$$\n",
    "\n",
    "Where the approximation improves the closer $\\Delta x$ is to 0. This is why back in the old day, the field used to be called infinitessimal calculus!\n",
    "\n",
    "As the change in $x$ becomes so small it is considered to be pretty much zero, we change notation from:\n",
    "\n",
    "$$\\frac{\\Delta y}{\\Delta x} \\xrightarrow{} \\frac{dy}{dx}$$\n",
    "\n",
    "Since we can do this for every single point, we end up obtaining __gradient functions__, which are functions that tell us the gradient of our original function at every value of $x$. By making some assumptions, mathematicians were able to obtain some nifty rules of differentiation for different functions. These are only some of the many rules, and the rest are all available online:\n",
    "\n",
    "| $f(x)$   | $f'(x)$      |\n",
    "|----------|--------------|\n",
    "| $x^{n}$  | $nx^{n-1}$   |\n",
    "| $sin(x)$ | $cos(x)$     |\n",
    "| $cos(x)$ | $-sin(x)$    |\n",
    "| $e^{x}$  | $e^{x}$      |\n",
    "| $a^{x}$  | $ln(a)a^{x}$ |\n",
    "\n",
    "All of these derivatives were derived by using the same approximation as above, so let's put it to test. We are now going to use NumPy to write a function that takes in a function and returns the derivative of that function. It will taken in a Python function equivalent of the function we want to differentiate, the x-values at which we compute the derivative, and the $x$-interval we use for an approximation of a really tiny change in $x (dx)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODING CHALLENGE\n",
    "\n",
    "# Test functions\n",
    "def x_square(x):\n",
    "    return ##\n",
    "\n",
    "def sin(x):\n",
    "    return ##\n",
    "\n",
    "# Function that approximates derivative of an input function\n",
    "def deriv(f_func, x_vals, dx=0.0001):\n",
    "    dy = np.array([])##\n",
    "    for x_val in x_vals:\n",
    "        slope = ##\n",
    "        dy = ##\n",
    "    return dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, now we have a function that approximates the derivative at all input x-values for a given function. Let us consider two examples, $f_{1}(x) = x^{2}$ and $f_{2}(x) = sin(x)$. Given the table above, their derivatives are given below:\n",
    "\n",
    "$$f'_{1}(x) = 2\\times x^{2-1} = 2x$$\n",
    "$$f'_{2}(x) = cos(x)$$\n",
    "\n",
    "Let us compute their respective derivatives and test out whether our function works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = ##\n",
    "\n",
    "y1 = x_square(x_vals)\n",
    "dy1 = deriv(x_square, x_vals)\n",
    "\n",
    "y2 = sin(x_vals)\n",
    "dy2 = deriv(sin, x_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now finally, we can plot them below alongside the original function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Code\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2)\n",
    "\n",
    "fig.update_layout(title=\"Functions and Their Respective Gradient Functions\")\n",
    "fig.update_yaxes(title_text=\"y\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"y\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"x\", row=1, col=2)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_vals, y=y1, marker_color=\"orange\", name=\"$f_{1}(x)=x^{2}$\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=x_vals, y=dy1, marker_color=\"black\", name=\"$f_{1}'(x)=2x$\"), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_vals, y=y2, marker_color=\"red\", name=\"$f_{2}(x)=sin(x)$\"), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=x_vals, y=dy2, marker_color=\"blue\", name=\"$f_{2}'(x)=cos(x)$\"), row=1, col=2)\n",
    "\n",
    "\n",
    "fig.update_layout(showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, now you can calculate the rate of change of any simple function with respect to its input, which also means you get the __meaning of calculus__!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
