{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "- Formula behind batch normalisation\n",
    "- Application in PyTorch\n",
    "- Advantage of batch normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "- Data normalisation\n",
    "- Feed-forward neural networks\n",
    "- Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro\n",
    "Before training a machine learning algorithm, it is common practice to normalise the input data, especially when the features have different scales, e.g. house prices and building year. This can lead to features with higher values having an unwanted greater impact on changes of a predictor. Normalising data can avoid this and lead to better performance. Since this technique is proven to work for the input data, it is natural to apply the same technique to the hidden layers in a neural network. <br>\n",
    "Batch normalisation tackles the problem of internal covariate shift in deep neural networks, which describes the phenomenon that the input distribution of each layer changes a lot as the input to each layer is affected by the parameters of all preceding layers s.t. even small changes in parameters have a big impact on the parameters further down the line. Solving this problem typically requires careful initialisation of the parameters as well as small learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Notation\n",
    "Batch normalisation basically sets the mean of each feature to zero and the variance to 1:\n",
    "\\begin{equation*}\n",
    "\\hat{x} = \\frac{x - \\mu(x)}{\\sqrt{\\sigma^2(x)}}\n",
    "\\end{equation*}\n",
    "where the mean and variance are computed over the batch during training and over the population after training.<br>\n",
    "The normalised features are then scaled and shifted by introducing the parameters $\\gamma$ and $\\beta$:\n",
    "\\begin{equation*}\n",
    "y = \\gamma \\hat{x} + \\beta\n",
    "\\end{equation*}\n",
    "These steps are applied to the activations of each layer before feeding them as input to the next layer. Therefore, we must also include them in the backpropagation, which we do by calculating the gradient w.r.t. the new parameters $\\gamma$ and $\\beta$. \n",
    "These parameters are important because the computation of $\\hat{x}$ where the mean is set to 0 with unit variance might not be desirable in every layer. Especially if we compute this for the input of a softmax function, it is desirable that the input has higher variance such that the output is a conclusive probability distribution over its input. The $\\gamma$ and $\\beta$ parameters learn to regulate $\\hat{x}$. If $\\gamma$ is set to $\\sqrt{\\sigma^2(x)}$ and $\\beta$ is set to $\\mu(x)$, we can restore the original values of $x$.\n",
    "<br><br>\n",
    "\n",
    "In a convolutional neural network, the normalisation is applied jointly over all locations in a feature map, s.t. we can learn the parameters $\\gamma$ and $\\beta$ per feature map and not per activation because we want to normalise the features in the same way regardless of whether they are in different convolutional windows. This means that we normalise the same activations in the same feature maps.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "We will use the CNN for MNIST image prediction that you already worked with before. Run the original code so you can compare performance to the same CNN using batch normalisation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0it [00:00, ?it/s]The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST-data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "9920512it [00:02, 3521918.84it/s]                             \n",
      "Extracting MNIST-data/MNIST/raw/train-images-idx3-ubyte.gz to MNIST-data/MNIST/raw\n",
      "0it [00:00, ?it/s]Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST-data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "32768it [00:00, 84089.48it/s]            \n",
      "0it [00:00, ?it/s]Extracting MNIST-data/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST-data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST-data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "1654784it [00:01, 1509963.31it/s]                             \n",
      "0it [00:00, ?it/s]Extracting MNIST-data/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST-data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST-data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "8192it [00:00, 35146.71it/s]            \n",
      "Extracting MNIST-data/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST-data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import get_dataloaders\n",
    "'''\n",

    "'''\n",
    "batch_size = 128\n",
    "train_loader, val_loader, test_loader = get_dataloaders(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self, batch_norm):\n",
    "        super().__init__()\n",
    "            # conv2d(in_channels, out_channels, kernel_size)\n",
    "            # in_channels is the number of layers which it takes in (i.e.num color channels in 1st layer)\n",
    "            # out_channels is the number of different filters that we use\n",
    "            # kernel_size is the depthxwidthxheight of the kernel#\n",
    "            # stride is how many pixels we shift the kernel by each time\n",
    "        self.batch_norm = batch_norm\n",
    "        if self.batch_norm:\n",
    "            print('yes')\n",
    "            self.conv_layers = torch.nn.Sequential( # put your convolutional architecture here using torch.nn.Sequential \n",
    "            torch.nn.Conv2d(1, 16, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(32)\n",
    "            )\n",
    "            self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(32*20*20, 10) # put your linear architecture here using torch.nn.Sequential \n",
    "        )\n",
    "        else:\n",
    "            self.conv_layers = torch.nn.Sequential( # put your convolutional architecture here using torch.nn.Sequential \n",
    "            torch.nn.Conv2d(1, 16, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU()\n",
    "            )\n",
    "            self.fc_layers = torch.nn.Sequential(\n",
    "                torch.nn.Linear(32*20*20, 10) # put your linear architecture here using torch.nn.Sequential \n",
    "            )\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)# pass through conv layers\n",
    "        x = x.view(x.shape[0], -1)# flatten output ready for fully connected layer\n",
    "        x = self.fc_layers(x)# pass through fully connected layer\n",
    "        x = F.softmax(x, dim=1)# softmax activation function on outputs\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available() # checks if gpu is available\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "learning_rate = 0.0005 # set learning rate\n",
    "epochs = 5 # set number of epochs\n",
    "\n",
    "cnn = ConvNet(batch_norm=False).to(device) #.to(device)#instantiate model\n",
    "criterion = torch.nn.CrossEntropyLoss() #use cross entropy loss function\n",
    "optimiser = torch.optim.Adam(cnn.parameters(), lr=learning_rate) # use Adam optimizer, passing it the parameters of your model and the learning rate\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "writer1 = SummaryWriter(log_dir=\"runs/cnn\") # we will use this to show our models performance on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "05005\nEpoch: 3 \tBatch: 353 \tLoss: 1.4823527336120605\nEpoch: 3 \tBatch: 354 \tLoss: 1.4692952632904053\nEpoch: 3 \tBatch: 355 \tLoss: 1.4763778448104858\nEpoch: 3 \tBatch: 356 \tLoss: 1.5029478073120117\nEpoch: 3 \tBatch: 357 \tLoss: 1.482025146484375\nEpoch: 3 \tBatch: 358 \tLoss: 1.4939398765563965\nEpoch: 3 \tBatch: 359 \tLoss: 1.5119682550430298\nEpoch: 3 \tBatch: 360 \tLoss: 1.4724559783935547\nEpoch: 3 \tBatch: 361 \tLoss: 1.4951804876327515\nEpoch: 3 \tBatch: 362 \tLoss: 1.470774531364441\nEpoch: 3 \tBatch: 363 \tLoss: 1.4798011779785156\nEpoch: 3 \tBatch: 364 \tLoss: 1.479568362236023\nEpoch: 3 \tBatch: 365 \tLoss: 1.5032215118408203\nEpoch: 3 \tBatch: 366 \tLoss: 1.500737190246582\nEpoch: 3 \tBatch: 367 \tLoss: 1.4723504781723022\nEpoch: 3 \tBatch: 368 \tLoss: 1.494850516319275\nEpoch: 3 \tBatch: 369 \tLoss: 1.4782570600509644\nEpoch: 3 \tBatch: 370 \tLoss: 1.4615105390548706\nEpoch: 3 \tBatch: 371 \tLoss: 1.5110297203063965\nEpoch: 3 \tBatch: 372 \tLoss: 1.4826791286468506\nEpoch: 3 \tBatch: 373 \tLoss: 1.4728959798812866\nEpoch: 3 \tBatch: 374 \tLoss: 1.4745405912399292\nEpoch: 3 \tBatch: 375 \tLoss: 1.4960557222366333\nEpoch: 3 \tBatch: 376 \tLoss: 1.4865154027938843\nEpoch: 3 \tBatch: 377 \tLoss: 1.4804339408874512\nEpoch: 3 \tBatch: 378 \tLoss: 1.4884133338928223\nEpoch: 3 \tBatch: 379 \tLoss: 1.46686851978302\nEpoch: 3 \tBatch: 380 \tLoss: 1.4788647890090942\nEpoch: 3 \tBatch: 381 \tLoss: 1.4802391529083252\nEpoch: 3 \tBatch: 382 \tLoss: 1.4710040092468262\nEpoch: 3 \tBatch: 383 \tLoss: 1.4819427728652954\nEpoch: 3 \tBatch: 384 \tLoss: 1.48908269405365\nEpoch: 3 \tBatch: 385 \tLoss: 1.4784730672836304\nEpoch: 3 \tBatch: 386 \tLoss: 1.4690678119659424\nEpoch: 3 \tBatch: 387 \tLoss: 1.508713960647583\nEpoch: 3 \tBatch: 388 \tLoss: 1.4882136583328247\nEpoch: 3 \tBatch: 389 \tLoss: 1.4786376953125\nEpoch: 3 \tBatch: 390 \tLoss: 1.4618116617202759\nEpoch: 4 \tBatch: 0 \tLoss: 1.477656602859497\nEpoch: 4 \tBatch: 1 \tLoss: 1.4905731678009033\nEpoch: 4 \tBatch: 2 \tLoss: 1.4786242246627808\nEpoch: 4 \tBatch: 3 \tLoss: 1.4873976707458496\nEpoch: 4 \tBatch: 4 \tLoss: 1.470674991607666\nEpoch: 4 \tBatch: 5 \tLoss: 1.5167514085769653\nEpoch: 4 \tBatch: 6 \tLoss: 1.4968576431274414\nEpoch: 4 \tBatch: 7 \tLoss: 1.4850499629974365\nEpoch: 4 \tBatch: 8 \tLoss: 1.4639090299606323\nEpoch: 4 \tBatch: 9 \tLoss: 1.4730889797210693\nEpoch: 4 \tBatch: 10 \tLoss: 1.4710010290145874\nEpoch: 4 \tBatch: 11 \tLoss: 1.4804415702819824\nEpoch: 4 \tBatch: 12 \tLoss: 1.4815157651901245\nEpoch: 4 \tBatch: 13 \tLoss: 1.4796441793441772\nEpoch: 4 \tBatch: 14 \tLoss: 1.4778316020965576\nEpoch: 4 \tBatch: 15 \tLoss: 1.4642220735549927\nEpoch: 4 \tBatch: 16 \tLoss: 1.4907965660095215\nEpoch: 4 \tBatch: 17 \tLoss: 1.520835280418396\nEpoch: 4 \tBatch: 18 \tLoss: 1.4941660165786743\nEpoch: 4 \tBatch: 19 \tLoss: 1.4818272590637207\nEpoch: 4 \tBatch: 20 \tLoss: 1.4815945625305176\nEpoch: 4 \tBatch: 21 \tLoss: 1.4879531860351562\nEpoch: 4 \tBatch: 22 \tLoss: 1.4977003335952759\nEpoch: 4 \tBatch: 23 \tLoss: 1.4903780221939087\nEpoch: 4 \tBatch: 24 \tLoss: 1.4905943870544434\nEpoch: 4 \tBatch: 25 \tLoss: 1.475903868675232\nEpoch: 4 \tBatch: 26 \tLoss: 1.475468397140503\nEpoch: 4 \tBatch: 27 \tLoss: 1.4778270721435547\nEpoch: 4 \tBatch: 28 \tLoss: 1.4790112972259521\nEpoch: 4 \tBatch: 29 \tLoss: 1.471056342124939\nEpoch: 4 \tBatch: 30 \tLoss: 1.4809343814849854\nEpoch: 4 \tBatch: 31 \tLoss: 1.4632924795150757\nEpoch: 4 \tBatch: 32 \tLoss: 1.4725017547607422\nEpoch: 4 \tBatch: 33 \tLoss: 1.4846614599227905\nEpoch: 4 \tBatch: 34 \tLoss: 1.4697158336639404\nEpoch: 4 \tBatch: 35 \tLoss: 1.4897514581680298\nEpoch: 4 \tBatch: 36 \tLoss: 1.4851784706115723\nEpoch: 4 \tBatch: 37 \tLoss: 1.4693578481674194\nEpoch: 4 \tBatch: 38 \tLoss: 1.484794020652771\nEpoch: 4 \tBatch: 39 \tLoss: 1.5076502561569214\nEpoch: 4 \tBatch: 40 \tLoss: 1.4934715032577515\nEpoch: 4 \tBatch: 41 \tLoss: 1.4680801630020142\nEpoch: 4 \tBatch: 42 \tLoss: 1.4815870523452759\nEpoch: 4 \tBatch: 43 \tLoss: 1.4692509174346924\nEpoch: 4 \tBatch: 44 \tLoss: 1.4996721744537354\nEpoch: 4 \tBatch: 45 \tLoss: 1.5071048736572266\nEpoch: 4 \tBatch: 46 \tLoss: 1.4612760543823242\nEpoch: 4 \tBatch: 47 \tLoss: 1.4874906539916992\nEpoch: 4 \tBatch: 48 \tLoss: 1.4846128225326538\nEpoch: 4 \tBatch: 49 \tLoss: 1.480773687362671\nEpoch: 4 \tBatch: 50 \tLoss: 1.4846456050872803\nEpoch: 4 \tBatch: 51 \tLoss: 1.4868173599243164\nEpoch: 4 \tBatch: 52 \tLoss: 1.4621299505233765\nEpoch: 4 \tBatch: 53 \tLoss: 1.4715197086334229\nEpoch: 4 \tBatch: 54 \tLoss: 1.4821946620941162\nEpoch: 4 \tBatch: 55 \tLoss: 1.4718947410583496\nEpoch: 4 \tBatch: 56 \tLoss: 1.4811335802078247\nEpoch: 4 \tBatch: 57 \tLoss: 1.4799857139587402\nEpoch: 4 \tBatch: 58 \tLoss: 1.467046856880188\nEpoch: 4 \tBatch: 59 \tLoss: 1.492646336555481\nEpoch: 4 \tBatch: 60 \tLoss: 1.4724348783493042\nEpoch: 4 \tBatch: 61 \tLoss: 1.4951366186141968\nEpoch: 4 \tBatch: 62 \tLoss: 1.4746466875076294\nEpoch: 4 \tBatch: 63 \tLoss: 1.4625529050827026\nEpoch: 4 \tBatch: 64 \tLoss: 1.4872255325317383\nEpoch: 4 \tBatch: 65 \tLoss: 1.4839791059494019\nEpoch: 4 \tBatch: 66 \tLoss: 1.4866397380828857\nEpoch: 4 \tBatch: 67 \tLoss: 1.4890177249908447\nEpoch: 4 \tBatch: 68 \tLoss: 1.4739102125167847\nEpoch: 4 \tBatch: 69 \tLoss: 1.4925941228866577\nEpoch: 4 \tBatch: 70 \tLoss: 1.4848910570144653\nEpoch: 4 \tBatch: 71 \tLoss: 1.4660656452178955\nEpoch: 4 \tBatch: 72 \tLoss: 1.4842151403427124\nEpoch: 4 \tBatch: 73 \tLoss: 1.4678629636764526\nEpoch: 4 \tBatch: 74 \tLoss: 1.4912493228912354\nEpoch: 4 \tBatch: 75 \tLoss: 1.4676536321640015\nEpoch: 4 \tBatch: 76 \tLoss: 1.4625070095062256\nEpoch: 4 \tBatch: 77 \tLoss: 1.493367314338684\nEpoch: 4 \tBatch: 78 \tLoss: 1.463971495628357\nEpoch: 4 \tBatch: 79 \tLoss: 1.4700926542282104\nEpoch: 4 \tBatch: 80 \tLoss: 1.4815149307250977\nEpoch: 4 \tBatch: 81 \tLoss: 1.4901891946792603\nEpoch: 4 \tBatch: 82 \tLoss: 1.466306209564209\nEpoch: 4 \tBatch: 83 \tLoss: 1.4724971055984497\nEpoch: 4 \tBatch: 84 \tLoss: 1.4883506298065186\nEpoch: 4 \tBatch: 85 \tLoss: 1.4857585430145264\nEpoch: 4 \tBatch: 86 \tLoss: 1.4696142673492432\nEpoch: 4 \tBatch: 87 \tLoss: 1.4642375707626343\nEpoch: 4 \tBatch: 88 \tLoss: 1.4786819219589233\nEpoch: 4 \tBatch: 89 \tLoss: 1.4694687128067017\nEpoch: 4 \tBatch: 90 \tLoss: 1.4777878522872925\nEpoch: 4 \tBatch: 91 \tLoss: 1.4763848781585693\nEpoch: 4 \tBatch: 92 \tLoss: 1.477268099784851\nEpoch: 4 \tBatch: 93 \tLoss: 1.497084379196167\nEpoch: 4 \tBatch: 94 \tLoss: 1.4669773578643799\nEpoch: 4 \tBatch: 95 \tLoss: 1.490977168083191\nEpoch: 4 \tBatch: 96 \tLoss: 1.475502848625183\nEpoch: 4 \tBatch: 97 \tLoss: 1.475436806678772\nEpoch: 4 \tBatch: 98 \tLoss: 1.496158242225647\nEpoch: 4 \tBatch: 99 \tLoss: 1.477996826171875\nEpoch: 4 \tBatch: 100 \tLoss: 1.5126001834869385\nEpoch: 4 \tBatch: 101 \tLoss: 1.4636461734771729\nEpoch: 4 \tBatch: 102 \tLoss: 1.4637969732284546\nEpoch: 4 \tBatch: 103 \tLoss: 1.4737039804458618\nEpoch: 4 \tBatch: 104 \tLoss: 1.4775102138519287\nEpoch: 4 \tBatch: 105 \tLoss: 1.4884567260742188\nEpoch: 4 \tBatch: 106 \tLoss: 1.4796032905578613\nEpoch: 4 \tBatch: 107 \tLoss: 1.4759718179702759\nEpoch: 4 \tBatch: 108 \tLoss: 1.4933674335479736\nEpoch: 4 \tBatch: 109 \tLoss: 1.4726662635803223\nEpoch: 4 \tBatch: 110 \tLoss: 1.466828465461731\nEpoch: 4 \tBatch: 111 \tLoss: 1.4781016111373901\nEpoch: 4 \tBatch: 112 \tLoss: 1.4820280075073242\nEpoch: 4 \tBatch: 113 \tLoss: 1.461297631263733\nEpoch: 4 \tBatch: 114 \tLoss: 1.477779746055603\nEpoch: 4 \tBatch: 115 \tLoss: 1.4766247272491455\nEpoch: 4 \tBatch: 116 \tLoss: 1.4712648391723633\nEpoch: 4 \tBatch: 117 \tLoss: 1.4870473146438599\nEpoch: 4 \tBatch: 118 \tLoss: 1.4767177104949951\nEpoch: 4 \tBatch: 119 \tLoss: 1.4832401275634766\nEpoch: 4 \tBatch: 120 \tLoss: 1.4692976474761963\nEpoch: 4 \tBatch: 121 \tLoss: 1.4817572832107544\nEpoch: 4 \tBatch: 122 \tLoss: 1.4695560932159424\nEpoch: 4 \tBatch: 123 \tLoss: 1.4789224863052368\nEpoch: 4 \tBatch: 124 \tLoss: 1.4743086099624634\nEpoch: 4 \tBatch: 125 \tLoss: 1.4752076864242554\nEpoch: 4 \tBatch: 126 \tLoss: 1.4694801568984985\nEpoch: 4 \tBatch: 127 \tLoss: 1.4757673740386963\nEpoch: 4 \tBatch: 128 \tLoss: 1.4703186750411987\nEpoch: 4 \tBatch: 129 \tLoss: 1.4867119789123535\nEpoch: 4 \tBatch: 130 \tLoss: 1.4928549528121948\nEpoch: 4 \tBatch: 131 \tLoss: 1.4766823053359985\nEpoch: 4 \tBatch: 132 \tLoss: 1.4733353853225708\nEpoch: 4 \tBatch: 133 \tLoss: 1.488571047782898\nEpoch: 4 \tBatch: 134 \tLoss: 1.4671286344528198\nEpoch: 4 \tBatch: 135 \tLoss: 1.466011881828308\nEpoch: 4 \tBatch: 136 \tLoss: 1.4914494752883911\nEpoch: 4 \tBatch: 137 \tLoss: 1.471346139907837\nEpoch: 4 \tBatch: 138 \tLoss: 1.4740149974822998\nEpoch: 4 \tBatch: 139 \tLoss: 1.4863181114196777\nEpoch: 4 \tBatch: 140 \tLoss: 1.4618321657180786\nEpoch: 4 \tBatch: 141 \tLoss: 1.4708606004714966\nEpoch: 4 \tBatch: 142 \tLoss: 1.481866717338562\nEpoch: 4 \tBatch: 143 \tLoss: 1.4794330596923828\nEpoch: 4 \tBatch: 144 \tLoss: 1.500260353088379\nEpoch: 4 \tBatch: 145 \tLoss: 1.4918025732040405\nEpoch: 4 \tBatch: 146 \tLoss: 1.47799551486969\nEpoch: 4 \tBatch: 147 \tLoss: 1.4846910238265991\nEpoch: 4 \tBatch: 148 \tLoss: 1.4953681230545044\nEpoch: 4 \tBatch: 149 \tLoss: 1.4985178709030151\nEpoch: 4 \tBatch: 150 \tLoss: 1.5004537105560303\nEpoch: 4 \tBatch: 151 \tLoss: 1.492401361465454\nEpoch: 4 \tBatch: 152 \tLoss: 1.4638973474502563\nEpoch: 4 \tBatch: 153 \tLoss: 1.4843125343322754\nEpoch: 4 \tBatch: 154 \tLoss: 1.481543779373169\nEpoch: 4 \tBatch: 155 \tLoss: 1.4692051410675049\nEpoch: 4 \tBatch: 156 \tLoss: 1.4726529121398926\nEpoch: 4 \tBatch: 157 \tLoss: 1.4769251346588135\nEpoch: 4 \tBatch: 158 \tLoss: 1.4981037378311157\nEpoch: 4 \tBatch: 159 \tLoss: 1.4729931354522705\nEpoch: 4 \tBatch: 160 \tLoss: 1.4803067445755005\nEpoch: 4 \tBatch: 161 \tLoss: 1.4788587093353271\nEpoch: 4 \tBatch: 162 \tLoss: 1.4952362775802612\nEpoch: 4 \tBatch: 163 \tLoss: 1.4756040573120117\nEpoch: 4 \tBatch: 164 \tLoss: 1.4781582355499268\nEpoch: 4 \tBatch: 165 \tLoss: 1.4857369661331177\nEpoch: 4 \tBatch: 166 \tLoss: 1.4886958599090576\nEpoch: 4 \tBatch: 167 \tLoss: 1.4794446229934692\nEpoch: 4 \tBatch: 168 \tLoss: 1.4725046157836914\nEpoch: 4 \tBatch: 169 \tLoss: 1.4798517227172852\nEpoch: 4 \tBatch: 170 \tLoss: 1.4783486127853394\nEpoch: 4 \tBatch: 171 \tLoss: 1.4660791158676147\nEpoch: 4 \tBatch: 172 \tLoss: 1.478758692741394\nEpoch: 4 \tBatch: 173 \tLoss: 1.4811437129974365\nEpoch: 4 \tBatch: 174 \tLoss: 1.4675713777542114\nEpoch: 4 \tBatch: 175 \tLoss: 1.470400094985962\nEpoch: 4 \tBatch: 176 \tLoss: 1.4729454517364502\nEpoch: 4 \tBatch: 177 \tLoss: 1.4986188411712646\nEpoch: 4 \tBatch: 178 \tLoss: 1.486670970916748\nEpoch: 4 \tBatch: 179 \tLoss: 1.4641376733779907\nEpoch: 4 \tBatch: 180 \tLoss: 1.477521538734436\nEpoch: 4 \tBatch: 181 \tLoss: 1.4919008016586304\nEpoch: 4 \tBatch: 182 \tLoss: 1.4725518226623535\nEpoch: 4 \tBatch: 183 \tLoss: 1.4668453931808472\nEpoch: 4 \tBatch: 184 \tLoss: 1.485201120376587\nEpoch: 4 \tBatch: 185 \tLoss: 1.482151985168457\nEpoch: 4 \tBatch: 186 \tLoss: 1.4902814626693726\nEpoch: 4 \tBatch: 187 \tLoss: 1.5014698505401611\nEpoch: 4 \tBatch: 188 \tLoss: 1.4863255023956299\nEpoch: 4 \tBatch: 189 \tLoss: 1.4636493921279907\nEpoch: 4 \tBatch: 190 \tLoss: 1.481175184249878\nEpoch: 4 \tBatch: 191 \tLoss: 1.4806009531021118\nEpoch: 4 \tBatch: 192 \tLoss: 1.4778650999069214\nEpoch: 4 \tBatch: 193 \tLoss: 1.4880934953689575\nEpoch: 4 \tBatch: 194 \tLoss: 1.4753910303115845\nEpoch: 4 \tBatch: 195 \tLoss: 1.496700644493103\nEpoch: 4 \tBatch: 196 \tLoss: 1.500996708869934\nEpoch: 4 \tBatch: 197 \tLoss: 1.471402645111084\nEpoch: 4 \tBatch: 198 \tLoss: 1.492171049118042\nEpoch: 4 \tBatch: 199 \tLoss: 1.471997857093811\nEpoch: 4 \tBatch: 200 \tLoss: 1.4643800258636475\nEpoch: 4 \tBatch: 201 \tLoss: 1.4756790399551392\nEpoch: 4 \tBatch: 202 \tLoss: 1.4810458421707153\nEpoch: 4 \tBatch: 203 \tLoss: 1.4698116779327393\nEpoch: 4 \tBatch: 204 \tLoss: 1.4837695360183716\nEpoch: 4 \tBatch: 205 \tLoss: 1.479643702507019\nEpoch: 4 \tBatch: 206 \tLoss: 1.4760924577713013\nEpoch: 4 \tBatch: 207 \tLoss: 1.4766291379928589\nEpoch: 4 \tBatch: 208 \tLoss: 1.4778586626052856\nEpoch: 4 \tBatch: 209 \tLoss: 1.4904597997665405\nEpoch: 4 \tBatch: 210 \tLoss: 1.470183253288269\nEpoch: 4 \tBatch: 211 \tLoss: 1.4862943887710571\nEpoch: 4 \tBatch: 212 \tLoss: 1.465038776397705\nEpoch: 4 \tBatch: 213 \tLoss: 1.4770300388336182\nEpoch: 4 \tBatch: 214 \tLoss: 1.4699209928512573\nEpoch: 4 \tBatch: 215 \tLoss: 1.4788891077041626\nEpoch: 4 \tBatch: 216 \tLoss: 1.475216269493103\nEpoch: 4 \tBatch: 217 \tLoss: 1.4830241203308105\nEpoch: 4 \tBatch: 218 \tLoss: 1.4616152048110962\nEpoch: 4 \tBatch: 219 \tLoss: 1.4707998037338257\nEpoch: 4 \tBatch: 220 \tLoss: 1.4782450199127197\nEpoch: 4 \tBatch: 221 \tLoss: 1.4753156900405884\nEpoch: 4 \tBatch: 222 \tLoss: 1.4863988161087036\nEpoch: 4 \tBatch: 223 \tLoss: 1.4793354272842407\nEpoch: 4 \tBatch: 224 \tLoss: 1.4899178743362427\nEpoch: 4 \tBatch: 225 \tLoss: 1.4719175100326538\nEpoch: 4 \tBatch: 226 \tLoss: 1.464342474937439\nEpoch: 4 \tBatch: 227 \tLoss: 1.4930986166000366\nEpoch: 4 \tBatch: 228 \tLoss: 1.4751451015472412\nEpoch: 4 \tBatch: 229 \tLoss: 1.4783434867858887\nEpoch: 4 \tBatch: 230 \tLoss: 1.4702876806259155\nEpoch: 4 \tBatch: 231 \tLoss: 1.469480037689209\nEpoch: 4 \tBatch: 232 \tLoss: 1.482862949371338\nEpoch: 4 \tBatch: 233 \tLoss: 1.4612832069396973\nEpoch: 4 \tBatch: 234 \tLoss: 1.481695294380188\nEpoch: 4 \tBatch: 235 \tLoss: 1.4797319173812866\nEpoch: 4 \tBatch: 236 \tLoss: 1.4823284149169922\nEpoch: 4 \tBatch: 237 \tLoss: 1.497331142425537\nEpoch: 4 \tBatch: 238 \tLoss: 1.4794963598251343\nEpoch: 4 \tBatch: 239 \tLoss: 1.486738681793213\nEpoch: 4 \tBatch: 240 \tLoss: 1.4826396703720093\nEpoch: 4 \tBatch: 241 \tLoss: 1.471451759338379\nEpoch: 4 \tBatch: 242 \tLoss: 1.4807192087173462\nEpoch: 4 \tBatch: 243 \tLoss: 1.4796297550201416\nEpoch: 4 \tBatch: 244 \tLoss: 1.4701308012008667\nEpoch: 4 \tBatch: 245 \tLoss: 1.4677560329437256\nEpoch: 4 \tBatch: 246 \tLoss: 1.477176547050476\nEpoch: 4 \tBatch: 247 \tLoss: 1.4730921983718872\nEpoch: 4 \tBatch: 248 \tLoss: 1.4871692657470703\nEpoch: 4 \tBatch: 249 \tLoss: 1.4689494371414185\nEpoch: 4 \tBatch: 250 \tLoss: 1.4712893962860107\nEpoch: 4 \tBatch: 251 \tLoss: 1.4964519739151\nEpoch: 4 \tBatch: 252 \tLoss: 1.4715752601623535\nEpoch: 4 \tBatch: 253 \tLoss: 1.4699440002441406\nEpoch: 4 \tBatch: 254 \tLoss: 1.4796191453933716\nEpoch: 4 \tBatch: 255 \tLoss: 1.472253680229187\nEpoch: 4 \tBatch: 256 \tLoss: 1.4771928787231445\nEpoch: 4 \tBatch: 257 \tLoss: 1.4697247743606567\nEpoch: 4 \tBatch: 258 \tLoss: 1.4893200397491455\nEpoch: 4 \tBatch: 259 \tLoss: 1.477786898612976\nEpoch: 4 \tBatch: 260 \tLoss: 1.4808294773101807\nEpoch: 4 \tBatch: 261 \tLoss: 1.4702454805374146\nEpoch: 4 \tBatch: 262 \tLoss: 1.4618070125579834\nEpoch: 4 \tBatch: 263 \tLoss: 1.484855055809021\nEpoch: 4 \tBatch: 264 \tLoss: 1.4699229001998901\nEpoch: 4 \tBatch: 265 \tLoss: 1.4874179363250732\nEpoch: 4 \tBatch: 266 \tLoss: 1.4701967239379883\nEpoch: 4 \tBatch: 267 \tLoss: 1.4783543348312378\nEpoch: 4 \tBatch: 268 \tLoss: 1.4726157188415527\nEpoch: 4 \tBatch: 269 \tLoss: 1.5056341886520386\nEpoch: 4 \tBatch: 270 \tLoss: 1.477442979812622\nEpoch: 4 \tBatch: 271 \tLoss: 1.469986915588379\nEpoch: 4 \tBatch: 272 \tLoss: 1.4739004373550415\nEpoch: 4 \tBatch: 273 \tLoss: 1.4841375350952148\nEpoch: 4 \tBatch: 274 \tLoss: 1.477997064590454\nEpoch: 4 \tBatch: 275 \tLoss: 1.4869450330734253\nEpoch: 4 \tBatch: 276 \tLoss: 1.478094458580017\nEpoch: 4 \tBatch: 277 \tLoss: 1.500109314918518\nEpoch: 4 \tBatch: 278 \tLoss: 1.4773032665252686\nEpoch: 4 \tBatch: 279 \tLoss: 1.4619250297546387\nEpoch: 4 \tBatch: 280 \tLoss: 1.4648420810699463\nEpoch: 4 \tBatch: 281 \tLoss: 1.4900202751159668\nEpoch: 4 \tBatch: 282 \tLoss: 1.469797968864441\nEpoch: 4 \tBatch: 283 \tLoss: 1.468603491783142\nEpoch: 4 \tBatch: 284 \tLoss: 1.4976633787155151\nEpoch: 4 \tBatch: 285 \tLoss: 1.4624420404434204\nEpoch: 4 \tBatch: 286 \tLoss: 1.472065806388855\nEpoch: 4 \tBatch: 287 \tLoss: 1.4802459478378296\nEpoch: 4 \tBatch: 288 \tLoss: 1.4832741022109985\nEpoch: 4 \tBatch: 289 \tLoss: 1.4857808351516724\nEpoch: 4 \tBatch: 290 \tLoss: 1.483939528465271\nEpoch: 4 \tBatch: 291 \tLoss: 1.4683492183685303\nEpoch: 4 \tBatch: 292 \tLoss: 1.4698349237442017\nEpoch: 4 \tBatch: 293 \tLoss: 1.4760044813156128\nEpoch: 4 \tBatch: 294 \tLoss: 1.4731065034866333\nEpoch: 4 \tBatch: 295 \tLoss: 1.4682878255844116\nEpoch: 4 \tBatch: 296 \tLoss: 1.4649126529693604\nEpoch: 4 \tBatch: 297 \tLoss: 1.4702560901641846\nEpoch: 4 \tBatch: 298 \tLoss: 1.4904996156692505\nEpoch: 4 \tBatch: 299 \tLoss: 1.5014293193817139\nEpoch: 4 \tBatch: 300 \tLoss: 1.4906686544418335\nEpoch: 4 \tBatch: 301 \tLoss: 1.4910808801651\nEpoch: 4 \tBatch: 302 \tLoss: 1.487231731414795\nEpoch: 4 \tBatch: 303 \tLoss: 1.4755765199661255\nEpoch: 4 \tBatch: 304 \tLoss: 1.4785317182540894\nEpoch: 4 \tBatch: 305 \tLoss: 1.475743293762207\nEpoch: 4 \tBatch: 306 \tLoss: 1.49003005027771\nEpoch: 4 \tBatch: 307 \tLoss: 1.4927483797073364\nEpoch: 4 \tBatch: 308 \tLoss: 1.4820574522018433\nEpoch: 4 \tBatch: 309 \tLoss: 1.4910742044448853\nEpoch: 4 \tBatch: 310 \tLoss: 1.4646575450897217\nEpoch: 4 \tBatch: 311 \tLoss: 1.4800113439559937\nEpoch: 4 \tBatch: 312 \tLoss: 1.4716084003448486\nEpoch: 4 \tBatch: 313 \tLoss: 1.47919762134552\nEpoch: 4 \tBatch: 314 \tLoss: 1.4818637371063232\nEpoch: 4 \tBatch: 315 \tLoss: 1.4918591976165771\nEpoch: 4 \tBatch: 316 \tLoss: 1.496756672859192\nEpoch: 4 \tBatch: 317 \tLoss: 1.4769320487976074\nEpoch: 4 \tBatch: 318 \tLoss: 1.4833152294158936\nEpoch: 4 \tBatch: 319 \tLoss: 1.4941407442092896\nEpoch: 4 \tBatch: 320 \tLoss: 1.4699193239212036\nEpoch: 4 \tBatch: 321 \tLoss: 1.4832432270050049\nEpoch: 4 \tBatch: 322 \tLoss: 1.477238655090332\nEpoch: 4 \tBatch: 323 \tLoss: 1.4750010967254639\nEpoch: 4 \tBatch: 324 \tLoss: 1.4870818853378296\nEpoch: 4 \tBatch: 325 \tLoss: 1.4701327085494995\nEpoch: 4 \tBatch: 326 \tLoss: 1.5023126602172852\nEpoch: 4 \tBatch: 327 \tLoss: 1.475041389465332\nEpoch: 4 \tBatch: 328 \tLoss: 1.4755538702011108\nEpoch: 4 \tBatch: 329 \tLoss: 1.4822773933410645\nEpoch: 4 \tBatch: 330 \tLoss: 1.5034997463226318\nEpoch: 4 \tBatch: 331 \tLoss: 1.479085922241211\nEpoch: 4 \tBatch: 332 \tLoss: 1.4808720350265503\nEpoch: 4 \tBatch: 333 \tLoss: 1.4796630144119263\nEpoch: 4 \tBatch: 334 \tLoss: 1.489518404006958\nEpoch: 4 \tBatch: 335 \tLoss: 1.4726113080978394\nEpoch: 4 \tBatch: 336 \tLoss: 1.485469102859497\nEpoch: 4 \tBatch: 337 \tLoss: 1.4743094444274902\nEpoch: 4 \tBatch: 338 \tLoss: 1.4840251207351685\nEpoch: 4 \tBatch: 339 \tLoss: 1.514119029045105\nEpoch: 4 \tBatch: 340 \tLoss: 1.4832792282104492\nEpoch: 4 \tBatch: 341 \tLoss: 1.4619609117507935\nEpoch: 4 \tBatch: 342 \tLoss: 1.4768049716949463\nEpoch: 4 \tBatch: 343 \tLoss: 1.4631599187850952\nEpoch: 4 \tBatch: 344 \tLoss: 1.4734818935394287\nEpoch: 4 \tBatch: 345 \tLoss: 1.4742621183395386\nEpoch: 4 \tBatch: 346 \tLoss: 1.4800294637680054\nEpoch: 4 \tBatch: 347 \tLoss: 1.492570161819458\nEpoch: 4 \tBatch: 348 \tLoss: 1.4799178838729858\nEpoch: 4 \tBatch: 349 \tLoss: 1.470324158668518\nEpoch: 4 \tBatch: 350 \tLoss: 1.4714243412017822\nEpoch: 4 \tBatch: 351 \tLoss: 1.477587342262268\nEpoch: 4 \tBatch: 352 \tLoss: 1.4920129776000977\nEpoch: 4 \tBatch: 353 \tLoss: 1.4944401979446411\nEpoch: 4 \tBatch: 354 \tLoss: 1.4767755270004272\nEpoch: 4 \tBatch: 355 \tLoss: 1.4737576246261597\nEpoch: 4 \tBatch: 356 \tLoss: 1.4979974031448364\nEpoch: 4 \tBatch: 357 \tLoss: 1.471489429473877\nEpoch: 4 \tBatch: 358 \tLoss: 1.48371160030365\nEpoch: 4 \tBatch: 359 \tLoss: 1.4774965047836304\nEpoch: 4 \tBatch: 360 \tLoss: 1.4636108875274658\nEpoch: 4 \tBatch: 361 \tLoss: 1.4716037511825562\nEpoch: 4 \tBatch: 362 \tLoss: 1.498110294342041\nEpoch: 4 \tBatch: 363 \tLoss: 1.4794427156448364\nEpoch: 4 \tBatch: 364 \tLoss: 1.4836206436157227\nEpoch: 4 \tBatch: 365 \tLoss: 1.4849570989608765\nEpoch: 4 \tBatch: 366 \tLoss: 1.466896653175354\nEpoch: 4 \tBatch: 367 \tLoss: 1.4866055250167847\nEpoch: 4 \tBatch: 368 \tLoss: 1.4647035598754883\nEpoch: 4 \tBatch: 369 \tLoss: 1.4791001081466675\nEpoch: 4 \tBatch: 370 \tLoss: 1.4759066104888916\nEpoch: 4 \tBatch: 371 \tLoss: 1.4756789207458496\nEpoch: 4 \tBatch: 372 \tLoss: 1.4843366146087646\nEpoch: 4 \tBatch: 373 \tLoss: 1.4867662191390991\nEpoch: 4 \tBatch: 374 \tLoss: 1.477831482887268\nEpoch: 4 \tBatch: 375 \tLoss: 1.4718209505081177\nEpoch: 4 \tBatch: 376 \tLoss: 1.4703038930892944\nEpoch: 4 \tBatch: 377 \tLoss: 1.500519037246704\nEpoch: 4 \tBatch: 378 \tLoss: 1.4696569442749023\nEpoch: 4 \tBatch: 379 \tLoss: 1.4949043989181519\nEpoch: 4 \tBatch: 380 \tLoss: 1.4716135263442993\nEpoch: 4 \tBatch: 381 \tLoss: 1.4798860549926758\nEpoch: 4 \tBatch: 382 \tLoss: 1.4765599966049194\nEpoch: 4 \tBatch: 383 \tLoss: 1.4755804538726807\nEpoch: 4 \tBatch: 384 \tLoss: 1.4825944900512695\nEpoch: 4 \tBatch: 385 \tLoss: 1.4766457080841064\nEpoch: 4 \tBatch: 386 \tLoss: 1.4883297681808472\nEpoch: 4 \tBatch: 387 \tLoss: 1.4633421897888184\nEpoch: 4 \tBatch: 388 \tLoss: 1.4856455326080322\nEpoch: 4 \tBatch: 389 \tLoss: 1.4714586734771729\nEpoch: 4 \tBatch: 390 \tLoss: 1.5058127641677856\nTraining Complete. Final loss = 1.5058127641677856\n"
    }
   ],
   "source": [
    "def train(model, epochs, writer, verbose=True, tag='Loss/Train'):\n",
    "    for epoch in range(epochs):\n",
    "        for idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # pass x through your model to get a prediction\n",
    "            prediction = model(inputs)             # pass the data forward through the model\n",
    "            loss = criterion(prediction, labels)   # compute the cost\n",
    "            if verbose: print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss.item())\n",
    "            optimiser.zero_grad()                  # reset the gradients attribute of all of the model's params to zero\n",
    "            loss.backward()                        # backward pass to compute and store all of the model's param's gradients\n",
    "            optimiser.step()                       # update the model's parameters\n",
    "            \n",
    "            writer.add_scalar(tag, loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "    print('Training Complete. Final loss =',loss.item())\n",
    "    \n",
    "train(cnn, epochs, writer=writer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train Accuracy: 98.652\nValidation Accuracy: 98.19\nTest Accuracy: 98.31\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "            \n",
    "def calc_accuracy(model, dataloader):\n",
    "    num_correct = 0\n",
    "    num_examples = len(dataloader.dataset)                       # test DATA not test LOADER\n",
    "    for inputs, labels in dataloader:                  # for all exampls, over all mini-batches in the test dataset\n",
    "        predictions = model(inputs)\n",
    "        predictions = torch.max(predictions, axis=1)    # reduce to find max indices along direction which column varies\n",
    "        predictions = predictions[1]                    # torch.max returns (values, indices)\n",
    "        num_correct += int(sum(predictions == labels))\n",
    "    percent_correct = num_correct / num_examples * 100\n",
    "    return percent_correct\n",
    "\n",
    "print('Train Accuracy:', calc_accuracy(cnn, train_loader))\n",
    "print('Validation Accuracy:', calc_accuracy(cnn, val_loader))\n",
    "print('Test Accuracy:', calc_accuracy(cnn, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Compare to cnn without batch normalisation <br>\n",
    "Use a larger lr with batch normalisation\n",
    "Add plots to show stability and fast convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "4 \tLoss: 1.4690825939178467\nEpoch: 3 \tBatch: 355 \tLoss: 1.504518985748291\nEpoch: 3 \tBatch: 356 \tLoss: 1.473148226737976\nEpoch: 3 \tBatch: 357 \tLoss: 1.4720790386199951\nEpoch: 3 \tBatch: 358 \tLoss: 1.4752596616744995\nEpoch: 3 \tBatch: 359 \tLoss: 1.462831735610962\nEpoch: 3 \tBatch: 360 \tLoss: 1.4618823528289795\nEpoch: 3 \tBatch: 361 \tLoss: 1.5017436742782593\nEpoch: 3 \tBatch: 362 \tLoss: 1.4638078212738037\nEpoch: 3 \tBatch: 363 \tLoss: 1.4766042232513428\nEpoch: 3 \tBatch: 364 \tLoss: 1.462192177772522\nEpoch: 3 \tBatch: 365 \tLoss: 1.4612401723861694\nEpoch: 3 \tBatch: 366 \tLoss: 1.4644782543182373\nEpoch: 3 \tBatch: 367 \tLoss: 1.4747031927108765\nEpoch: 3 \tBatch: 368 \tLoss: 1.4773313999176025\nEpoch: 3 \tBatch: 369 \tLoss: 1.4613133668899536\nEpoch: 3 \tBatch: 370 \tLoss: 1.46896231174469\nEpoch: 3 \tBatch: 371 \tLoss: 1.4839258193969727\nEpoch: 3 \tBatch: 372 \tLoss: 1.4717891216278076\nEpoch: 3 \tBatch: 373 \tLoss: 1.4728102684020996\nEpoch: 3 \tBatch: 374 \tLoss: 1.4613990783691406\nEpoch: 3 \tBatch: 375 \tLoss: 1.4664561748504639\nEpoch: 3 \tBatch: 376 \tLoss: 1.4691585302352905\nEpoch: 3 \tBatch: 377 \tLoss: 1.4716354608535767\nEpoch: 3 \tBatch: 378 \tLoss: 1.4768370389938354\nEpoch: 3 \tBatch: 379 \tLoss: 1.4704915285110474\nEpoch: 3 \tBatch: 380 \tLoss: 1.5089614391326904\nEpoch: 3 \tBatch: 381 \tLoss: 1.465684413909912\nEpoch: 3 \tBatch: 382 \tLoss: 1.4723973274230957\nEpoch: 3 \tBatch: 383 \tLoss: 1.4773693084716797\nEpoch: 3 \tBatch: 384 \tLoss: 1.4753626585006714\nEpoch: 3 \tBatch: 385 \tLoss: 1.467221736907959\nEpoch: 3 \tBatch: 386 \tLoss: 1.473514199256897\nEpoch: 3 \tBatch: 387 \tLoss: 1.4691200256347656\nEpoch: 3 \tBatch: 388 \tLoss: 1.4808262586593628\nEpoch: 3 \tBatch: 389 \tLoss: 1.4661245346069336\nEpoch: 3 \tBatch: 390 \tLoss: 1.4736868143081665\nEpoch: 4 \tBatch: 0 \tLoss: 1.4699130058288574\nEpoch: 4 \tBatch: 1 \tLoss: 1.4838626384735107\nEpoch: 4 \tBatch: 2 \tLoss: 1.471688985824585\nEpoch: 4 \tBatch: 3 \tLoss: 1.4676172733306885\nEpoch: 4 \tBatch: 4 \tLoss: 1.4636610746383667\nEpoch: 4 \tBatch: 5 \tLoss: 1.478004813194275\nEpoch: 4 \tBatch: 6 \tLoss: 1.4711556434631348\nEpoch: 4 \tBatch: 7 \tLoss: 1.4645925760269165\nEpoch: 4 \tBatch: 8 \tLoss: 1.4789557456970215\nEpoch: 4 \tBatch: 9 \tLoss: 1.4751670360565186\nEpoch: 4 \tBatch: 10 \tLoss: 1.4643656015396118\nEpoch: 4 \tBatch: 11 \tLoss: 1.480182409286499\nEpoch: 4 \tBatch: 12 \tLoss: 1.4676566123962402\nEpoch: 4 \tBatch: 13 \tLoss: 1.4840803146362305\nEpoch: 4 \tBatch: 14 \tLoss: 1.461391806602478\nEpoch: 4 \tBatch: 15 \tLoss: 1.4685808420181274\nEpoch: 4 \tBatch: 16 \tLoss: 1.4934583902359009\nEpoch: 4 \tBatch: 17 \tLoss: 1.4685758352279663\nEpoch: 4 \tBatch: 18 \tLoss: 1.4744129180908203\nEpoch: 4 \tBatch: 19 \tLoss: 1.4769973754882812\nEpoch: 4 \tBatch: 20 \tLoss: 1.482743740081787\nEpoch: 4 \tBatch: 21 \tLoss: 1.469278335571289\nEpoch: 4 \tBatch: 22 \tLoss: 1.4761624336242676\nEpoch: 4 \tBatch: 23 \tLoss: 1.4675010442733765\nEpoch: 4 \tBatch: 24 \tLoss: 1.4674922227859497\nEpoch: 4 \tBatch: 25 \tLoss: 1.4619505405426025\nEpoch: 4 \tBatch: 26 \tLoss: 1.46193528175354\nEpoch: 4 \tBatch: 27 \tLoss: 1.4777666330337524\nEpoch: 4 \tBatch: 28 \tLoss: 1.4804751873016357\nEpoch: 4 \tBatch: 29 \tLoss: 1.4653027057647705\nEpoch: 4 \tBatch: 30 \tLoss: 1.4633760452270508\nEpoch: 4 \tBatch: 31 \tLoss: 1.4680819511413574\nEpoch: 4 \tBatch: 32 \tLoss: 1.4689069986343384\nEpoch: 4 \tBatch: 33 \tLoss: 1.4657325744628906\nEpoch: 4 \tBatch: 34 \tLoss: 1.4644356966018677\nEpoch: 4 \tBatch: 35 \tLoss: 1.466165542602539\nEpoch: 4 \tBatch: 36 \tLoss: 1.4906924962997437\nEpoch: 4 \tBatch: 37 \tLoss: 1.4939953088760376\nEpoch: 4 \tBatch: 38 \tLoss: 1.4694846868515015\nEpoch: 4 \tBatch: 39 \tLoss: 1.4646704196929932\nEpoch: 4 \tBatch: 40 \tLoss: 1.475852608680725\nEpoch: 4 \tBatch: 41 \tLoss: 1.4689927101135254\nEpoch: 4 \tBatch: 42 \tLoss: 1.4795068502426147\nEpoch: 4 \tBatch: 43 \tLoss: 1.4695954322814941\nEpoch: 4 \tBatch: 44 \tLoss: 1.4615166187286377\nEpoch: 4 \tBatch: 45 \tLoss: 1.495262622833252\nEpoch: 4 \tBatch: 46 \tLoss: 1.4785284996032715\nEpoch: 4 \tBatch: 47 \tLoss: 1.4700078964233398\nEpoch: 4 \tBatch: 48 \tLoss: 1.4881207942962646\nEpoch: 4 \tBatch: 49 \tLoss: 1.4686962366104126\nEpoch: 4 \tBatch: 50 \tLoss: 1.4697456359863281\nEpoch: 4 \tBatch: 51 \tLoss: 1.461236596107483\nEpoch: 4 \tBatch: 52 \tLoss: 1.4612071514129639\nEpoch: 4 \tBatch: 53 \tLoss: 1.4611644744873047\nEpoch: 4 \tBatch: 54 \tLoss: 1.479928731918335\nEpoch: 4 \tBatch: 55 \tLoss: 1.4793695211410522\nEpoch: 4 \tBatch: 56 \tLoss: 1.4673962593078613\nEpoch: 4 \tBatch: 57 \tLoss: 1.4690046310424805\nEpoch: 4 \tBatch: 58 \tLoss: 1.4699926376342773\nEpoch: 4 \tBatch: 59 \tLoss: 1.4767377376556396\nEpoch: 4 \tBatch: 60 \tLoss: 1.4774693250656128\nEpoch: 4 \tBatch: 61 \tLoss: 1.477221965789795\nEpoch: 4 \tBatch: 62 \tLoss: 1.4812953472137451\nEpoch: 4 \tBatch: 63 \tLoss: 1.4627491235733032\nEpoch: 4 \tBatch: 64 \tLoss: 1.4613680839538574\nEpoch: 4 \tBatch: 65 \tLoss: 1.4686574935913086\nEpoch: 4 \tBatch: 66 \tLoss: 1.4768086671829224\nEpoch: 4 \tBatch: 67 \tLoss: 1.4785363674163818\nEpoch: 4 \tBatch: 68 \tLoss: 1.4613168239593506\nEpoch: 4 \tBatch: 69 \tLoss: 1.4685108661651611\nEpoch: 4 \tBatch: 70 \tLoss: 1.4694249629974365\nEpoch: 4 \tBatch: 71 \tLoss: 1.4728163480758667\nEpoch: 4 \tBatch: 72 \tLoss: 1.46309232711792\nEpoch: 4 \tBatch: 73 \tLoss: 1.4692015647888184\nEpoch: 4 \tBatch: 74 \tLoss: 1.479949712753296\nEpoch: 4 \tBatch: 75 \tLoss: 1.469700574874878\nEpoch: 4 \tBatch: 76 \tLoss: 1.4612596035003662\nEpoch: 4 \tBatch: 77 \tLoss: 1.4793035984039307\nEpoch: 4 \tBatch: 78 \tLoss: 1.4693228006362915\nEpoch: 4 \tBatch: 79 \tLoss: 1.4792675971984863\nEpoch: 4 \tBatch: 80 \tLoss: 1.4849215745925903\nEpoch: 4 \tBatch: 81 \tLoss: 1.4682481288909912\nEpoch: 4 \tBatch: 82 \tLoss: 1.4690018892288208\nEpoch: 4 \tBatch: 83 \tLoss: 1.4690673351287842\nEpoch: 4 \tBatch: 84 \tLoss: 1.4745638370513916\nEpoch: 4 \tBatch: 85 \tLoss: 1.47182297706604\nEpoch: 4 \tBatch: 86 \tLoss: 1.4708086252212524\nEpoch: 4 \tBatch: 87 \tLoss: 1.4619659185409546\nEpoch: 4 \tBatch: 88 \tLoss: 1.4774954319000244\nEpoch: 4 \tBatch: 89 \tLoss: 1.4803802967071533\nEpoch: 4 \tBatch: 90 \tLoss: 1.483511209487915\nEpoch: 4 \tBatch: 91 \tLoss: 1.4859808683395386\nEpoch: 4 \tBatch: 92 \tLoss: 1.4698293209075928\nEpoch: 4 \tBatch: 93 \tLoss: 1.4761143922805786\nEpoch: 4 \tBatch: 94 \tLoss: 1.4689388275146484\nEpoch: 4 \tBatch: 95 \tLoss: 1.4627180099487305\nEpoch: 4 \tBatch: 96 \tLoss: 1.4695606231689453\nEpoch: 4 \tBatch: 97 \tLoss: 1.4616076946258545\nEpoch: 4 \tBatch: 98 \tLoss: 1.469222068786621\nEpoch: 4 \tBatch: 99 \tLoss: 1.4659030437469482\nEpoch: 4 \tBatch: 100 \tLoss: 1.4774811267852783\nEpoch: 4 \tBatch: 101 \tLoss: 1.462805151939392\nEpoch: 4 \tBatch: 102 \tLoss: 1.5010868310928345\nEpoch: 4 \tBatch: 103 \tLoss: 1.4621175527572632\nEpoch: 4 \tBatch: 104 \tLoss: 1.4704639911651611\nEpoch: 4 \tBatch: 105 \tLoss: 1.477354645729065\nEpoch: 4 \tBatch: 106 \tLoss: 1.4693632125854492\nEpoch: 4 \tBatch: 107 \tLoss: 1.4661622047424316\nEpoch: 4 \tBatch: 108 \tLoss: 1.4770097732543945\nEpoch: 4 \tBatch: 109 \tLoss: 1.4723104238510132\nEpoch: 4 \tBatch: 110 \tLoss: 1.4674592018127441\nEpoch: 4 \tBatch: 111 \tLoss: 1.46139395236969\nEpoch: 4 \tBatch: 112 \tLoss: 1.4811064004898071\nEpoch: 4 \tBatch: 113 \tLoss: 1.486756443977356\nEpoch: 4 \tBatch: 114 \tLoss: 1.467606782913208\nEpoch: 4 \tBatch: 115 \tLoss: 1.4676512479782104\nEpoch: 4 \tBatch: 116 \tLoss: 1.4773355722427368\nEpoch: 4 \tBatch: 117 \tLoss: 1.4760768413543701\nEpoch: 4 \tBatch: 118 \tLoss: 1.4775700569152832\nEpoch: 4 \tBatch: 119 \tLoss: 1.4771324396133423\nEpoch: 4 \tBatch: 120 \tLoss: 1.4685113430023193\nEpoch: 4 \tBatch: 121 \tLoss: 1.468449592590332\nEpoch: 4 \tBatch: 122 \tLoss: 1.4780380725860596\nEpoch: 4 \tBatch: 123 \tLoss: 1.489159107208252\nEpoch: 4 \tBatch: 124 \tLoss: 1.4674110412597656\nEpoch: 4 \tBatch: 125 \tLoss: 1.4842530488967896\nEpoch: 4 \tBatch: 126 \tLoss: 1.471152663230896\nEpoch: 4 \tBatch: 127 \tLoss: 1.469123363494873\nEpoch: 4 \tBatch: 128 \tLoss: 1.4997090101242065\nEpoch: 4 \tBatch: 129 \tLoss: 1.4694700241088867\nEpoch: 4 \tBatch: 130 \tLoss: 1.472432255744934\nEpoch: 4 \tBatch: 131 \tLoss: 1.4623942375183105\nEpoch: 4 \tBatch: 132 \tLoss: 1.4697339534759521\nEpoch: 4 \tBatch: 133 \tLoss: 1.461787462234497\nEpoch: 4 \tBatch: 134 \tLoss: 1.4769164323806763\nEpoch: 4 \tBatch: 135 \tLoss: 1.461242914199829\nEpoch: 4 \tBatch: 136 \tLoss: 1.470884919166565\nEpoch: 4 \tBatch: 137 \tLoss: 1.4617632627487183\nEpoch: 4 \tBatch: 138 \tLoss: 1.4739816188812256\nEpoch: 4 \tBatch: 139 \tLoss: 1.4773482084274292\nEpoch: 4 \tBatch: 140 \tLoss: 1.4779232740402222\nEpoch: 4 \tBatch: 141 \tLoss: 1.47685968875885\nEpoch: 4 \tBatch: 142 \tLoss: 1.4892579317092896\nEpoch: 4 \tBatch: 143 \tLoss: 1.4767873287200928\nEpoch: 4 \tBatch: 144 \tLoss: 1.4769827127456665\nEpoch: 4 \tBatch: 145 \tLoss: 1.4620029926300049\nEpoch: 4 \tBatch: 146 \tLoss: 1.4622459411621094\nEpoch: 4 \tBatch: 147 \tLoss: 1.4621855020523071\nEpoch: 4 \tBatch: 148 \tLoss: 1.4611531496047974\nEpoch: 4 \tBatch: 149 \tLoss: 1.4842525720596313\nEpoch: 4 \tBatch: 150 \tLoss: 1.48173987865448\nEpoch: 4 \tBatch: 151 \tLoss: 1.473522424697876\nEpoch: 4 \tBatch: 152 \tLoss: 1.464881420135498\nEpoch: 4 \tBatch: 153 \tLoss: 1.4684418439865112\nEpoch: 4 \tBatch: 154 \tLoss: 1.48698890209198\nEpoch: 4 \tBatch: 155 \tLoss: 1.4890031814575195\nEpoch: 4 \tBatch: 156 \tLoss: 1.4691966772079468\nEpoch: 4 \tBatch: 157 \tLoss: 1.4693423509597778\nEpoch: 4 \tBatch: 158 \tLoss: 1.492142677307129\nEpoch: 4 \tBatch: 159 \tLoss: 1.4642558097839355\nEpoch: 4 \tBatch: 160 \tLoss: 1.4643359184265137\nEpoch: 4 \tBatch: 161 \tLoss: 1.4702855348587036\nEpoch: 4 \tBatch: 162 \tLoss: 1.4722546339035034\nEpoch: 4 \tBatch: 163 \tLoss: 1.4688868522644043\nEpoch: 4 \tBatch: 164 \tLoss: 1.4613252878189087\nEpoch: 4 \tBatch: 165 \tLoss: 1.4695382118225098\nEpoch: 4 \tBatch: 166 \tLoss: 1.497280240058899\nEpoch: 4 \tBatch: 167 \tLoss: 1.4615024328231812\nEpoch: 4 \tBatch: 168 \tLoss: 1.4612292051315308\nEpoch: 4 \tBatch: 169 \tLoss: 1.4715615510940552\nEpoch: 4 \tBatch: 170 \tLoss: 1.4767059087753296\nEpoch: 4 \tBatch: 171 \tLoss: 1.4694799184799194\nEpoch: 4 \tBatch: 172 \tLoss: 1.465155005455017\nEpoch: 4 \tBatch: 173 \tLoss: 1.465867280960083\nEpoch: 4 \tBatch: 174 \tLoss: 1.4705058336257935\nEpoch: 4 \tBatch: 175 \tLoss: 1.4690788984298706\nEpoch: 4 \tBatch: 176 \tLoss: 1.4613240957260132\nEpoch: 4 \tBatch: 177 \tLoss: 1.4841556549072266\nEpoch: 4 \tBatch: 178 \tLoss: 1.4644156694412231\nEpoch: 4 \tBatch: 179 \tLoss: 1.4721194505691528\nEpoch: 4 \tBatch: 180 \tLoss: 1.47622549533844\nEpoch: 4 \tBatch: 181 \tLoss: 1.4690901041030884\nEpoch: 4 \tBatch: 182 \tLoss: 1.4619232416152954\nEpoch: 4 \tBatch: 183 \tLoss: 1.4691555500030518\nEpoch: 4 \tBatch: 184 \tLoss: 1.4945006370544434\nEpoch: 4 \tBatch: 185 \tLoss: 1.4787330627441406\nEpoch: 4 \tBatch: 186 \tLoss: 1.462217092514038\nEpoch: 4 \tBatch: 187 \tLoss: 1.4761205911636353\nEpoch: 4 \tBatch: 188 \tLoss: 1.4632130861282349\nEpoch: 4 \tBatch: 189 \tLoss: 1.4611743688583374\nEpoch: 4 \tBatch: 190 \tLoss: 1.4765255451202393\nEpoch: 4 \tBatch: 191 \tLoss: 1.4613041877746582\nEpoch: 4 \tBatch: 192 \tLoss: 1.4923369884490967\nEpoch: 4 \tBatch: 193 \tLoss: 1.4665378332138062\nEpoch: 4 \tBatch: 194 \tLoss: 1.4689675569534302\nEpoch: 4 \tBatch: 195 \tLoss: 1.4717352390289307\nEpoch: 4 \tBatch: 196 \tLoss: 1.468907117843628\nEpoch: 4 \tBatch: 197 \tLoss: 1.4611769914627075\nEpoch: 4 \tBatch: 198 \tLoss: 1.4611512422561646\nEpoch: 4 \tBatch: 199 \tLoss: 1.4613571166992188\nEpoch: 4 \tBatch: 200 \tLoss: 1.461219310760498\nEpoch: 4 \tBatch: 201 \tLoss: 1.4762026071548462\nEpoch: 4 \tBatch: 202 \tLoss: 1.469177007675171\nEpoch: 4 \tBatch: 203 \tLoss: 1.4667623043060303\nEpoch: 4 \tBatch: 204 \tLoss: 1.4690934419631958\nEpoch: 4 \tBatch: 205 \tLoss: 1.4689911603927612\nEpoch: 4 \tBatch: 206 \tLoss: 1.4615137577056885\nEpoch: 4 \tBatch: 207 \tLoss: 1.4892746210098267\nEpoch: 4 \tBatch: 208 \tLoss: 1.4691786766052246\nEpoch: 4 \tBatch: 209 \tLoss: 1.469356656074524\nEpoch: 4 \tBatch: 210 \tLoss: 1.4615710973739624\nEpoch: 4 \tBatch: 211 \tLoss: 1.4689650535583496\nEpoch: 4 \tBatch: 212 \tLoss: 1.467437982559204\nEpoch: 4 \tBatch: 213 \tLoss: 1.4691240787506104\nEpoch: 4 \tBatch: 214 \tLoss: 1.461156964302063\nEpoch: 4 \tBatch: 215 \tLoss: 1.4736698865890503\nEpoch: 4 \tBatch: 216 \tLoss: 1.4846349954605103\nEpoch: 4 \tBatch: 217 \tLoss: 1.4617176055908203\nEpoch: 4 \tBatch: 218 \tLoss: 1.4627619981765747\nEpoch: 4 \tBatch: 219 \tLoss: 1.4621728658676147\nEpoch: 4 \tBatch: 220 \tLoss: 1.4618375301361084\nEpoch: 4 \tBatch: 221 \tLoss: 1.4759891033172607\nEpoch: 4 \tBatch: 222 \tLoss: 1.4689202308654785\nEpoch: 4 \tBatch: 223 \tLoss: 1.4676947593688965\nEpoch: 4 \tBatch: 224 \tLoss: 1.4690988063812256\nEpoch: 4 \tBatch: 225 \tLoss: 1.47231924533844\nEpoch: 4 \tBatch: 226 \tLoss: 1.4620475769042969\nEpoch: 4 \tBatch: 227 \tLoss: 1.4953114986419678\nEpoch: 4 \tBatch: 228 \tLoss: 1.4675370454788208\nEpoch: 4 \tBatch: 229 \tLoss: 1.461158275604248\nEpoch: 4 \tBatch: 230 \tLoss: 1.4611520767211914\nEpoch: 4 \tBatch: 231 \tLoss: 1.463156819343567\nEpoch: 4 \tBatch: 232 \tLoss: 1.476783275604248\nEpoch: 4 \tBatch: 233 \tLoss: 1.4612035751342773\nEpoch: 4 \tBatch: 234 \tLoss: 1.4690333604812622\nEpoch: 4 \tBatch: 235 \tLoss: 1.4799418449401855\nEpoch: 4 \tBatch: 236 \tLoss: 1.4643752574920654\nEpoch: 4 \tBatch: 237 \tLoss: 1.4639919996261597\nEpoch: 4 \tBatch: 238 \tLoss: 1.461279034614563\nEpoch: 4 \tBatch: 239 \tLoss: 1.468970775604248\nEpoch: 4 \tBatch: 240 \tLoss: 1.461498498916626\nEpoch: 4 \tBatch: 241 \tLoss: 1.4665480852127075\nEpoch: 4 \tBatch: 242 \tLoss: 1.4767025709152222\nEpoch: 4 \tBatch: 243 \tLoss: 1.4691259860992432\nEpoch: 4 \tBatch: 244 \tLoss: 1.4733442068099976\nEpoch: 4 \tBatch: 245 \tLoss: 1.4773855209350586\nEpoch: 4 \tBatch: 246 \tLoss: 1.4624037742614746\nEpoch: 4 \tBatch: 247 \tLoss: 1.4636276960372925\nEpoch: 4 \tBatch: 248 \tLoss: 1.4772310256958008\nEpoch: 4 \tBatch: 249 \tLoss: 1.4634255170822144\nEpoch: 4 \tBatch: 250 \tLoss: 1.4809635877609253\nEpoch: 4 \tBatch: 251 \tLoss: 1.4692387580871582\nEpoch: 4 \tBatch: 252 \tLoss: 1.4799522161483765\nEpoch: 4 \tBatch: 253 \tLoss: 1.4685442447662354\nEpoch: 4 \tBatch: 254 \tLoss: 1.4681220054626465\nEpoch: 4 \tBatch: 255 \tLoss: 1.483364462852478\nEpoch: 4 \tBatch: 256 \tLoss: 1.467991828918457\nEpoch: 4 \tBatch: 257 \tLoss: 1.469150424003601\nEpoch: 4 \tBatch: 258 \tLoss: 1.4843155145645142\nEpoch: 4 \tBatch: 259 \tLoss: 1.470763087272644\nEpoch: 4 \tBatch: 260 \tLoss: 1.471771001815796\nEpoch: 4 \tBatch: 261 \tLoss: 1.4611790180206299\nEpoch: 4 \tBatch: 262 \tLoss: 1.4842395782470703\nEpoch: 4 \tBatch: 263 \tLoss: 1.4829992055892944\nEpoch: 4 \tBatch: 264 \tLoss: 1.47947096824646\nEpoch: 4 \tBatch: 265 \tLoss: 1.482795000076294\nEpoch: 4 \tBatch: 266 \tLoss: 1.4847396612167358\nEpoch: 4 \tBatch: 267 \tLoss: 1.4613432884216309\nEpoch: 4 \tBatch: 268 \tLoss: 1.4664666652679443\nEpoch: 4 \tBatch: 269 \tLoss: 1.4686102867126465\nEpoch: 4 \tBatch: 270 \tLoss: 1.4615520238876343\nEpoch: 4 \tBatch: 271 \tLoss: 1.4704476594924927\nEpoch: 4 \tBatch: 272 \tLoss: 1.48435640335083\nEpoch: 4 \tBatch: 273 \tLoss: 1.4693117141723633\nEpoch: 4 \tBatch: 274 \tLoss: 1.4675071239471436\nEpoch: 4 \tBatch: 275 \tLoss: 1.4685324430465698\nEpoch: 4 \tBatch: 276 \tLoss: 1.4694327116012573\nEpoch: 4 \tBatch: 277 \tLoss: 1.4791834354400635\nEpoch: 4 \tBatch: 278 \tLoss: 1.4770764112472534\nEpoch: 4 \tBatch: 279 \tLoss: 1.4625591039657593\nEpoch: 4 \tBatch: 280 \tLoss: 1.4690533876419067\nEpoch: 4 \tBatch: 281 \tLoss: 1.4716287851333618\nEpoch: 4 \tBatch: 282 \tLoss: 1.4827488660812378\nEpoch: 4 \tBatch: 283 \tLoss: 1.4611577987670898\nEpoch: 4 \tBatch: 284 \tLoss: 1.4842593669891357\nEpoch: 4 \tBatch: 285 \tLoss: 1.4689908027648926\nEpoch: 4 \tBatch: 286 \tLoss: 1.4689773321151733\nEpoch: 4 \tBatch: 287 \tLoss: 1.462822675704956\nEpoch: 4 \tBatch: 288 \tLoss: 1.473617672920227\nEpoch: 4 \tBatch: 289 \tLoss: 1.4912011623382568\nEpoch: 4 \tBatch: 290 \tLoss: 1.4620788097381592\nEpoch: 4 \tBatch: 291 \tLoss: 1.4644203186035156\nEpoch: 4 \tBatch: 292 \tLoss: 1.475767731666565\nEpoch: 4 \tBatch: 293 \tLoss: 1.4706635475158691\nEpoch: 4 \tBatch: 294 \tLoss: 1.4701836109161377\nEpoch: 4 \tBatch: 295 \tLoss: 1.4677983522415161\nEpoch: 4 \tBatch: 296 \tLoss: 1.4612315893173218\nEpoch: 4 \tBatch: 297 \tLoss: 1.4940866231918335\nEpoch: 4 \tBatch: 298 \tLoss: 1.4611667394638062\nEpoch: 4 \tBatch: 299 \tLoss: 1.4633628129959106\nEpoch: 4 \tBatch: 300 \tLoss: 1.4620553255081177\nEpoch: 4 \tBatch: 301 \tLoss: 1.4855360984802246\nEpoch: 4 \tBatch: 302 \tLoss: 1.4665149450302124\nEpoch: 4 \tBatch: 303 \tLoss: 1.4688091278076172\nEpoch: 4 \tBatch: 304 \tLoss: 1.4643123149871826\nEpoch: 4 \tBatch: 305 \tLoss: 1.4853566884994507\nEpoch: 4 \tBatch: 306 \tLoss: 1.4611834287643433\nEpoch: 4 \tBatch: 307 \tLoss: 1.4696929454803467\nEpoch: 4 \tBatch: 308 \tLoss: 1.4769132137298584\nEpoch: 4 \tBatch: 309 \tLoss: 1.4745222330093384\nEpoch: 4 \tBatch: 310 \tLoss: 1.4780049324035645\nEpoch: 4 \tBatch: 311 \tLoss: 1.461251139640808\nEpoch: 4 \tBatch: 312 \tLoss: 1.4873777627944946\nEpoch: 4 \tBatch: 313 \tLoss: 1.4689643383026123\nEpoch: 4 \tBatch: 314 \tLoss: 1.4788435697555542\nEpoch: 4 \tBatch: 315 \tLoss: 1.468051791191101\nEpoch: 4 \tBatch: 316 \tLoss: 1.4616084098815918\nEpoch: 4 \tBatch: 317 \tLoss: 1.4611619710922241\nEpoch: 4 \tBatch: 318 \tLoss: 1.4712765216827393\nEpoch: 4 \tBatch: 319 \tLoss: 1.4616166353225708\nEpoch: 4 \tBatch: 320 \tLoss: 1.461341142654419\nEpoch: 4 \tBatch: 321 \tLoss: 1.467482328414917\nEpoch: 4 \tBatch: 322 \tLoss: 1.4689723253250122\nEpoch: 4 \tBatch: 323 \tLoss: 1.4647250175476074\nEpoch: 4 \tBatch: 324 \tLoss: 1.469618797302246\nEpoch: 4 \tBatch: 325 \tLoss: 1.4766874313354492\nEpoch: 4 \tBatch: 326 \tLoss: 1.461493968963623\nEpoch: 4 \tBatch: 327 \tLoss: 1.4716829061508179\nEpoch: 4 \tBatch: 328 \tLoss: 1.4623162746429443\nEpoch: 4 \tBatch: 329 \tLoss: 1.496612787246704\nEpoch: 4 \tBatch: 330 \tLoss: 1.479475975036621\nEpoch: 4 \tBatch: 331 \tLoss: 1.4691660404205322\nEpoch: 4 \tBatch: 332 \tLoss: 1.4687443971633911\nEpoch: 4 \tBatch: 333 \tLoss: 1.477345585823059\nEpoch: 4 \tBatch: 334 \tLoss: 1.4693275690078735\nEpoch: 4 \tBatch: 335 \tLoss: 1.4804340600967407\nEpoch: 4 \tBatch: 336 \tLoss: 1.4689698219299316\nEpoch: 4 \tBatch: 337 \tLoss: 1.47149658203125\nEpoch: 4 \tBatch: 338 \tLoss: 1.472541093826294\nEpoch: 4 \tBatch: 339 \tLoss: 1.4784947633743286\nEpoch: 4 \tBatch: 340 \tLoss: 1.4777172803878784\nEpoch: 4 \tBatch: 341 \tLoss: 1.4692388772964478\nEpoch: 4 \tBatch: 342 \tLoss: 1.4697353839874268\nEpoch: 4 \tBatch: 343 \tLoss: 1.4778484106063843\nEpoch: 4 \tBatch: 344 \tLoss: 1.4697940349578857\nEpoch: 4 \tBatch: 345 \tLoss: 1.4749778509140015\nEpoch: 4 \tBatch: 346 \tLoss: 1.4632304906845093\nEpoch: 4 \tBatch: 347 \tLoss: 1.4692015647888184\nEpoch: 4 \tBatch: 348 \tLoss: 1.4690388441085815\nEpoch: 4 \tBatch: 349 \tLoss: 1.4710893630981445\nEpoch: 4 \tBatch: 350 \tLoss: 1.469031572341919\nEpoch: 4 \tBatch: 351 \tLoss: 1.4611644744873047\nEpoch: 4 \tBatch: 352 \tLoss: 1.4632385969161987\nEpoch: 4 \tBatch: 353 \tLoss: 1.4615700244903564\nEpoch: 4 \tBatch: 354 \tLoss: 1.4858503341674805\nEpoch: 4 \tBatch: 355 \tLoss: 1.4613227844238281\nEpoch: 4 \tBatch: 356 \tLoss: 1.4612104892730713\nEpoch: 4 \tBatch: 357 \tLoss: 1.468969702720642\nEpoch: 4 \tBatch: 358 \tLoss: 1.4613193273544312\nEpoch: 4 \tBatch: 359 \tLoss: 1.467898964881897\nEpoch: 4 \tBatch: 360 \tLoss: 1.469333291053772\nEpoch: 4 \tBatch: 361 \tLoss: 1.468969464302063\nEpoch: 4 \tBatch: 362 \tLoss: 1.4939981698989868\nEpoch: 4 \tBatch: 363 \tLoss: 1.468963384628296\nEpoch: 4 \tBatch: 364 \tLoss: 1.4619218111038208\nEpoch: 4 \tBatch: 365 \tLoss: 1.4769625663757324\nEpoch: 4 \tBatch: 366 \tLoss: 1.4780281782150269\nEpoch: 4 \tBatch: 367 \tLoss: 1.477011799812317\nEpoch: 4 \tBatch: 368 \tLoss: 1.4685003757476807\nEpoch: 4 \tBatch: 369 \tLoss: 1.4893484115600586\nEpoch: 4 \tBatch: 370 \tLoss: 1.4617626667022705\nEpoch: 4 \tBatch: 371 \tLoss: 1.4616910219192505\nEpoch: 4 \tBatch: 372 \tLoss: 1.4690967798233032\nEpoch: 4 \tBatch: 373 \tLoss: 1.4624354839324951\nEpoch: 4 \tBatch: 374 \tLoss: 1.4689794778823853\nEpoch: 4 \tBatch: 375 \tLoss: 1.4612016677856445\nEpoch: 4 \tBatch: 376 \tLoss: 1.4767509698867798\nEpoch: 4 \tBatch: 377 \tLoss: 1.4635740518569946\nEpoch: 4 \tBatch: 378 \tLoss: 1.4718090295791626\nEpoch: 4 \tBatch: 379 \tLoss: 1.4688917398452759\nEpoch: 4 \tBatch: 380 \tLoss: 1.4612746238708496\nEpoch: 4 \tBatch: 381 \tLoss: 1.4860221147537231\nEpoch: 4 \tBatch: 382 \tLoss: 1.4834476709365845\nEpoch: 4 \tBatch: 383 \tLoss: 1.474955677986145\nEpoch: 4 \tBatch: 384 \tLoss: 1.4612358808517456\nEpoch: 4 \tBatch: 385 \tLoss: 1.4689176082611084\nEpoch: 4 \tBatch: 386 \tLoss: 1.470223307609558\nEpoch: 4 \tBatch: 387 \tLoss: 1.4611603021621704\nEpoch: 4 \tBatch: 388 \tLoss: 1.4769072532653809\nEpoch: 4 \tBatch: 389 \tLoss: 1.464343547821045\nEpoch: 4 \tBatch: 390 \tLoss: 1.4611542224884033\nTraining Complete. Final loss = 1.4611542224884033\nTrain Accuracy: 99.316\nValidation Accuracy: 98.54\nTest Accuracy: 98.63\n"
    }
   ],
   "source": [
    "#Set argument batch_norm to True\n",
    "cnn2 = ConvNet(batch_norm=True).to(device)\n",
    "optimiser = torch.optim.Adam(cnn2.parameters(), lr=learning_rate)\n",
    "writer2 = SummaryWriter(log_dir=\"runs/cnn_bn_lr0005\")\n",
    "train(cnn2, epochs, writer=writer2)\n",
    "print('Train Accuracy:', calc_accuracy(cnn2, train_loader))\n",
    "print('Validation Accuracy:', calc_accuracy(cnn2, val_loader))\n",
    "print('Test Accuracy:', calc_accuracy(cnn2, test_loader))"
   ]
  },
  {
   "source": [
    "From the plot in tensorboard, you should see that the convergence happens a lot earlier. The model therefore seems to be more robust from the beginning. Next, you can try using a higher learing rate. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ": 1.4697396755218506\nEpoch: 3 \tBatch: 355 \tLoss: 1.4726462364196777\nEpoch: 3 \tBatch: 356 \tLoss: 1.4646005630493164\nEpoch: 3 \tBatch: 357 \tLoss: 1.4698542356491089\nEpoch: 3 \tBatch: 358 \tLoss: 1.4690487384796143\nEpoch: 3 \tBatch: 359 \tLoss: 1.4615857601165771\nEpoch: 3 \tBatch: 360 \tLoss: 1.4766308069229126\nEpoch: 3 \tBatch: 361 \tLoss: 1.4773776531219482\nEpoch: 3 \tBatch: 362 \tLoss: 1.4771806001663208\nEpoch: 3 \tBatch: 363 \tLoss: 1.4713208675384521\nEpoch: 3 \tBatch: 364 \tLoss: 1.46147620677948\nEpoch: 3 \tBatch: 365 \tLoss: 1.4868123531341553\nEpoch: 3 \tBatch: 366 \tLoss: 1.465869426727295\nEpoch: 3 \tBatch: 367 \tLoss: 1.486151099205017\nEpoch: 3 \tBatch: 368 \tLoss: 1.4854943752288818\nEpoch: 3 \tBatch: 369 \tLoss: 1.469657301902771\nEpoch: 3 \tBatch: 370 \tLoss: 1.4883553981781006\nEpoch: 3 \tBatch: 371 \tLoss: 1.476812481880188\nEpoch: 3 \tBatch: 372 \tLoss: 1.4651477336883545\nEpoch: 3 \tBatch: 373 \tLoss: 1.4831621646881104\nEpoch: 3 \tBatch: 374 \tLoss: 1.474079966545105\nEpoch: 3 \tBatch: 375 \tLoss: 1.4694504737854004\nEpoch: 3 \tBatch: 376 \tLoss: 1.4689915180206299\nEpoch: 3 \tBatch: 377 \tLoss: 1.4628417491912842\nEpoch: 3 \tBatch: 378 \tLoss: 1.4721732139587402\nEpoch: 3 \tBatch: 379 \tLoss: 1.461173415184021\nEpoch: 3 \tBatch: 380 \tLoss: 1.476733684539795\nEpoch: 3 \tBatch: 381 \tLoss: 1.4778070449829102\nEpoch: 3 \tBatch: 382 \tLoss: 1.4703187942504883\nEpoch: 3 \tBatch: 383 \tLoss: 1.465504765510559\nEpoch: 3 \tBatch: 384 \tLoss: 1.4617125988006592\nEpoch: 3 \tBatch: 385 \tLoss: 1.4852380752563477\nEpoch: 3 \tBatch: 386 \tLoss: 1.4768805503845215\nEpoch: 3 \tBatch: 387 \tLoss: 1.4693593978881836\nEpoch: 3 \tBatch: 388 \tLoss: 1.4808423519134521\nEpoch: 3 \tBatch: 389 \tLoss: 1.4614760875701904\nEpoch: 3 \tBatch: 390 \tLoss: 1.4766286611557007\nEpoch: 4 \tBatch: 0 \tLoss: 1.4946300983428955\nEpoch: 4 \tBatch: 1 \tLoss: 1.4756815433502197\nEpoch: 4 \tBatch: 2 \tLoss: 1.4645198583602905\nEpoch: 4 \tBatch: 3 \tLoss: 1.4674358367919922\nEpoch: 4 \tBatch: 4 \tLoss: 1.4629038572311401\nEpoch: 4 \tBatch: 5 \tLoss: 1.4690780639648438\nEpoch: 4 \tBatch: 6 \tLoss: 1.4766216278076172\nEpoch: 4 \tBatch: 7 \tLoss: 1.4778242111206055\nEpoch: 4 \tBatch: 8 \tLoss: 1.4890940189361572\nEpoch: 4 \tBatch: 9 \tLoss: 1.468136191368103\nEpoch: 4 \tBatch: 10 \tLoss: 1.4751944541931152\nEpoch: 4 \tBatch: 11 \tLoss: 1.469606876373291\nEpoch: 4 \tBatch: 12 \tLoss: 1.465159296989441\nEpoch: 4 \tBatch: 13 \tLoss: 1.468285083770752\nEpoch: 4 \tBatch: 14 \tLoss: 1.467079758644104\nEpoch: 4 \tBatch: 15 \tLoss: 1.4649344682693481\nEpoch: 4 \tBatch: 16 \tLoss: 1.4621703624725342\nEpoch: 4 \tBatch: 17 \tLoss: 1.4741432666778564\nEpoch: 4 \tBatch: 18 \tLoss: 1.4723931550979614\nEpoch: 4 \tBatch: 19 \tLoss: 1.4666568040847778\nEpoch: 4 \tBatch: 20 \tLoss: 1.461679220199585\nEpoch: 4 \tBatch: 21 \tLoss: 1.4690485000610352\nEpoch: 4 \tBatch: 22 \tLoss: 1.4649487733840942\nEpoch: 4 \tBatch: 23 \tLoss: 1.4620484113693237\nEpoch: 4 \tBatch: 24 \tLoss: 1.4642565250396729\nEpoch: 4 \tBatch: 25 \tLoss: 1.4715123176574707\nEpoch: 4 \tBatch: 26 \tLoss: 1.4619181156158447\nEpoch: 4 \tBatch: 27 \tLoss: 1.4732171297073364\nEpoch: 4 \tBatch: 28 \tLoss: 1.4611576795578003\nEpoch: 4 \tBatch: 29 \tLoss: 1.472307562828064\nEpoch: 4 \tBatch: 30 \tLoss: 1.4675097465515137\nEpoch: 4 \tBatch: 31 \tLoss: 1.465914011001587\nEpoch: 4 \tBatch: 32 \tLoss: 1.4703510999679565\nEpoch: 4 \tBatch: 33 \tLoss: 1.4689505100250244\nEpoch: 4 \tBatch: 34 \tLoss: 1.4676121473312378\nEpoch: 4 \tBatch: 35 \tLoss: 1.4615750312805176\nEpoch: 4 \tBatch: 36 \tLoss: 1.4736576080322266\nEpoch: 4 \tBatch: 37 \tLoss: 1.4841638803482056\nEpoch: 4 \tBatch: 38 \tLoss: 1.4769474267959595\nEpoch: 4 \tBatch: 39 \tLoss: 1.4695994853973389\nEpoch: 4 \tBatch: 40 \tLoss: 1.4614397287368774\nEpoch: 4 \tBatch: 41 \tLoss: 1.4723126888275146\nEpoch: 4 \tBatch: 42 \tLoss: 1.4618678092956543\nEpoch: 4 \tBatch: 43 \tLoss: 1.466623067855835\nEpoch: 4 \tBatch: 44 \tLoss: 1.4924551248550415\nEpoch: 4 \tBatch: 45 \tLoss: 1.4651193618774414\nEpoch: 4 \tBatch: 46 \tLoss: 1.4689021110534668\nEpoch: 4 \tBatch: 47 \tLoss: 1.5004469156265259\nEpoch: 4 \tBatch: 48 \tLoss: 1.4699867963790894\nEpoch: 4 \tBatch: 49 \tLoss: 1.4754434823989868\nEpoch: 4 \tBatch: 50 \tLoss: 1.4743071794509888\nEpoch: 4 \tBatch: 51 \tLoss: 1.4915664196014404\nEpoch: 4 \tBatch: 52 \tLoss: 1.4863369464874268\nEpoch: 4 \tBatch: 53 \tLoss: 1.491308569908142\nEpoch: 4 \tBatch: 54 \tLoss: 1.4830331802368164\nEpoch: 4 \tBatch: 55 \tLoss: 1.4729843139648438\nEpoch: 4 \tBatch: 56 \tLoss: 1.4617193937301636\nEpoch: 4 \tBatch: 57 \tLoss: 1.490458607673645\nEpoch: 4 \tBatch: 58 \tLoss: 1.4612653255462646\nEpoch: 4 \tBatch: 59 \tLoss: 1.4635471105575562\nEpoch: 4 \tBatch: 60 \tLoss: 1.5005642175674438\nEpoch: 4 \tBatch: 61 \tLoss: 1.478166103363037\nEpoch: 4 \tBatch: 62 \tLoss: 1.4645384550094604\nEpoch: 4 \tBatch: 63 \tLoss: 1.4683789014816284\nEpoch: 4 \tBatch: 64 \tLoss: 1.4814995527267456\nEpoch: 4 \tBatch: 65 \tLoss: 1.4691424369812012\nEpoch: 4 \tBatch: 66 \tLoss: 1.4805750846862793\nEpoch: 4 \tBatch: 67 \tLoss: 1.469759464263916\nEpoch: 4 \tBatch: 68 \tLoss: 1.4624491930007935\nEpoch: 4 \tBatch: 69 \tLoss: 1.4693939685821533\nEpoch: 4 \tBatch: 70 \tLoss: 1.4723167419433594\nEpoch: 4 \tBatch: 71 \tLoss: 1.4689658880233765\nEpoch: 4 \tBatch: 72 \tLoss: 1.4615838527679443\nEpoch: 4 \tBatch: 73 \tLoss: 1.463057279586792\nEpoch: 4 \tBatch: 74 \tLoss: 1.468964695930481\nEpoch: 4 \tBatch: 75 \tLoss: 1.462422490119934\nEpoch: 4 \tBatch: 76 \tLoss: 1.46245276927948\nEpoch: 4 \tBatch: 77 \tLoss: 1.4696124792099\nEpoch: 4 \tBatch: 78 \tLoss: 1.4757152795791626\nEpoch: 4 \tBatch: 79 \tLoss: 1.462104082107544\nEpoch: 4 \tBatch: 80 \tLoss: 1.4766284227371216\nEpoch: 4 \tBatch: 81 \tLoss: 1.4674217700958252\nEpoch: 4 \tBatch: 82 \tLoss: 1.4787399768829346\nEpoch: 4 \tBatch: 83 \tLoss: 1.4824423789978027\nEpoch: 4 \tBatch: 84 \tLoss: 1.4921530485153198\nEpoch: 4 \tBatch: 85 \tLoss: 1.466067910194397\nEpoch: 4 \tBatch: 86 \tLoss: 1.4774409532546997\nEpoch: 4 \tBatch: 87 \tLoss: 1.475122332572937\nEpoch: 4 \tBatch: 88 \tLoss: 1.4615236520767212\nEpoch: 4 \tBatch: 89 \tLoss: 1.4736579656600952\nEpoch: 4 \tBatch: 90 \tLoss: 1.4716259241104126\nEpoch: 4 \tBatch: 91 \tLoss: 1.4621551036834717\nEpoch: 4 \tBatch: 92 \tLoss: 1.478488802909851\nEpoch: 4 \tBatch: 93 \tLoss: 1.4612808227539062\nEpoch: 4 \tBatch: 94 \tLoss: 1.4703700542449951\nEpoch: 4 \tBatch: 95 \tLoss: 1.470157504081726\nEpoch: 4 \tBatch: 96 \tLoss: 1.462664008140564\nEpoch: 4 \tBatch: 97 \tLoss: 1.4611929655075073\nEpoch: 4 \tBatch: 98 \tLoss: 1.4704612493515015\nEpoch: 4 \tBatch: 99 \tLoss: 1.4937384128570557\nEpoch: 4 \tBatch: 100 \tLoss: 1.4694437980651855\nEpoch: 4 \tBatch: 101 \tLoss: 1.4685693979263306\nEpoch: 4 \tBatch: 102 \tLoss: 1.4734524488449097\nEpoch: 4 \tBatch: 103 \tLoss: 1.4631603956222534\nEpoch: 4 \tBatch: 104 \tLoss: 1.4658269882202148\nEpoch: 4 \tBatch: 105 \tLoss: 1.4699631929397583\nEpoch: 4 \tBatch: 106 \tLoss: 1.470495581626892\nEpoch: 4 \tBatch: 107 \tLoss: 1.4614611864089966\nEpoch: 4 \tBatch: 108 \tLoss: 1.4613476991653442\nEpoch: 4 \tBatch: 109 \tLoss: 1.4770721197128296\nEpoch: 4 \tBatch: 110 \tLoss: 1.477483868598938\nEpoch: 4 \tBatch: 111 \tLoss: 1.4765491485595703\nEpoch: 4 \tBatch: 112 \tLoss: 1.47175133228302\nEpoch: 4 \tBatch: 113 \tLoss: 1.469126582145691\nEpoch: 4 \tBatch: 114 \tLoss: 1.481077790260315\nEpoch: 4 \tBatch: 115 \tLoss: 1.4964804649353027\nEpoch: 4 \tBatch: 116 \tLoss: 1.4760899543762207\nEpoch: 4 \tBatch: 117 \tLoss: 1.4692126512527466\nEpoch: 4 \tBatch: 118 \tLoss: 1.4769296646118164\nEpoch: 4 \tBatch: 119 \tLoss: 1.4643590450286865\nEpoch: 4 \tBatch: 120 \tLoss: 1.491190791130066\nEpoch: 4 \tBatch: 121 \tLoss: 1.470069408416748\nEpoch: 4 \tBatch: 122 \tLoss: 1.4848525524139404\nEpoch: 4 \tBatch: 123 \tLoss: 1.4920859336853027\nEpoch: 4 \tBatch: 124 \tLoss: 1.4750339984893799\nEpoch: 4 \tBatch: 125 \tLoss: 1.477143406867981\nEpoch: 4 \tBatch: 126 \tLoss: 1.4702956676483154\nEpoch: 4 \tBatch: 127 \tLoss: 1.4852519035339355\nEpoch: 4 \tBatch: 128 \tLoss: 1.4730840921401978\nEpoch: 4 \tBatch: 129 \tLoss: 1.4715511798858643\nEpoch: 4 \tBatch: 130 \tLoss: 1.4795337915420532\nEpoch: 4 \tBatch: 131 \tLoss: 1.4892797470092773\nEpoch: 4 \tBatch: 132 \tLoss: 1.481529951095581\nEpoch: 4 \tBatch: 133 \tLoss: 1.4639486074447632\nEpoch: 4 \tBatch: 134 \tLoss: 1.4706840515136719\nEpoch: 4 \tBatch: 135 \tLoss: 1.4615579843521118\nEpoch: 4 \tBatch: 136 \tLoss: 1.4688224792480469\nEpoch: 4 \tBatch: 137 \tLoss: 1.47953462600708\nEpoch: 4 \tBatch: 138 \tLoss: 1.4620798826217651\nEpoch: 4 \tBatch: 139 \tLoss: 1.4710348844528198\nEpoch: 4 \tBatch: 140 \tLoss: 1.4627418518066406\nEpoch: 4 \tBatch: 141 \tLoss: 1.4720693826675415\nEpoch: 4 \tBatch: 142 \tLoss: 1.4690724611282349\nEpoch: 4 \tBatch: 143 \tLoss: 1.4689900875091553\nEpoch: 4 \tBatch: 144 \tLoss: 1.470324993133545\nEpoch: 4 \tBatch: 145 \tLoss: 1.4684607982635498\nEpoch: 4 \tBatch: 146 \tLoss: 1.4613022804260254\nEpoch: 4 \tBatch: 147 \tLoss: 1.4719547033309937\nEpoch: 4 \tBatch: 148 \tLoss: 1.4658334255218506\nEpoch: 4 \tBatch: 149 \tLoss: 1.4636774063110352\nEpoch: 4 \tBatch: 150 \tLoss: 1.4798966646194458\nEpoch: 4 \tBatch: 151 \tLoss: 1.4825323820114136\nEpoch: 4 \tBatch: 152 \tLoss: 1.4867528676986694\nEpoch: 4 \tBatch: 153 \tLoss: 1.4698596000671387\nEpoch: 4 \tBatch: 154 \tLoss: 1.4696614742279053\nEpoch: 4 \tBatch: 155 \tLoss: 1.4689159393310547\nEpoch: 4 \tBatch: 156 \tLoss: 1.4771995544433594\nEpoch: 4 \tBatch: 157 \tLoss: 1.4851371049880981\nEpoch: 4 \tBatch: 158 \tLoss: 1.4621152877807617\nEpoch: 4 \tBatch: 159 \tLoss: 1.4613591432571411\nEpoch: 4 \tBatch: 160 \tLoss: 1.462798833847046\nEpoch: 4 \tBatch: 161 \tLoss: 1.4638550281524658\nEpoch: 4 \tBatch: 162 \tLoss: 1.4920271635055542\nEpoch: 4 \tBatch: 163 \tLoss: 1.469355821609497\nEpoch: 4 \tBatch: 164 \tLoss: 1.4767658710479736\nEpoch: 4 \tBatch: 165 \tLoss: 1.4977415800094604\nEpoch: 4 \tBatch: 166 \tLoss: 1.4624764919281006\nEpoch: 4 \tBatch: 167 \tLoss: 1.4842184782028198\nEpoch: 4 \tBatch: 168 \tLoss: 1.4615224599838257\nEpoch: 4 \tBatch: 169 \tLoss: 1.469079613685608\nEpoch: 4 \tBatch: 170 \tLoss: 1.4728175401687622\nEpoch: 4 \tBatch: 171 \tLoss: 1.461916208267212\nEpoch: 4 \tBatch: 172 \tLoss: 1.47079336643219\nEpoch: 4 \tBatch: 173 \tLoss: 1.4697717428207397\nEpoch: 4 \tBatch: 174 \tLoss: 1.4621227979660034\nEpoch: 4 \tBatch: 175 \tLoss: 1.4768599271774292\nEpoch: 4 \tBatch: 176 \tLoss: 1.4675734043121338\nEpoch: 4 \tBatch: 177 \tLoss: 1.4699300527572632\nEpoch: 4 \tBatch: 178 \tLoss: 1.4613020420074463\nEpoch: 4 \tBatch: 179 \tLoss: 1.4784637689590454\nEpoch: 4 \tBatch: 180 \tLoss: 1.474158763885498\nEpoch: 4 \tBatch: 181 \tLoss: 1.4843240976333618\nEpoch: 4 \tBatch: 182 \tLoss: 1.4692245721817017\nEpoch: 4 \tBatch: 183 \tLoss: 1.491302728652954\nEpoch: 4 \tBatch: 184 \tLoss: 1.4767985343933105\nEpoch: 4 \tBatch: 185 \tLoss: 1.4758381843566895\nEpoch: 4 \tBatch: 186 \tLoss: 1.471017837524414\nEpoch: 4 \tBatch: 187 \tLoss: 1.4694952964782715\nEpoch: 4 \tBatch: 188 \tLoss: 1.4611735343933105\nEpoch: 4 \tBatch: 189 \tLoss: 1.4713221788406372\nEpoch: 4 \tBatch: 190 \tLoss: 1.4622604846954346\nEpoch: 4 \tBatch: 191 \tLoss: 1.4691247940063477\nEpoch: 4 \tBatch: 192 \tLoss: 1.4717767238616943\nEpoch: 4 \tBatch: 193 \tLoss: 1.4714409112930298\nEpoch: 4 \tBatch: 194 \tLoss: 1.4806828498840332\nEpoch: 4 \tBatch: 195 \tLoss: 1.4676918983459473\nEpoch: 4 \tBatch: 196 \tLoss: 1.4767311811447144\nEpoch: 4 \tBatch: 197 \tLoss: 1.4789942502975464\nEpoch: 4 \tBatch: 198 \tLoss: 1.47075617313385\nEpoch: 4 \tBatch: 199 \tLoss: 1.497684121131897\nEpoch: 4 \tBatch: 200 \tLoss: 1.4704164266586304\nEpoch: 4 \tBatch: 201 \tLoss: 1.4767225980758667\nEpoch: 4 \tBatch: 202 \tLoss: 1.464243769645691\nEpoch: 4 \tBatch: 203 \tLoss: 1.471469521522522\nEpoch: 4 \tBatch: 204 \tLoss: 1.4719852209091187\nEpoch: 4 \tBatch: 205 \tLoss: 1.4624147415161133\nEpoch: 4 \tBatch: 206 \tLoss: 1.4747464656829834\nEpoch: 4 \tBatch: 207 \tLoss: 1.4776474237442017\nEpoch: 4 \tBatch: 208 \tLoss: 1.4851768016815186\nEpoch: 4 \tBatch: 209 \tLoss: 1.4622148275375366\nEpoch: 4 \tBatch: 210 \tLoss: 1.468940019607544\nEpoch: 4 \tBatch: 211 \tLoss: 1.461174726486206\nEpoch: 4 \tBatch: 212 \tLoss: 1.461320161819458\nEpoch: 4 \tBatch: 213 \tLoss: 1.4621045589447021\nEpoch: 4 \tBatch: 214 \tLoss: 1.4693686962127686\nEpoch: 4 \tBatch: 215 \tLoss: 1.4686615467071533\nEpoch: 4 \tBatch: 216 \tLoss: 1.4844213724136353\nEpoch: 4 \tBatch: 217 \tLoss: 1.4796925783157349\nEpoch: 4 \tBatch: 218 \tLoss: 1.474039077758789\nEpoch: 4 \tBatch: 219 \tLoss: 1.4611914157867432\nEpoch: 4 \tBatch: 220 \tLoss: 1.4696812629699707\nEpoch: 4 \tBatch: 221 \tLoss: 1.4929639101028442\nEpoch: 4 \tBatch: 222 \tLoss: 1.476388692855835\nEpoch: 4 \tBatch: 223 \tLoss: 1.4723684787750244\nEpoch: 4 \tBatch: 224 \tLoss: 1.4681692123413086\nEpoch: 4 \tBatch: 225 \tLoss: 1.496944546699524\nEpoch: 4 \tBatch: 226 \tLoss: 1.4848092794418335\nEpoch: 4 \tBatch: 227 \tLoss: 1.4612597227096558\nEpoch: 4 \tBatch: 228 \tLoss: 1.4775480031967163\nEpoch: 4 \tBatch: 229 \tLoss: 1.474155068397522\nEpoch: 4 \tBatch: 230 \tLoss: 1.4670315980911255\nEpoch: 4 \tBatch: 231 \tLoss: 1.4746856689453125\nEpoch: 4 \tBatch: 232 \tLoss: 1.4841313362121582\nEpoch: 4 \tBatch: 233 \tLoss: 1.4694631099700928\nEpoch: 4 \tBatch: 234 \tLoss: 1.4637442827224731\nEpoch: 4 \tBatch: 235 \tLoss: 1.4615892171859741\nEpoch: 4 \tBatch: 236 \tLoss: 1.4773542881011963\nEpoch: 4 \tBatch: 237 \tLoss: 1.461937665939331\nEpoch: 4 \tBatch: 238 \tLoss: 1.4613093137741089\nEpoch: 4 \tBatch: 239 \tLoss: 1.4690783023834229\nEpoch: 4 \tBatch: 240 \tLoss: 1.4733270406723022\nEpoch: 4 \tBatch: 241 \tLoss: 1.4710347652435303\nEpoch: 4 \tBatch: 242 \tLoss: 1.491091251373291\nEpoch: 4 \tBatch: 243 \tLoss: 1.4614441394805908\nEpoch: 4 \tBatch: 244 \tLoss: 1.4768562316894531\nEpoch: 4 \tBatch: 245 \tLoss: 1.470885992050171\nEpoch: 4 \tBatch: 246 \tLoss: 1.4783707857131958\nEpoch: 4 \tBatch: 247 \tLoss: 1.4995017051696777\nEpoch: 4 \tBatch: 248 \tLoss: 1.467012643814087\nEpoch: 4 \tBatch: 249 \tLoss: 1.4703036546707153\nEpoch: 4 \tBatch: 250 \tLoss: 1.4701862335205078\nEpoch: 4 \tBatch: 251 \tLoss: 1.481945276260376\nEpoch: 4 \tBatch: 252 \tLoss: 1.4864252805709839\nEpoch: 4 \tBatch: 253 \tLoss: 1.4614551067352295\nEpoch: 4 \tBatch: 254 \tLoss: 1.4612644910812378\nEpoch: 4 \tBatch: 255 \tLoss: 1.4652644395828247\nEpoch: 4 \tBatch: 256 \tLoss: 1.476948857307434\nEpoch: 4 \tBatch: 257 \tLoss: 1.4780844449996948\nEpoch: 4 \tBatch: 258 \tLoss: 1.4611949920654297\nEpoch: 4 \tBatch: 259 \tLoss: 1.4690368175506592\nEpoch: 4 \tBatch: 260 \tLoss: 1.46897554397583\nEpoch: 4 \tBatch: 261 \tLoss: 1.4700589179992676\nEpoch: 4 \tBatch: 262 \tLoss: 1.4692139625549316\nEpoch: 4 \tBatch: 263 \tLoss: 1.4730324745178223\nEpoch: 4 \tBatch: 264 \tLoss: 1.4632325172424316\nEpoch: 4 \tBatch: 265 \tLoss: 1.4691565036773682\nEpoch: 4 \tBatch: 266 \tLoss: 1.479114294052124\nEpoch: 4 \tBatch: 267 \tLoss: 1.4649431705474854\nEpoch: 4 \tBatch: 268 \tLoss: 1.4878443479537964\nEpoch: 4 \tBatch: 269 \tLoss: 1.4707210063934326\nEpoch: 4 \tBatch: 270 \tLoss: 1.4666680097579956\nEpoch: 4 \tBatch: 271 \tLoss: 1.4693313837051392\nEpoch: 4 \tBatch: 272 \tLoss: 1.4748170375823975\nEpoch: 4 \tBatch: 273 \tLoss: 1.4822359085083008\nEpoch: 4 \tBatch: 274 \tLoss: 1.4789178371429443\nEpoch: 4 \tBatch: 275 \tLoss: 1.4843634366989136\nEpoch: 4 \tBatch: 276 \tLoss: 1.4774588346481323\nEpoch: 4 \tBatch: 277 \tLoss: 1.4694089889526367\nEpoch: 4 \tBatch: 278 \tLoss: 1.4692049026489258\nEpoch: 4 \tBatch: 279 \tLoss: 1.4916034936904907\nEpoch: 4 \tBatch: 280 \tLoss: 1.4778209924697876\nEpoch: 4 \tBatch: 281 \tLoss: 1.4725563526153564\nEpoch: 4 \tBatch: 282 \tLoss: 1.4625009298324585\nEpoch: 4 \tBatch: 283 \tLoss: 1.4686975479125977\nEpoch: 4 \tBatch: 284 \tLoss: 1.4690135717391968\nEpoch: 4 \tBatch: 285 \tLoss: 1.462493896484375\nEpoch: 4 \tBatch: 286 \tLoss: 1.4692192077636719\nEpoch: 4 \tBatch: 287 \tLoss: 1.4654830694198608\nEpoch: 4 \tBatch: 288 \tLoss: 1.4623026847839355\nEpoch: 4 \tBatch: 289 \tLoss: 1.475851058959961\nEpoch: 4 \tBatch: 290 \tLoss: 1.4745228290557861\nEpoch: 4 \tBatch: 291 \tLoss: 1.4744304418563843\nEpoch: 4 \tBatch: 292 \tLoss: 1.4811125993728638\nEpoch: 4 \tBatch: 293 \tLoss: 1.4845787286758423\nEpoch: 4 \tBatch: 294 \tLoss: 1.4824994802474976\nEpoch: 4 \tBatch: 295 \tLoss: 1.4828091859817505\nEpoch: 4 \tBatch: 296 \tLoss: 1.4780546426773071\nEpoch: 4 \tBatch: 297 \tLoss: 1.4830498695373535\nEpoch: 4 \tBatch: 298 \tLoss: 1.470796823501587\nEpoch: 4 \tBatch: 299 \tLoss: 1.4689363241195679\nEpoch: 4 \tBatch: 300 \tLoss: 1.4633227586746216\nEpoch: 4 \tBatch: 301 \tLoss: 1.4613579511642456\nEpoch: 4 \tBatch: 302 \tLoss: 1.4615713357925415\nEpoch: 4 \tBatch: 303 \tLoss: 1.4734247922897339\nEpoch: 4 \tBatch: 304 \tLoss: 1.4611884355545044\nEpoch: 4 \tBatch: 305 \tLoss: 1.4938480854034424\nEpoch: 4 \tBatch: 306 \tLoss: 1.469030499458313\nEpoch: 4 \tBatch: 307 \tLoss: 1.4676713943481445\nEpoch: 4 \tBatch: 308 \tLoss: 1.4774563312530518\nEpoch: 4 \tBatch: 309 \tLoss: 1.4868699312210083\nEpoch: 4 \tBatch: 310 \tLoss: 1.4692192077636719\nEpoch: 4 \tBatch: 311 \tLoss: 1.4611564874649048\nEpoch: 4 \tBatch: 312 \tLoss: 1.4765565395355225\nEpoch: 4 \tBatch: 313 \tLoss: 1.4768071174621582\nEpoch: 4 \tBatch: 314 \tLoss: 1.4619450569152832\nEpoch: 4 \tBatch: 315 \tLoss: 1.477746844291687\nEpoch: 4 \tBatch: 316 \tLoss: 1.4756678342819214\nEpoch: 4 \tBatch: 317 \tLoss: 1.4612752199172974\nEpoch: 4 \tBatch: 318 \tLoss: 1.471091866493225\nEpoch: 4 \tBatch: 319 \tLoss: 1.4860750436782837\nEpoch: 4 \tBatch: 320 \tLoss: 1.4689651727676392\nEpoch: 4 \tBatch: 321 \tLoss: 1.4745646715164185\nEpoch: 4 \tBatch: 322 \tLoss: 1.4784467220306396\nEpoch: 4 \tBatch: 323 \tLoss: 1.4655566215515137\nEpoch: 4 \tBatch: 324 \tLoss: 1.4611902236938477\nEpoch: 4 \tBatch: 325 \tLoss: 1.4691250324249268\nEpoch: 4 \tBatch: 326 \tLoss: 1.4612150192260742\nEpoch: 4 \tBatch: 327 \tLoss: 1.471775770187378\nEpoch: 4 \tBatch: 328 \tLoss: 1.4803353548049927\nEpoch: 4 \tBatch: 329 \tLoss: 1.47672700881958\nEpoch: 4 \tBatch: 330 \tLoss: 1.4857149124145508\nEpoch: 4 \tBatch: 331 \tLoss: 1.4620527029037476\nEpoch: 4 \tBatch: 332 \tLoss: 1.5051257610321045\nEpoch: 4 \tBatch: 333 \tLoss: 1.4667311906814575\nEpoch: 4 \tBatch: 334 \tLoss: 1.4636815786361694\nEpoch: 4 \tBatch: 335 \tLoss: 1.463057279586792\nEpoch: 4 \tBatch: 336 \tLoss: 1.4671447277069092\nEpoch: 4 \tBatch: 337 \tLoss: 1.478186011314392\nEpoch: 4 \tBatch: 338 \tLoss: 1.4751023054122925\nEpoch: 4 \tBatch: 339 \tLoss: 1.4640201330184937\nEpoch: 4 \tBatch: 340 \tLoss: 1.4694154262542725\nEpoch: 4 \tBatch: 341 \tLoss: 1.4611955881118774\nEpoch: 4 \tBatch: 342 \tLoss: 1.4611583948135376\nEpoch: 4 \tBatch: 343 \tLoss: 1.4709354639053345\nEpoch: 4 \tBatch: 344 \tLoss: 1.4696625471115112\nEpoch: 4 \tBatch: 345 \tLoss: 1.4846922159194946\nEpoch: 4 \tBatch: 346 \tLoss: 1.4664051532745361\nEpoch: 4 \tBatch: 347 \tLoss: 1.476877212524414\nEpoch: 4 \tBatch: 348 \tLoss: 1.4689741134643555\nEpoch: 4 \tBatch: 349 \tLoss: 1.461700677871704\nEpoch: 4 \tBatch: 350 \tLoss: 1.4845479726791382\nEpoch: 4 \tBatch: 351 \tLoss: 1.4768383502960205\nEpoch: 4 \tBatch: 352 \tLoss: 1.4722596406936646\nEpoch: 4 \tBatch: 353 \tLoss: 1.4697840213775635\nEpoch: 4 \tBatch: 354 \tLoss: 1.47409987449646\nEpoch: 4 \tBatch: 355 \tLoss: 1.4769670963287354\nEpoch: 4 \tBatch: 356 \tLoss: 1.4700697660446167\nEpoch: 4 \tBatch: 357 \tLoss: 1.464060664176941\nEpoch: 4 \tBatch: 358 \tLoss: 1.4688974618911743\nEpoch: 4 \tBatch: 359 \tLoss: 1.4611961841583252\nEpoch: 4 \tBatch: 360 \tLoss: 1.4713821411132812\nEpoch: 4 \tBatch: 361 \tLoss: 1.4617693424224854\nEpoch: 4 \tBatch: 362 \tLoss: 1.464396595954895\nEpoch: 4 \tBatch: 363 \tLoss: 1.4768221378326416\nEpoch: 4 \tBatch: 364 \tLoss: 1.4612209796905518\nEpoch: 4 \tBatch: 365 \tLoss: 1.4698599576950073\nEpoch: 4 \tBatch: 366 \tLoss: 1.462604284286499\nEpoch: 4 \tBatch: 367 \tLoss: 1.4846714735031128\nEpoch: 4 \tBatch: 368 \tLoss: 1.4677249193191528\nEpoch: 4 \tBatch: 369 \tLoss: 1.4801079034805298\nEpoch: 4 \tBatch: 370 \tLoss: 1.467362642288208\nEpoch: 4 \tBatch: 371 \tLoss: 1.4613609313964844\nEpoch: 4 \tBatch: 372 \tLoss: 1.4754588603973389\nEpoch: 4 \tBatch: 373 \tLoss: 1.4680588245391846\nEpoch: 4 \tBatch: 374 \tLoss: 1.4634156227111816\nEpoch: 4 \tBatch: 375 \tLoss: 1.4686609506607056\nEpoch: 4 \tBatch: 376 \tLoss: 1.4618980884552002\nEpoch: 4 \tBatch: 377 \tLoss: 1.4624391794204712\nEpoch: 4 \tBatch: 378 \tLoss: 1.4846221208572388\nEpoch: 4 \tBatch: 379 \tLoss: 1.4628283977508545\nEpoch: 4 \tBatch: 380 \tLoss: 1.4708530902862549\nEpoch: 4 \tBatch: 381 \tLoss: 1.4826189279556274\nEpoch: 4 \tBatch: 382 \tLoss: 1.4671146869659424\nEpoch: 4 \tBatch: 383 \tLoss: 1.4685112237930298\nEpoch: 4 \tBatch: 384 \tLoss: 1.4716145992279053\nEpoch: 4 \tBatch: 385 \tLoss: 1.461151123046875\nEpoch: 4 \tBatch: 386 \tLoss: 1.4787960052490234\nEpoch: 4 \tBatch: 387 \tLoss: 1.4612032175064087\nEpoch: 4 \tBatch: 388 \tLoss: 1.4615247249603271\nEpoch: 4 \tBatch: 389 \tLoss: 1.4743282794952393\nEpoch: 4 \tBatch: 390 \tLoss: 1.469662070274353\nTraining Complete. Final loss = 1.469662070274353\nTrain Accuracy: 98.956\nValidation Accuracy: 99.03\nTest Accuracy: 98.42\n"
    }
   ],
   "source": [
    "#Try a larger learning rate\n",
    "cnn3 = ConvNet(batch_norm=True).to(device)\n",
    "optimiser = torch.optim.Adam(cnn3.parameters(), lr=0.05) #before lr=0.0005\n",
    "writer3 = SummaryWriter(log_dir=\"runs/cnn_bn_lr05\")\n",
    "train(cnn2, epochs, writer=writer3)\n",
    "print('Train Accuracy:', calc_accuracy(cnn2, train_loader))\n",
    "print('Validation Accuracy:', calc_accuracy(cnn2, val_loader))\n",
    "print('Test Accuracy:', calc_accuracy(cnn2, test_loader))"
   ]
  },
  {
   "source": [
    "![Loss Comparison](images/loss.png)\n",
    "From the three runs, we can see that the vanilla CNN (orange) took the longest to converge. The CNN with batch normalisation (red) was much faster to do so and we were even able to use a larger learning rate (blue), which leads to even faster convergence, because we have a more robust cnn due to batch normalisation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "With batch normalisation, we can use larger learning rates and have to be less careful with the selection of initial parameters, which leads to faster convergence and better performance. The networks do not even need to be trained so long, hence we could introduce early stopping now. If you want to learn about early stopping, you can do so [here](https://www.kaggle.com/akhileshrai/tutorial-early-stopping-vanilla-rnn-pytorch). Batch normalisation even works as a regularisation technique. By estimating the mean and variance for each batch, we add noise to the input data of each following layer, which helps the regularisation capability of the network. <br>\n",
    "It should be used with caution in combination with small batch sizes as the estimation of the mean and variance per batch is unstable, if we only have a small batch sample to calculate these values from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
